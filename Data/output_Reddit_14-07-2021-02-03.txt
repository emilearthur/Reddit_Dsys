[{"id": "odrudt", "title": "[D] Machine Learning - WAYR (What Are You Reading) - Week 116", "score": 22, "url": "https://www.reddit.com/r/MachineLearning/comments/odrudt/d_machine_learning_wayr_what_are_you_reading_week/", "author": "ML_WAYR_bot", "subreddit": "r/MachineLearning", "description": "This is a place to share machine learning research papers, journals, and articles that you're reading this week. If it relates to what you're researching, by all means elaborate and give us your insight, otherwise it could just be an interesting paper you've read.\n\nPlease try to provide some insight from your understanding and please don't post things which are present in wiki.\n\nPreferably you should link the arxiv page (not the PDF, you can easily access the PDF from the summary page but not the other way around) or any other pertinent links.\n\nPrevious weeks :\n\n|1-10|11-20|21-30|31-40|41-50|51-60|61-70|71-80|81-90|91-100|101-110|111-120|\n|----|-----|-----|-----|-----|-----|-----|-----|-----|------|-------|-------|\n|[Week 1](https://www.reddit.com/4qyjiq)|[Week 11](https://www.reddit.com/57xw56)|[Week 21](https://www.reddit.com/60ildf)|[Week 31](https://www.reddit.com/6s0k1u)|[Week 41](https://www.reddit.com/7tn2ax)|[Week 51](https://reddit.com/9s9el5)|[Week 61](https://reddit.com/bfsx4z)|[Week 71](https://reddit.com/d7vno3)|[Week 81](https://reddit.com/f1f0iq)|[Week 91](https://reddit.com/hlt38o)|[Week 101](https://reddit.com/k81ywb)|[Week 111](https://reddit.com/myg8sm)||||||||||\n|[Week 2](https://www.reddit.com/4s2xqm)|[Week 12](https://www.reddit.com/5acb1t)|[Week 22](https://www.reddit.com/64jwde)|[Week 32](https://www.reddit.com/72ab5y)|[Week 42](https://www.reddit.com/7wvjfk)|[Week 52](https://reddit.com/a4opot)|[Week 62](https://reddit.com/bl29ov)|[Week 72](https://reddit.com/de8h48)|[Week 82](https://reddit.com/f8fs6z)|[Week 92](https://reddit.com/hu6zq9)|[Week 102](https://reddit.com/kh27nx)|[Week 112](https://reddit.com/n8m6ds)||\n|[Week 3](https://www.reddit.com/4t7mqm)|[Week 13](https://www.reddit.com/5cwfb6)|[Week 23](https://www.reddit.com/674331)|[Week 33](https://www.reddit.com/75405d)|[Week 43](https://www.reddit.com/807ex4)|[Week 53](https://reddit.com/a8yaro)|[Week 63](https://reddit.com/bqlb3v)|[Week 73](https://reddit.com/dkox1s)|[Week 83](https://reddit.com/ffi41b)|[Week 93](https://reddit.com/iaz892)|[Week 103](https://reddit.com/kpsxtc)|[Week 113](https://reddit.com/njfsc6)||\n|[Week 4](https://www.reddit.com/4ub2kw)|[Week 14](https://www.reddit.com/5fc5mh)|[Week 24](https://www.reddit.com/68hhhb)|[Week 34](https://www.reddit.com/782js9)|[Week 44](https://reddit.com/8aluhs)|[Week 54](https://reddit.com/ad9ssz)|[Week 64](https://reddit.com/bw1jm7)|[Week 74](https://reddit.com/dr6nca)|[Week 84](https://reddit.com/fn62r1)|[Week 94](https://reddit.com/ijjcep)|[Week 104](https://reddit.com/kzevku)|[Week 114](https://reddit.com/ntu6lq)||\n|[Week 5](https://www.reddit.com/4xomf7)|[Week 15](https://www.reddit.com/5hy4ur)|[Week 25](https://www.reddit.com/69teiz)|[Week 35](https://www.reddit.com/7b0av0)|[Week 45](https://reddit.com/8tnnez)|[Week 55](https://reddit.com/ai29gi)|[Week 65](https://reddit.com/c7itkk)|[Week 75](https://reddit.com/dxshkg)|[Week 85](https://reddit.com/fvk7j6)|[Week 95](https://reddit.com/is5hj9)|[Week 105](https://reddit.com/l9lvgs)|[Week 115](https://reddit.com/o4dph1)||\n|[Week 6](https://www.reddit.com/4zcyvk)|[Week 16](https://www.reddit.com/5kd6vd)|[Week 26](https://www.reddit.com/6d7nb1)|[Week 36](https://www.reddit.com/7e3fx6)|[Week 46](https://reddit.com/8x48oj)|[Week 56](https://reddit.com/ap8ctk)|[Week 66](https://reddit.com/cd7gko)|[Week 76](https://reddit.com/e4nmyk)|[Week 86](https://reddit.com/g4eavg)|[Week 96](https://reddit.com/j0xr24)|[Week 106](https://reddit.com/ljx92n)||\n|[Week 7](https://www.reddit.com/52t6mo)|[Week 17](https://www.reddit.com/5ob7dx)|[Week 27](https://www.reddit.com/6gngwc)|[Week 37](https://www.reddit.com/7hcc2c)|[Week 47](https://reddit.com/910jmh)|[Week 57](https://reddit.com/auci7c)|[Week 67](https://reddit.com/cj0kyc)|[Week 77](https://reddit.com/eb4lxk)|[Week 87](https://reddit.com/gcx3uf)|[Week 97](https://reddit.com/j9cbfs)|[Week 107](https://reddit.com/luqbxl)||\n|[Week 8](https://www.reddit.com/53heol)|[Week 18](https://www.reddit.com/5r14yd)|[Week 28](https://www.reddit.com/6jgdva)|[Week 38](https://www.reddit.com/7kgcqr)|[Week 48](https://reddit.com/94up0g)|[Week 58](https://reddit.com/azjoht)|[Week 68](https://reddit.com/cp1jex)|[Week 78](https://reddit.com/ehbfst)|[Week 88](https://reddit.com/glm6sv)|[Week 98](https://reddit.com/jhzz9v)|[Week 108](https://reddit.com/m52u5z)||\n|[Week 9](https://www.reddit.com/54kvsu)|[Week 19](https://www.reddit.com/5tt9cz)|[Week 29](https://www.reddit.com/6m9l1v)|[Week 39](https://www.reddit.com/7nayri)|[Week 49](https://reddit.com/98n2rt)|[Week 59](https://reddit.com/b50r5y)|[Week 69](https://reddit.com/cvde5a)|[Week 79](https://reddit.com/entcxy)|[Week 89](https://reddit.com/gu5t0d)|[Week 99](https://reddit.com/jqjgo2)|[Week 109](https://reddit.com/mf8m6u)||\n|[Week 10](https://www.reddit.com/56s2oa)|[Week 20](https://www.reddit.com/5wh2wb)|[Week 30](https://www.reddit.com/6p3ha7)|[Week 40](https://www.reddit.com/7qel9p)|[Week 50](https://reddit.com/9cf158)|[Week 60](https://reddit.com/bakew0)|[Week 70](https://reddit.com/d1g1k9)|[Week 80](https://reddit.com/euctyw)|[Week 90](https://reddit.com/hddf7j)|[Week 100](https://reddit.com/jz3evt)|[Week 110](https://reddit.com/moy40m)||\n\nMost upvoted papers two weeks ago:\n\n/u/NEGU93: [here](https://www.reddit.com/r/MachineLearning/comments/o2q1h8/r_complexvalued_neural_networks/)\n\nBesides that, there are no rules, have fun."}, {"id": "ojdmza", "title": "[D] Similarity of a high ranking journal publication with an earlier published arxiv paper?", "score": 156, "url": "https://www.reddit.com/r/MachineLearning/comments/ojdmza/d_similarity_of_a_high_ranking_journal/", "author": "CatCalm8947", "subreddit": "r/MachineLearning", "description": "I notice a very peculiar similarity between a paper from top journal of our field (IJCV) and an arXiv paper that has been published earlier that submission date of IJCV paper. So I assume the arXiv paper is suspicious and I am wondering should I contact the an authority who would take action or it does not worth trouble. such actions  disappoints me from academia.\n\nThe main idea of IJCV is thoroughly explained earlier in the arXiv paper. No reference is given to arXiv paper. A section of arXiv paper is obviously copy-pasted (with modification is verbs and name). At least two figures are **directly** copied like the following.  Just compare section of \" 3.2. Supervision or Knowledge Transfer \" from original arxiv paper to section of \" 3.2 Cross-Architecture Knowledge Transfer \" from the suspected paper (IJCV). Bluntly paraphrasing verbs like \"Lets assume\" -> \"Suppose\", etc. can be observed many many times.\n\nIJCV paper  ( Tavakolian and Hadid)  :\n\n [https://link.springer.com/article/10.1007/s11263-019-01191-3](https://link.springer.com/article/10.1007/s11263-019-01191-3)\n\narXiv paper ( Diba et al.):\n\n [https://arxiv.org/pdf/1711.08200.pdf](https://arxiv.org/pdf/1711.08200.pdf)\n\nThe main figure are exactly the same, just compare the below:\n\n&#x200B;\n\n[same figures from both publications](https://preview.redd.it/5fqyyydxtya71.png?width=1494&format=png&auto=webp&s=fc365fbcc6bc1c5d586c016ee1a75f882b9e08e0)\n\nThe original arXiv paper suggests a method to accelerate training of the 3D DNNs based on initialization of weights of a pre-trained 2D. The idea is simple and straightforward. Now, the IJCV paper took this idea and claims it as its own contribution for pain assessment application.\n\n&#x200B;\n\nWhat can be done in these situations, does it mean we should not upload on arxiv..."}, {"id": "ojt1jw", "title": "[R] Stanford\u2019s AI Researchers Introduce QA-GNN Model That Jointly Reasons With Language Models And Knowledge Graphs", "score": 6, "url": "https://www.reddit.com/r/MachineLearning/comments/ojt1jw/r_stanfords_ai_researchers_introduce_qagnn_model/", "author": "techsucker", "subreddit": "r/MachineLearning", "description": "Question-answering systems are the backbone of our digital lives. From search engines to personal assistants, we use them every day and never even realize it! For example, when you ask a question like \u201cWhere was Leonardo da Vinci born?\u201d these intelligent computer programs need to gather background knowledge about him (Leonardo\u2019s birthplace is Italy) as well as computational reasoning over that information in order for an answer to be generated \u2013 which will often happen automatically without us even realizing what happened behind the scenes.\n\nIn recent AI research, background knowledge is usually available in the form of Knowledge Graphs (KGs) and Language Models (LMs) which are pre-trained on a large set of documents. KG\u2019s represent entities as nodes and relations between them as edges, e.g., \\[Leonardo da Vinci \u2014 born in \u2013 Italy\\]. Some other examples of KGs include\u00a0[Freebase (general-purpose facts](https://www.semanticscholar.org/paper/Freebase%3A-a-collaboratively-created-graph-database-Bollacker-Evans/1976c9eeccc7115d18a04f1e7fb5145db6b96002)),\u00a0[ConceptNet (commonsense)](https://arxiv.org/abs/1612.03975) and Examples of pre-trained LMs include [BERT (trained on Wikipedia articles and 10,000 books)](https://arxiv.org/abs/1810.04805), [RoBERTa (extending BERT),\u00a0](https://arxiv.org/abs/1907.11692)etc.\n\nSummary: [https://www.marktechpost.com/2021/07/13/stanfords-ai-researchers-introduce-qa-gnn-that-jointly-reasons-with-language-models-and-knowledge-graphs/](https://www.marktechpost.com/2021/07/13/stanfords-ai-researchers-introduce-qa-gnn-that-jointly-reasons-with-language-models-and-knowledge-graphs/)\n\nGithub: [https://github.com/michiyasunaga/qagnn](https://github.com/michiyasunaga/qagnn)\n\nPaper: [https://arxiv.org/pdf/2104.06378.pdf](https://arxiv.org/pdf/2104.06378.pdf)"}, {"id": "ojl611", "title": "[D] JAX in production", "score": 20, "url": "https://www.reddit.com/r/MachineLearning/comments/ojl611/d_jax_in_production/", "author": "_katta", "subreddit": "r/MachineLearning", "description": "Have you ever used JAX in your production code? Was it pure JAX? What are the use-cases?"}, {"id": "ojtyll", "title": "[D] the evolutoon of ai \"bots\" in video games", "score": 4, "url": "https://www.reddit.com/r/MachineLearning/comments/ojtyll/d_the_evolutoon_of_ai_bots_in_video_games/", "author": "SQL_beginner", "subreddit": "r/MachineLearning", "description": "Does anyone know at what point in video games history did the developers start programming the \"bots\" with algorithms that resemble ai and machine learning?\n\nFor example, a shot in the dark: it seems like the enemies in the early mario games had a more or less  fixed or random pattern, whereas higher level enemies in the gameboy pokemon games used some type of rule-based algorithm (e.g. if health < 20, use healing item). \n\nBut at what point in video game history did bots start to appear that were programmed using machine learning or ai algorithms? I don't play video hames at all, but I heard there are certain newer video games where the enemies actually \"learn\" your playing style and as a result, can put up a better fight \n\nDoes anyone know about this?\n\nThanks"}, {"id": "ojp76r", "title": "Self Taught Machine Learning Engineers: Tell us how did you land a job in ML [D]", "score": 6, "url": "https://www.reddit.com/r/MachineLearning/comments/ojp76r/self_taught_machine_learning_engineers_tell_us/", "author": "Jolly-Web9421", "subreddit": "r/MachineLearning", "description": "Even better if you landed a job without even having a degree (only a high school one)  \nI'm starting my journey as a self-taught (without a degree) and I really need some inspiration and encouragement lol\n\nAnd if you like to give some advice, that would be great!"}, {"id": "ojrndw", "title": "[D] Starting out in Scalable ML / Sys - ML / Distributed ML?", "score": 3, "url": "https://www.reddit.com/r/MachineLearning/comments/ojrndw/d_starting_out_in_scalable_ml_sys_ml_distributed/", "author": "A27_97", "subreddit": "r/MachineLearning", "description": "I'm currently a Software Engineer working on Distributed Systems. I have a background in ML, but unfortunately no ML work in my company. I would like to combine the both so I can apply to similar roles - I would like to get started on some research, but unsure of where to begin from. How reproducible are papers in this area? It looks to me that reproducing algorithms may be significantly higher than other papers due to lack of available resource."}, {"id": "ojka3z", "title": "[D] Useful classes to take for ML/Research Engineers?", "score": 10, "url": "https://www.reddit.com/r/MachineLearning/comments/ojka3z/d_useful_classes_to_take_for_mlresearch_engineers/", "author": "piykat", "subreddit": "r/MachineLearning", "description": "I'll be starting my MS CS program this Fall, post which I'd like to work as an ML Engineer or Research Engineer at a top industrial lab. Had some questions on how I can customize my course to make me a more attractive candidate for these positions\n\n1. I'll be taking grad-level Algorithms, Machine Learning, and Deep Learning class. Apart from these, I have options among the following - Statistical Machine Learning, Distributed Systems, Computer Vision, Natural Language Processing, Data Mining. Assuming that I can only take 3 out of these 5 courses, which ones should I take?\n2. Would doing a research project or writing a thesis help me in securing Research Engineer positions?"}, {"id": "oj1w02", "title": "[P] Install or update CUDA, NVIDIA Drivers, Pytorch, Tensorflow, and CuDNN with a single command: Lambda Stack", "score": 248, "url": "https://www.reddit.com/r/MachineLearning/comments/oj1w02/p_install_or_update_cuda_nvidia_drivers_pytorch/", "author": "sabalaba", "subreddit": "r/MachineLearning", "description": "I'm sure most of you have spent a lot of time in command line hell trying to install or update CUDA, NVIDIA Drivers, Pytorch, Tensorflow, etc. We made Lambda Stack to simplify installation and updates. It's a debian PPA that manages all of the libraries and dependencies, resulting in a one-line install that \"just works\".\n\nThis is a new video overview of Lambda Stack:\nhttps://www.youtube.com/watch?v=sEUOa0s-RQY\n\nThis is our Lambda Stack how-to blog post:\nhttps://lambdalabs.com/blog/install-tensorflow-and-pytorch-on-rtx-30-series/\n\nAnd this is the one liner to install (requires Ubuntu 20.04 or 18.04):\n\n    LAMBDA_REPO=$(mktemp) && \\\n    wget -O${LAMBDA_REPO} https://lambdalabs.com/static/misc/lambda-stack-repo.deb && \\\n    sudo dpkg -i ${LAMBDA_REPO} && rm -f ${LAMBDA_REPO} && \\\n    sudo apt-get update && sudo apt-get install -y lambda-stack-cuda\n\nTo update your CUDA/framework/drivers just run this:\n\n    sudo apt-get update && sudo apt-get dist-upgrade\n\nWould love any feedback!"}, {"id": "oj75ae", "title": "[D] What are Diffusion Models? by Lilian Weng", "score": 93, "url": "https://www.reddit.com/r/MachineLearning/comments/oj75ae/d_what_are_diffusion_models_by_lilian_weng/", "author": "regalalgorithm", "subreddit": "r/MachineLearning", "description": "I am sure many here have read Lilian Weng's excellent overview articles on various topics in ML, and she just released [a new one on Diffusion Models](https://lilianweng.github.io/lil-log/2021/07/11/diffusion-models.html) yesterday!\n\ntbh it was a bit dense for me, but as with her prior posts it is a great introduction to the topic for those inclined to learn more about it."}, {"id": "oji7fs", "title": "[D] What pdf parser do you use for paragraph parsing for huggingface models", "score": 11, "url": "https://www.reddit.com/r/MachineLearning/comments/oji7fs/d_what_pdf_parser_do_you_use_for_paragraph/", "author": "gevezex", "subreddit": "r/MachineLearning", "description": "For  the models in Huggingface it is essential that you use neat parsed  paragraphs and/or sentences out of pdf documents as most of the time  documents are (still) in pdf format. What is your best practice with pdf  parsers. Are you using open source pdf parsers or payed api's like  amazons textract or google document ai and the such?\n\nPersonally  I used a lot of open source versions like pdfminer, tika and xpdfreader  but all of these are \"meeh\" if you want to parse neat paragraphs and  sentences (pagenumbers / enumerated list / headers / footers are one of  the many nightmares).\n\nSo I am very interested on how you solved this issue as machine learning experts."}, {"id": "ojsouo", "title": "[P] AI Arena - Closed Beta", "score": 0, "url": "https://www.reddit.com/r/MachineLearning/comments/ojsouo/p_ai_arena_closed_beta/", "author": "brandinho77", "subreddit": "r/MachineLearning", "description": "Hi all,\n\nI don't know if anyone remembers, but I wrote an interactive article (and shared it here) on [Bayesian Q-Learning](https://www.reddit.com/r/MachineLearning/comments/jg475u/r_a_bayesian_perspective_on_qlearning/). Now I'm working on a more ambitious project - giving researchers the ability to monetize their IP by tokenizing their models on the blockchain. Our first step is implementing this via a fun battle game, which is similar to super smash bros - users can own, train, and battle with fighter NFTs powered by neural networks. At the moment the neural networks are automatically generated and can only be trained in our app, but overtime we plan on launching a python environment for researchers to train custom models and import them onto the blockchain to compete against all other models globally. If anyone is interested in learning more, check out our website: [https://aiarena.io/#/](https://aiarena.io/#/)\n\nTo read our docs click on the \"learn more\" button on the main page. We are still building it out, and we would love some initial feedback from the machine learning community. To get details on our closed beta, and to learn about how we plan on incentivizing participants, please check out the following doc:\n\n[https://drive.google.com/file/d/1xDV70fbfGo3\\_osnZeHSuXkkoykT1h2Hz/view?usp=sharing](https://drive.google.com/file/d/1xDV70fbfGo3_osnZeHSuXkkoykT1h2Hz/view?usp=sharing)\n\nHappy to answer any questions that anyone has!"}, {"id": "ojhvkl", "title": "[P] StyleGAN2 for character portraits and style transfer", "score": 5, "url": "https://www.reddit.com/r/MachineLearning/comments/ojhvkl/p_stylegan2_for_character_portraits_and_style/", "author": "p00pl00ps", "subreddit": "r/MachineLearning", "description": "Hi all!\n\nI've been working on a fun little projects for a while. It's a simple webapp which serves a couple of styleGAN2 models I  trained on a dataset of hand-drawn portraits in the style of classic RPGs.\n\n&#x200B;\n\nhttps://preview.redd.it/96egpvrvyza71.png?width=256&format=png&auto=webp&s=4e52e42cb3710ddaeebd59cd9da735341a9d5a09\n\n&#x200B;\n\n* The first model simply generates a character portrait, and does so using conditional labels allowing you to pick gender and traditional fantasy races.\n* The second model takes real faces, aligns them and [pixel2style2pixel](https://github.com/eladrich/pixel2style2pixel/tree/master/models) invert to the stylegan latent space and thus perform style transfer. The image above is the output of putting in a picture of a certain famous ML figure...see if you can recognise them!\n\nBoth models have their fair share of issues and need more work, but I thought the results were cool enough to share. You can play with th API  [here](https://rp-gen.com) if you're interested (excuse bugs / breakages due to server overloads...)\n\n&#x200B;\n\nEnjoy and le me know what you think!"}, {"id": "ojj7cr", "title": "[R] ERNIE 3.0: Large-scale Knowledge Enhanced Pre-training for Language Understanding and Generation", "score": 2, "url": "https://arxiv.org/abs/2107.02137", "author": "kizumada", "subreddit": "r/MachineLearning", "description": ""}, {"id": "ojfuqu", "title": "[R] Neural Waveshaping Synthesis", "score": 4, "url": "https://arxiv.org/abs/2107.05050", "author": "ratiugssab", "subreddit": "r/MachineLearning", "description": ""}, {"id": "ojqdjl", "title": "[D] Best no code SaaS / website for image annotation, training, and model creation?", "score": 1, "url": "https://www.reddit.com/r/MachineLearning/comments/ojqdjl/d_best_no_code_saas_website_for_image_annotation/", "author": "gecko39", "subreddit": "r/MachineLearning", "description": "I'm doing a small weekend project where I want to train a semantic segmentation network based on a pretrained imagenet model. I was curious to see if I could accomplish this via one of the many services I feel like exist out there. \nI checked roboflow, but they do not appear to support semantic segmentation. \n\nCriteria:\n\n* Annotate pixel labels for 100 or so images for 2-3 classes ( ideally with smart annotation / one click ) \n* Train model ( hopefully with some automatic image augmentation or domain generalization) \n* Download / Export model ( as tensorflow/pytorch/onnx )  along with boilerplate code to run inference on image \n* Free or cheap enough to try it once for this project \n\nI can do this myself by coding it, but figured i'd test something new.\n\nThanks"}, {"id": "oji890", "title": "[R] Mava: a research framework for distributed multi-agent reinforcement learning", "score": 3, "url": "https://www.reddit.com/r/MachineLearning/comments/oji890/r_mava_a_research_framework_for_distributed/", "author": "ktessera", "subreddit": "r/MachineLearning", "description": "[Paper](https://arxiv.org/abs/2107.01460) | [Repo](https://github.com/instadeepai/Mava)\n\nWe recently launched Mava, a research framework for distributed multi-agent reinforcement learning. Mava integrates with DeepMind\u2019s open-source RL ecosystem by building on top of [Acme](https://github.com/deepmind/acme), but extended to the multi-agent use case. We also use [reverb](https://github.com/deepmind/reverb) and [Launchpad](https://github.com/deepmind/launchpad) for data management and distribution. \n\nMava integrates with popular MARL envs like PettingZoo, SMAC, RoboCup, OpenSpiel, Flatland, and has implementations of popular MARL  algorithms. Hopefully, our framework can be of use to people working in the space and we would appreciate any feedback!"}, {"id": "ojepja", "title": "[D] Which ML topics/tasks are under-served for practical uses?", "score": 4, "url": "https://www.reddit.com/r/MachineLearning/comments/ojepja/d_which_ml_topicstasks_are_underserved_for/", "author": "PhYsIcS-GUY227", "subreddit": "r/MachineLearning", "description": "Was thinking about this today at work and wanted to hear what Reddit has to say. Some ML tasks, like object detection, have great open source projects like YOLOv5 that are used and contributed to by the community, while others not so much. \n\nWhether it\u2019s that no project exists or that there is something but it\u2019s de-facto unusable (because it\u2019s not reproducible/not modifiable/hard to integrate with/ data, models or experiments not available/etc).\n\nPut another way, which ML task/s do you wish had a great open source project?"}, {"id": "ojf2hk", "title": "[R] The DEformer: An Order-Agnostic Distribution Estimating Transformer", "score": 4, "url": "https://arxiv.org/abs/2106.06989", "author": "michaelaalcorn", "subreddit": "r/MachineLearning", "description": ""}, {"id": "ojlqsz", "title": "[P] How creatively can you augment text data? Check NL-Augmenter\ud83e\udd8e \u2192 \ud83d\udc0d", "score": 0, "url": "https://www.reddit.com/r/MachineLearning/comments/ojlqsz/p_how_creatively_can_you_augment_text_data_check/", "author": "VeterinarianFar3937", "subreddit": "r/MachineLearning", "description": "Hi r/MachineLearning Members!\n\nWe, a team of researchers spanning Google AI Language, UW, CMU and 7 other institutions organizing NL-Augmenter\u00a0\ud83e\udd8e \u2192 \ud83d\udc0d.\u00a0, are now inviting transformation submissions to the same!\n\nAll submitters of accepted transformations (and filters) will be included as co-authors on a paper announcing this framework. NL-Augmenter\u00a0\ud83e\udd8e \u2192 \ud83d\udc0d is a part of the wider GEM benchmark, [GEM (Generation, Evaluation, Metrics)](https://gem-benchmark.com/nl_augmenter) workshop at ACL, 2021 and their future iterations.\n\nThe NL-Augmenter is a collaborative effort intended to add transformations of datasets dealing with natural language. Transformations augment text datasets in diverse ways, including: introducing spelling errors, randomizing names, and numbers, paraphrasing ... and whatever creative augmentation you contribute to the benchmark.\u00a0We invite submissions of transformations to this framework by way of GitHub pull request, through **July 25, 2021**.\u00a0**All submitters of accepted transformations (and filters) will be included as co-authors on a paper announcing this framework.**\n\nProject:\u00a0[https://github.com/GEM-benchmark/NL-Augmenter](https://github.com/GEM-benchmark/NL-Augmenter)\n\nWe\u00a0strongly believe that the benefits of open science should reach\u00a0everyone and hence we are making this effort to reach you. We also encourage you to share this with other researchers in\u00a0your department who\u00a0would benefit from\u00a0this open collaboration. To know more about the\u00a0framework, check our [motivation and review criteria](https://github.com/GEM-benchmark/NL-Augmenter/blob/main/docs/doc.md)\u00a0and some of\u00a0[our recent work](https://arxiv.org/pdf/2106.09069.pdf).\n\nOrganizers:\n\nKaustubh Dhole (Amelia R&D) , Sebastian Gehrmann (Google AI Language), Varun Gangal (LTI, Carnegie Mellon University), Jascha Sohl-Dickstein (Google Brain), Tonghuang Wu (University of Washington), Simon Mille (Universitat Pompeu Fabra)\u00a0, Zhenhao Li (Imperial College, London), Saad Mahmood (Trivago R&D), Aadesh Gupta (Amelia R&D), Samson Tan (Salesforce Research), Jinho Choi (Emory University)"}, {"id": "ojj1c1", "title": "[R] Does anybody know if Voynich Manuscript was used in any ML projects?", "score": 1, "url": "https://www.reddit.com/r/MachineLearning/comments/ojj1c1/r_does_anybody_know_if_voynich_manuscript_was/", "author": "facelesshero_dale", "subreddit": "r/MachineLearning", "description": "I know how complicated would such project be. I was just curious anybody in here had ever heard or stumbled upon data science projects regarding deciphering ancient manuscripts. Also, does anybody know if there is datasets made out of some ancient manuscripts that are open to access for Data Science Projects?\n\nReposting the question, the original post was removed because of rule 3 (not having a proper tag)"}, {"id": "oj7u47", "title": "[P] ML-Agents plays DodgeBall", "score": 8, "url": "https://www.reddit.com/r/MachineLearning/comments/oj7u47/p_mlagents_plays_dodgeball/", "author": "hardmaru", "subreddit": "r/MachineLearning", "description": "A fun multi-agent RL project from the folks at Unity: https://blog.unity.com/technology/ml-agents-plays-dodgeball"}, {"id": "oinb2v", "title": "[R] The Bayesian Learning Rule", "score": 195, "url": "https://arxiv.org/abs/2107.04562", "author": "hardmaru", "subreddit": "r/MachineLearning", "description": ""}, {"id": "oj98br", "title": "[D] Does it make sense to mix a RTX 3090 with a Tesla K80 for more memory?", "score": 5, "url": "https://www.reddit.com/r/MachineLearning/comments/oj98br/d_does_it_make_sense_to_mix_a_rtx_3090_with_a/", "author": "mortadelass", "subreddit": "r/MachineLearning", "description": "RTX 3090s are now \"affordable again\" and I just got one for my Deep Learning Research. However, you still have only 24GB and I have a few jobs in which I need more than that. My Idea: buy a Tesla K80. They are 10% of the price of a RTX 3090 and deliver more 24GB of memory. I know they are much slower, but I can accept lower speed over more memory for a few tasks.  \n\n\nDoes the mix makes sense? Anyone has experience with mixing Amperes and Maxwells?"}, {"id": "oj7hxx", "title": "[D] \"Black Swan Events\" : The Final Frontier of Machine Learning?", "score": 6, "url": "https://www.reddit.com/r/MachineLearning/comments/oj7hxx/d_black_swan_events_the_final_frontier_of_machine/", "author": "jj4646", "subreddit": "r/MachineLearning", "description": "Logically speaking - do Machine Learning algorithms have any chance at successfully predicting \"black swan events\" (i.e. extremely rare events)? No matter how advanced neural networks can get - will they ever be able to even somewhat accurately predict \"black swan events\" (e.g. earthquakes, pandemics)?"}, {"id": "ojd5uf", "title": "[D] Data Analysis and Visualisation on High-Dimensional Dataset", "score": 1, "url": "https://www.reddit.com/r/MachineLearning/comments/ojd5uf/d_data_analysis_and_visualisation_on/", "author": "bobskithememe", "subreddit": "r/MachineLearning", "description": "Hello, all!\n\nI currently have a large dataset consisting of 68,000 rows and 1,550 columns. The dataset is an amalgamation of several datasets I've cleaned and merged together. Before modelling, I would like to carry out EDA and visualisation to see how the features play out and to uncover patterns etc.\n\nI'm not entirely sure what's the best approach given the dataset has 1,550 features. I'm planning to do a lot of PCA and clustering given I cannot really plot individual features from the start. Once, I've found interesting relationships and trends, I can do more conventional plotting with features.\n\nI was thinking of using affinity propagation, mean shift, DBSCAN, birch and a Gaussian mixture model but I'm unsure whether to do it on a group of features or all at once.\n\nI'm interested to hear any ideas on how you would approach this or what would be the best plan of attack?\n\nThanks in advance!"}, {"id": "ojf09q", "title": "[D] Are Batch normalization equivalent to standardization in the preprocessing?", "score": 0, "url": "https://www.reddit.com/r/MachineLearning/comments/ojf09q/d_are_batch_normalization_equivalent_to/", "author": "lsov2", "subreddit": "r/MachineLearning", "description": "Is batch normalization or layer normalization equivalent to standardizing my input features to mean=0 and std=1 during my preprocessing? Do they address the same issue? (minimizing the covariate shift)"}, {"id": "oj3iy6", "title": "[P] Unsupervised Behavioral Learning (UBL) revisited", "score": 3, "url": "https://www.reddit.com/r/MachineLearning/comments/oj3iy6/p_unsupervised_behavioral_learning_ubl_revisited/", "author": "CireNeikual", "subreddit": "r/MachineLearning", "description": "[https://ogma.ai/2021/07/unsupervised-behavioral-learning-ubl-revisited/](https://ogma.ai/2021/07/unsupervised-behavioral-learning-ubl-revisited/)\n\nI have recently decided to revisit Unsupervised Behavioral Learning (UBL) using the latest AOgmaNeo tech. UBL is an alternative to reinforcement learning (RL) that attempts to seek certain goal states instead of maximizing a reward. It learns completely passively from a stream of states and actions. For future work we intend to make an \"adapter\" that allows one to use UBL as a regular RL agent.\n\nIn the video linked in the blog post, we demonstrate a simple multi-goal task with a tiny \"rat\" robot and a cardboard T-maze with markings. All training happens on-board the Pi Zero as usual, in a fully online manner. I set the goals with a controller and then move the robot around the maze so it can autonomously return to the saved goal states. The robot is not re-trained when the goal is changed, only the goal state is swapped out at the top of the hierarchy.\n\nLet me know what you think!"}, {"id": "oj19x1", "title": "[D] What could possibly cause a test/train loss graph like this?", "score": 7, "url": "https://www.reddit.com/r/MachineLearning/comments/oj19x1/d_what_could_possibly_cause_a_testtrain_loss/", "author": "Eodmg", "subreddit": "r/MachineLearning", "description": " I'm training an RNN which gets as input an MNIST image in the first timestep and has to output the correct digit ten timesteps in a row. I'm trying to increase the size of the RNN until I observe epoch-wise double descent, but even for a 40000x40000 internal matrix, it doesn't seem to happen. What I do see are these random sudden drops in test loss, often accompanied by a spike in the train loss. Does anyone have a possible explanation for this? It isn't the result of a drop in the learning rate, since I keep it constant throughout training. Thanks \n\n[Train Loss](https://preview.redd.it/bqfel0f6rua71.png?width=2913&format=png&auto=webp&s=186892819e865b63e3505d599c9565376fbd2c36)\n\n[Test Loss](https://preview.redd.it/06fuf8g4rua71.png?width=2931&format=png&auto=webp&s=7b757eedc2e523ea4e82ed7d0b692cb1d40e8480)"}, {"id": "oj7p2c", "title": "[D] Applications of Reinforcement Learning for Search/Optimization of functions and hyperparameters", "score": 1, "url": "https://www.reddit.com/r/MachineLearning/comments/oj7p2c/d_applications_of_reinforcement_learning_for/", "author": "jj4646", "subreddit": "r/MachineLearning", "description": "Is it possible to take reinforcement learning methodologies (e.g. q-learning) and use them to find the roots of highly complex functions for the purpose of function optimization or hyperparameter tuning? \n\nCould the reinforcement learning algorithms accomplish these tasks by trying to find potential improvements/consider which areas to explore next? \n\nIf you have some cost or objective function, could reinforcement learning algorithms replace the use of algorithms like \"genetic algorithms\" for search and optimization?\n\nOr have I misunderstood the limitations of reinforcement learning?\n\nThanks"}, {"id": "oj7d4v", "title": "[D] Accepting solutions from optimization algorithms that haven't converged", "score": 0, "url": "https://www.reddit.com/r/MachineLearning/comments/oj7d4v/d_accepting_solutions_from_optimization/", "author": "jj4646", "subreddit": "r/MachineLearning", "description": "What is the opinion of the ML community on accepting the solution from an optimization algorithm that has not converged?\n\n Suppose you let an optimization algorithm (e.g. gradient descent, bayesian optimization) run for many hours, and in the end it tells you that \"convergence has not been reached\". \n\nOn one hand, you would think that it might be unwise to accept a solution from an optimization algorithm that has not converged, for it might be unstable. But the other hand, the optimization algorithm would have still done a lot of work exploring the response surface, and it still might find a point that is better than random guessing, despite not having reached convergence. Also, you could use this solution from the optimization algorithm and judge it yourself based on the error rates it provides on train/test data?\n\nIn short - is it illogical to accept a solution from an optimization algorithm that has not converged? This strikes me as an interesting problem : because many times a converged solution can still result in overfitting.\n\nI would be curious to hear everyones thoughts."}, {"id": "oitzhb", "title": "[R] MIT Proposes Novel End-to-End Procedure for Corrupted Data Cleaning, Estimation, and Inference", "score": 8, "url": "https://www.reddit.com/r/MachineLearning/comments/oitzhb/r_mit_proposes_novel_endtoend_procedure_for/", "author": "Yuqing7", "subreddit": "r/MachineLearning", "description": "A research team from MIT proposes a unified framework for estimation and inference in the presence of various forms of economic data corruption such as measurement errors, missing values, discretization, and differential privacy. \n\nHere is a quick read: [MIT Proposes Novel End-to-End Procedure for Corrupted Data Cleaning, Estimation, and Inference.](https://syncedreview.com/2021/07/12/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-59/)\n\nThe paper *Causal Inference with Corrupted Data: Measurement Error, Missing Values, Discretization, and Differential Privacy* is on [arXiv](https://arxiv.org/abs/2107.02780)."}, {"id": "ohxnts", "title": "[D] This AI reveals how much time politicians stare at their phone at work", "score": 3781, "url": "https://i.redd.it/34sgziebfia71.jpg", "author": "TheInsaneApp", "subreddit": "r/MachineLearning", "description": ""}, {"id": "oif5dp", "title": "[D] Adding guassian noise to Discriminator layers in GAN helps really stablizing training, generates sharper images and avoid mode collapse.", "score": 74, "url": "https://www.reddit.com/r/MachineLearning/comments/oif5dp/d_adding_guassian_noise_to_discriminator_layers/", "author": "0x00groot", "subreddit": "r/MachineLearning", "description": "Very simple tweak which isn't usually seen in basic GAN tutorials. Might be helpful if you are new to GANs and it's just not converging.\n\n[256x256px, results in 3 hours on colab.](https://preview.redd.it/bvy6pgfcyna71.png?width=901&format=png&auto=webp&s=9e3de840216015302f73a728d289c39fee0af21d)\n\nI am also new to GANs and just learning. I first noticed this when learning about GANs last year in tensorflow. I followed the most basic tutorial from tf docs. But results were always smudgy, fuzzy and not convincing, and easily collapsing, especially at resolutions >= 128x128. But adding Gaussian noise to each layer of Discriminator dramatically made the results much better. Inspiration was from some ganhacks and papers adding noise to just the input or generator, but haven't seen results for discriminator.\n\nFound similar results when implementing the same in Pytorch recently. Models with same architecture, config and seed. Only difference is adding of guassian noise to discriminator layers gives much better results. Have had success in training 128x128 and 256x256 face generation in just a few hours on colab.\n\nBelow are few results. Using very basic convolutional gan architecture.\n\n[128x128 Results with guassian noise in discriminator layers on celeba](https://preview.redd.it/2jyhdupytna71.png?width=1782&format=png&auto=webp&s=b8d9c04d5c38d6d6a32d6636c70b46ac6916e47c)\n\n[128x128 Results without noise on celeba, easily collapsed.](https://preview.redd.it/2gvsvb9otna71.png?width=1785&format=png&auto=webp&s=c663a2e20176df24077a086ff0a847357df858b8)\n\nAlso results on Flickr dataset 256x256 resolution in 3 hours.\n\n[256x256 Flickr dataset with guassian noise in discriminator layers ](https://preview.redd.it/yzohrt68vna71.png?width=1782&format=png&auto=webp&s=0dfac7693f6afb3b8959e0b38bd02605a5e5d050)\n\nOfcourse results aren't too crazy and still contain artifacts as this is a very basic architecture and trained for a short time. But not bad and much better as compared to without gaussian noise in discriminator.\n\nMore results runs and logs of runs with no noise, noise decay, adding noise to only generator layers, adding noise only to input, both generator and discriminator can be found here. Open different runs to see more outputs at different timesteps.: [https://wandb.ai/shivamshrirao/facegan\\_pytorch](https://wandb.ai/shivamshrirao/facegan_pytorch)\n\nView more 256px results [here](https://wandb.ai/shivamshrirao/facegan_pytorch/runs/1424g5hk).\n\nPytorch code: [https://github.com/ShivamShrirao/facegan\\_pytorch](https://github.com/ShivamShrirao/facegan_pytorch)\n\nTensorflow code (bit old): [https://github.com/ShivamShrirao/GANs\\_TF\\_2.0](https://github.com/ShivamShrirao/GANs_TF_2.0)\n\n[256x256px Flickr dataset](https://preview.redd.it/wsdz5ia0yna71.png?width=901&format=png&auto=webp&s=4183bc125088902d351c1b803589733adde2a680)"}, {"id": "oiudyw", "title": "[P] Tangram: All-in-One Automated Machine Learning Framework", "score": 4, "url": "https://www.reddit.com/r/MachineLearning/comments/oiudyw/p_tangram_allinone_automated_machine_learning/", "author": "davidyamnitsky", "subreddit": "r/MachineLearning", "description": "Homepage: [https://www.tangram.xyz](https://www.tangram.xyz/)\n\nGitHub: [https://github.com/tangramxyz/tangram](https://github.com/tangramxyz/tangram)\n\nHi all, I'd like to share Tangram, an all-in-one automated machine learning framework written in Rust. With Tangram, you:\n\n* Run `tangram train` to train a model from a CSV file on the command line.\n* Make predictions with libraries for Elixir, Go, JavaScript, Python, Ruby, and Rust.\n* Run `tangram app`to start a web application where you can learn more about your models and monitor them in production.\n\nCheck it out and let us know what you think!"}, {"id": "oigg6v", "title": "[N] AI Generated Art Scene Explodes as Hackers Create Groundbreaking New Tools", "score": 35, "url": "https://www.reddit.com/r/MachineLearning/comments/oigg6v/n_ai_generated_art_scene_explodes_as_hackers/", "author": "regalalgorithm", "subreddit": "r/MachineLearning", "description": "Quiet a good overview of the recent CLIP craze on the verge today - [AI Generated Art Scene Explodes as Hackers Create Groundbreaking New Tools](https://www.vice.com/en/article/n7bqj7/ai-generated-art-scene-explodes-as-hackers-create-groundbreaking-new-tools)\n\nML Berkeley Blog also had a great overview of this - [https://ml.berkeley.edu/blog/posts/clip-art/](https://ml.berkeley.edu/blog/posts/clip-art/)"}, {"id": "oimf70", "title": "[R] Generative network for non-image data", "score": 5, "url": "https://www.reddit.com/r/MachineLearning/comments/oimf70/r_generative_network_for_nonimage_data/", "author": "polidrupa", "subreddit": "r/MachineLearning", "description": "I  have a pool of non-image data that can be modeled as positive  real-valued vectors of a space of dimension \\~1000 aprox. I would like to  create a generative model to suggest \"similar\" vectors (i.e., vectors  that according to the model could come from the same distribution), kind  of like what GANs do.\n\nHowever, I  haven't worked with non-image generative models, and I have very little  idea of what is standard in situations like this. E.g., what models are  commonly used? GANs, VAEs, something not based in neural networks?  Finally, do you have any papers you can recommend?\n\nBest"}, {"id": "oiu3ko", "title": "[P] Evaluation of generated medical excerpts from reports", "score": 0, "url": "https://www.reddit.com/r/MachineLearning/comments/oiu3ko/p_evaluation_of_generated_medical_excerpts_from/", "author": "suseJattack", "subreddit": "r/MachineLearning", "description": "Hello everyone!\n\nI am finishing my Master's in Data Science and just built a language generator based on GPT-2 for comment generation. We now are in the part of assessing whether the comments generated are actually any good or not.\n\nFor that, I used some metrics like BLEU or Rouge, but I would love to have some professional supervision from actual doctors.\n\nI built [this](https://forms.gle/sXUGKWaQj8jgz9EQ9) google form in order to make it easy for human evaluation. 30 questions, the title of each question is a random comment from my generator and the goal is to assess the coherence and rigor of every comment.\n\nDoes anyone know of a subreddit that I can post the form to? Does any of you guys work with these kind of data and could advice me on what metrics to use?\n\nThanks so much for your help! Stay safe."}, {"id": "oizqf2", "title": "[P] how to write a neurips paper [3] : gearing up with knowledge", "score": 0, "url": "https://www.reddit.com/r/MachineLearning/comments/oizqf2/p_how_to_write_a_neurips_paper_3_gearing_up_with/", "author": "evanthebouncy", "subreddit": "r/MachineLearning", "description": "hey guys, I just finished the 3rd installment on the blog series \"how to write a neurips paper\" where I go into behind the scenes of how to conceive a research idea to presenting it to others in seminars.\n\nin this chapter, we take a look on some ways one can acquire a piece of missing knowledge required to do original research under the lens of search problems. \n\n[https://evanthebouncy.medium.com/how-to-write-a-neurips-paper-3-855d64ffb7f0](https://evanthebouncy.medium.com/how-to-write-a-neurips-paper-3-855d64ffb7f0)\n\nI can take some questions here, and hope you enjoy the read ! !\n\n\\--evan"}, {"id": "oiowkw", "title": "[R] What are the most important differences between classical statistics and ML?", "score": 0, "url": "https://www.reddit.com/r/MachineLearning/comments/oiowkw/r_what_are_the_most_important_differences_between/", "author": "Saki-San", "subreddit": "r/MachineLearning", "description": "I'm writing on the relationship between explanation and prediction in the context of ML (from the perspective of philosophy of science), and I'm currently reading some literature on the difference between traditional statistical models and ML models (in some texts this is termed as 'classical statistics' vs. 'statistical learning'). Literature examples:\n\n[https://www.frontiersin.org/articles/10.3389/fnins.2017.00543/full](https://www.frontiersin.org/articles/10.3389/fnins.2017.00543/full)\n\n[https://projecteuclid.org/journals/statistical-science/volume-25/issue-3/To-Explain-or-to-Predict/10.1214/10-STS330.full](https://projecteuclid.org/journals/statistical-science/volume-25/issue-3/To-Explain-or-to-Predict/10.1214/10-STS330.full)\n\nThe differences between them seem to be that:\n\n**Traditional** statistical models are more theory-driven, mostly fashioned for problems with small samples, more directed at hypothesis testing than ML;\n\n**ML** models are more flexible, data-driven, in the sense that they make minimal assumptions about the data-generating (or 'target') systems. Also, despite high prediction accuracy, they may be difficult to directly relate to existing theories or knowledge.\n\nIf I understand this distinction correctly, the typical examples for each type of models are:\n\n**Traditional statistics**: linear-regression-like models, regression-type analyses and null-hypothesis testing using t-test and ANOVA;\n\n**ML**: support vector machines, random-forest algorithms, DNNs Bonus question - do all these ML subtypes work 'semi-autonomously', in the sense that the model itself adjusts weights of the parameters through an automated process rather than the engineer doing it 'by-hand'?\n\nI come from a philosophical background, so I cannot completely track the technical descriptions, but am I on the right track here? Can anyone confirm whether I understand these differences correctly? I am aware that there are grey areas or some differences in degrees or merely in how a model is used, and also I see that \"no single criterion exists that alone allows for a clear-cut distinction between classical statistics and statistical learning in all cases\", but are there also clear examples that fall only under one of these two types?If you have any better examples for each of these two types of models, I would be grateful learn about them as well.\n\nFinally, if you have theoretical expertise in the field, how would you define theoretically the difference between these types of models? Is there anything worth mentioning that I'm missing here?  \n\n\nEDIT: I don't understand why the post is getting downvoted. If I tagged it incorrectly, I'd appreciate if someone would let me know so I can correct it.  \nIf there is a problem with my question, I'd also appreciate if someone would help me understand what the problem is, or refer me to more appropriate sources.  \nI am just trying to learn something here..."}, {"id": "oiortw", "title": "[D] set matching", "score": 0, "url": "https://www.reddit.com/r/MachineLearning/comments/oiortw/d_set_matching/", "author": "flow_smith", "subreddit": "r/MachineLearning", "description": "Hello,\n\nLet's say we have a set of words S = {w1, w2, w3...w\\_n} detected on one image where every word has it's relative size (bounding box) in the image p\\_n. P = {p1, p2, p3, ..., p\\_n} .  \n\n\non the other hand we have a Database DB = {S1, S2, ..., SN} where also every S\\_i is a set of words with their corresponding sizes.  \n\n\nThese words has a lot of spelling mistakes, and we can see them as independent, meaning they dont form sentences.  \n\n\nSo the task is to do set matching, and retrieve the best candidate S\\_j that matches S. \n\nI was wondering if anyone has any guidance to a paper or an interesting research work please ?  \n\n\nBottlenecks: outliers -> a lot of non necessary words can be on S that are not on the ground-truth and so they hurt the matching.  \n\n\nBest Regards,"}, {"id": "oiijxc", "title": "[D] Does anyone know of a good 2D navigation environment?", "score": 3, "url": "https://www.reddit.com/r/MachineLearning/comments/oiijxc/d_does_anyone_know_of_a_good_2d_navigation/", "author": "Impallion", "subreddit": "r/MachineLearning", "description": "I was looking into using the 2D navigation environment extension for OpenAI Gym from McGill, but it seems out of date and I couldn't get it to work. \n\nDoes anyone know of any environments for navigation tasks? Thank you!\n\nSpecifically, I am looking for a continuous control environment with ray line-of-sight detection with modifiable map/world for obstacles"}, {"id": "oimzi1", "title": "[R] Adversarial Mixture Density Networks: Learning to Drive Safely from Collision Data", "score": 0, "url": "https://arxiv.org/abs/2107.04485", "author": "Caffeinated-Scholar", "subreddit": "r/MachineLearning", "description": ""}, {"id": "ohxnx7", "title": "[R] Towards Fast, Accurate and Stable 3D Dense Face Alignment", "score": 99, "url": "https://v.redd.it/d7d19038fia71", "author": "Illustrious_Row_9971", "subreddit": "r/MachineLearning", "description": ""}, {"id": "ohk6b7", "title": "[R] RMA algorithm: Robots that learn to adapt instantly to changing real-world conditions (link in comments)", "score": 1083, "url": "https://v.redd.it/xok1j6cofea71", "author": "pathak22", "subreddit": "r/MachineLearning", "description": ""}, {"id": "oilvqw", "title": "[Discussion] Is there an AI that can detect and warn phobia/sensitivity-triggering content in videos?", "score": 0, "url": "https://www.reddit.com/r/MachineLearning/comments/oilvqw/discussion_is_there_an_ai_that_can_detect_and/", "author": "Neggy5", "subreddit": "r/MachineLearning", "description": "I think this would be a revolutionary project for those with severe phobias, autism and sensory triggers as someone that experiences all 3.\n\nKinda like a scanner that finds timestamps where triggering content appears, whether audio or visual. For example, if someone has a fear of spiders and they scan a video using the AI, timestamps would be highlighted with a warning of when a spider would appear.\n\nWith all information going to video, inclusivity and accessibility is clearly hindered due to phobias and sensory triggers. It\u2019s probably the only way I can watch videos myself as I never use Youtube for this reason. Thoughts? Can this be made at this current time?"}, {"id": "ohrq44", "title": "[R] Aggregating Nested Transformers", "score": 10, "url": "https://www.youtube.com/watch?v=XJE2CY1p0EM", "author": "_4lexander_", "subreddit": "r/MachineLearning", "description": ""}, {"id": "oi53e3", "title": "[R] Hidden Markov Model primary Probability", "score": 0, "url": "https://www.reddit.com/r/MachineLearning/comments/oi53e3/r_hidden_markov_model_primary_probability/", "author": "bluemoon77777", "subreddit": "r/MachineLearning", "description": "Hey, is there any way to calculate the primary probability for a Hidden Markov Model if it\u2018s not given?"}, {"id": "oi1xlh", "title": "[P] Hands-on Live Workshop: Methods for Data Selection in Autonomous Vehicles - Link in comments", "score": 0, "url": "https://i.redd.it/sy0sqs2u5ka71.png", "author": "pinter69", "subreddit": "r/MachineLearning", "description": ""}, {"id": "ohxday", "title": "[P] AIeyes: Walking simulator where the world is seen through a real time neural net", "score": 3, "url": "https://www.reddit.com/r/MachineLearning/comments/ohxday/p_aieyes_walking_simulator_where_the_world_is/", "author": "regalalgorithm", "subreddit": "r/MachineLearning", "description": "[Really neat indie game AIeyes](https://tmdev.itch.io/aieyes) just came out on [itch.io](https://itch.io) \\-\"Walking simulator in a city where the world is seen through a real time neural network, which is fed simple colored blocks that  are translated to textures and forms.\"\n\nBasically walk within a procedural world that is renders inputs to pix2pix, the output of which is then shown to the player. Not complicated, but quite neat! What do you think?"}, {"id": "ohg414", "title": "[D] Are other fields of Computer Science actually better than Machine Learning?", "score": 60, "url": "https://www.reddit.com/r/MachineLearning/comments/ohg414/d_are_other_fields_of_computer_science_actually/", "author": "optimized-adam", "subreddit": "r/MachineLearning", "description": "I\u2018m a CS master student close to finishing my degree and I have mostly done courses in the ML field. After I finish my masters, I want to do a PhD. Right now, I\u2019m leaning towards staying in the ML field but when reading this sub, there\u2019s a lot of discontentment with the academic process. \n\nSo here\u2019s my question: are other CS fields (like data engineering, databases, systems engineering etc.) actually better in this regard?\n\nAs an side note, I\u2019m also wondering about career prospects. It seems that the ML space is getting very saturated because of the hype and I\u2019m not sure if, speaking strictly career-wise, other fields are a better choice.\n\nHappy to hear your thoughts!"}, {"id": "ohtso3", "title": "[D] What are we allowed to do during the response period?", "score": 6, "url": "https://www.reddit.com/r/MachineLearning/comments/ohtso3/d_what_are_we_allowed_to_do_during_the_response/", "author": "AICoderGamer", "subreddit": "r/MachineLearning", "description": "I developed a novel NLP algorithm for work that in testing does really well. I decided to also submit it for review for publication in EMNLP 2021. I am the only author and this is my first time trying to publish something, so forgive me if this is well-known information.\n\nReviews just came out today, and my review scores are 2.5, 3, and 3.5. While not amazing, they all seem to mention a lack of comparison to a particular set of algorithms in their reviews. None of them seem to have issues with the actual content of the publication, with them finding it novel, liking the theory, and are impressed with the results.\n\nI didn't reference this set of algorithms as they would fail in the most important important metric I assess: computation time. I thought this was an obvious result, but it clearly is not. I feel this is definitely something I can add baselines for and compare against my algorithm and do well against.\n\nSince the main issue with my submission is something that I can easily account for and change, I wanted to know if it is allowed for me to disclose these additional results.\n\nIf not, how would you propose I proceed to convince the reviewers to bump up my scores? I feel like they would all be willing to accept the submission if I add these additional baselines.\n\nAny guidance on this matter will be greatly appreciated!"}, {"id": "ohr572", "title": "[D] Is gradual unfreezing still used?", "score": 5, "url": "https://www.reddit.com/r/MachineLearning/comments/ohr572/d_is_gradual_unfreezing_still_used/", "author": "rajicon17", "subreddit": "r/MachineLearning", "description": "Gradual Unfreezing is an interesting transfer learning technique, relying on unfreezing later layers of the model and gradually unfreezing more and more, introduced in the ULMFit model.  I always thought this was an interesting idea, but I feel like I haven't seen it much lately.  Is it still used at all?"}, {"id": "ohiqmb", "title": "[R] One sentence highlight for every ICML-2021 Paper", "score": 18, "url": "https://www.reddit.com/r/MachineLearning/comments/ohiqmb/r_one_sentence_highlight_for_every_icml2021_paper/", "author": "biandangou", "subreddit": "r/MachineLearning", "description": "Here is the list of all 1183 ICML (international conference on machine learning) papers, and a one sentence highlight for each of them. ICML2021 will be held online from July 18.\n\n[https://www.paperdigest.org/2021/07/icml-2021-highlights/](https://www.paperdigest.org/2021/07/icml-2021-highlights/)"}, {"id": "oh9fsn", "title": "[D] Are there any research groups in schools working on machine learning and climate change?", "score": 90, "url": "https://www.reddit.com/r/MachineLearning/comments/oh9fsn/d_are_there_any_research_groups_in_schools/", "author": "Seankala", "subreddit": "r/MachineLearning", "description": "I'm aware that there are some professors here and there who have interests in climate change (e.g., Yoshua Bengio) but I'm not sure if I've seen an academic group that works specifically on applying ML to climate change. Is there anywhere I should start looking? Any tips are appreciated, thanks."}, {"id": "oi44xz", "title": "[D] How does the mind work (as an algorithm)?", "score": 0, "url": "https://www.reddit.com/r/MachineLearning/comments/oi44xz/d_how_does_the_mind_work_as_an_algorithm/", "author": "Rina-Panigrahy", "subreddit": "r/MachineLearning", "description": "What is the right logical architecture that captures the essential capabilities of the mind? Is it logically equivalent to some deep learned system perhaps augmented with some kind of memory or knowledge graph and consists of several deep learned modules that interact with each other?\n\nIt would be nice to have a discussion about these questions -- please join the following google-meet link for an informal discussion (there are no planned participants yet and anyone can chime in) We could touch upon basic questions  around an architecture for the mind (such as [https://docs.google.com/document/d/1tS-B2BTXixqucUPzLm5o6VnbvTY5EMbxzr\\_ox5RF3zQ/edit](https://docs.google.com/document/d/1tS-B2BTXixqucUPzLm5o6VnbvTY5EMbxzr_ox5RF3zQ/edit) discussed in this panel discussion: [https://www.youtube.com/watch?v=g5DGBWjiULQ&t=6496s](https://www.youtube.com/watch?v=g5DGBWjiULQ&t=6496s)) and also use inspiration from psychology, neuroscience and may touch upon philosophical issues such as \"what is the mind?\"\n\nIf you are interested in joining please email me at [rinapy@gmail.com](mailto:rinapy@gmail.com) with subject \"How does the mind work\" so I have a sense of how many people will join.\n\nDiscussion: How does the mind work as an algorithm?\n\nWednesday, July 21 \u00b7 5:00 \u2013 6:00pm (PST) ([Google-Calendar-link](https://calendar.google.com/event?action=TEMPLATE&tmeid=NzRxbWVhcjhpbnJwZnM0cWRmcm1wN290YmwgcmluYXB5QG0&tmsrc=rinapy%40gmail.com))\n\nGoogle Meet joining info\n\nVideo call link: [https://meet.google.com/fdw-qmsa-ndx](https://meet.google.com/fdw-qmsa-ndx)\n\nThanks, Best,\n\nRina Panigrahy ([http://theory.stanford.edu/\\~rinap/](http://theory.stanford.edu/~rinap/))"}, {"id": "ohiw3b", "title": "[R] Researchers From University of Sydney And Japan\u2019s NIMS Have Discovered A Way To Create Artificial Networks Of Nanowires That Can Be Tuned In The Same Brain-Like Manner When Electrically Stimulated", "score": 9, "url": "https://www.reddit.com/r/MachineLearning/comments/ohiw3b/r_researchers_from_university_of_sydney_and/", "author": "techsucker", "subreddit": "r/MachineLearning", "description": "A team of researchers from the University of Sydney and Japan\u2019s National Institute for Material Science have demonstrated that they can utilize a random network of nanowires to mimic the structure as well as the dynamics of the brain to solve simple tasks involving processing.\n\nDeep neural networks already resemble one aspect of the brain that is a highly interconnected network of neurons. However, artificial neurons behave in a very different manner as compared to biological ones since they only perform computations. In the human brain, neurons can also remember their previous activity, which dramatically influences their future behavior. The in-built memory is an essential aspect of how the brain processes information. A significant strand in neuromorphic engineering focuses on trying to recreate this functionality.\n\nSummary: [https://www.marktechpost.com/2021/07/10/researchers-from-university-of-sydney-and-japans-nims-have-discovered-a-way-to-create-artificial-networks-of-nanowires-that-can-be-tuned-in-the-same-brain-like-manner-when-electrically-stimul/](https://www.marktechpost.com/2021/07/10/researchers-from-university-of-sydney-and-japans-nims-have-discovered-a-way-to-create-artificial-networks-of-nanowires-that-can-be-tuned-in-the-same-brain-like-manner-when-electrically-stimul/) \n\nPaper: https://www.nature.com/articles/s41467-021-24260-z"}, {"id": "ohew0u", "title": "[Discussion] Confused by the timeline of backpropagation and MLP", "score": 22, "url": "https://www.reddit.com/r/MachineLearning/comments/ohew0u/discussion_confused_by_the_timeline_of/", "author": "CharlieLam0615", "subreddit": "r/MachineLearning", "description": "I've been reading some history on Deep Learning. However, the timeline of backpropagation and multi-layer perceptron is a bit confusing.\n\nBackpropagation was proposed by Werbos in 1975 in his PhD thesis which enabled the practical training of multi-layer neural networks, but multi-layer perceptron didn't exist until 1986 in the work of Rumelhart, Hinton, and Williams.\n\nIf MLP didn't even exist at the time of 1975, then how come backpropagation was invented to facilitate the training of MLP?"}, {"id": "ohh04v", "title": "[P] FEDOT - AutoML framework for composite pipelines", "score": 7, "url": "https://www.reddit.com/r/MachineLearning/comments/ohh04v/p_fedot_automl_framework_for_composite_pipelines/", "author": "nikolay_o_nikitin", "subreddit": "r/MachineLearning", "description": "Hi! I want to discuss the academic project FEDOT ([https://github.com/nccr-itmo/FEDOT](https://github.com/nccr-itmo/FEDOT)) that is devoted to the evolutionary AutoML for composite pipelines. I am one of the developers of this framework and hope to obtain some feedback on this solution and share our experience.\n\nFEDOT is an open-source framework for automated modeling and machine learning (AutoML). It can build custom modeling pipelines for different real-world processes in an automated way using an evolutionary approach. FEDOT supports classification (binary and multiclass), regression, and time series forecasting tasks, as well as different data types and multi-modal cases. Also, sensitivity analysis of the pipelines, custom pipelines design as the initial assumption of optimization, domain-specific objective functions, and other interesting features are implemented.  \n\n\nThe framework is actively developing, so any issues, pull requests and comments are welcome on GitHub.  \n\n\nLatest preprint with description of main concepts : [https://arxiv.org/abs/2106.15397](https://arxiv.org/abs/2106.15397)  \nIntro: [https://www.youtube.com/watch?v=RjbuV6i6de4](https://www.youtube.com/watch?v=RjbuV6i6de4)\n\nAlso, several posts on TDS are available if you want to go deeper:\n\n\\- [How AutoML helps to create composite AI?](https://towardsdatascience.com/how-automl-helps-to-create-composite-ai-f09e05287563)  \n\\- [AutoML for time series: definitely a good idea](https://towardsdatascience.com/automl-for-time-series-definitely-a-good-idea-c51d39b2b3f)  \n\\- [AutoML for time series: advanced approaches with FEDOT framework](https://towardsdatascience.com/automl-for-time-series-advanced-approaches-with-fedot-framework-4f9d8ea3382c)"}, {"id": "ohqp3r", "title": "[D] Detecting rather a human wrote an email intended just for the recipient.", "score": 1, "url": "https://www.reddit.com/r/MachineLearning/comments/ohqp3r/d_detecting_rather_a_human_wrote_an_email/", "author": "Panzercannon03", "subreddit": "r/MachineLearning", "description": "As many of you have probably  experienced, we have WAY too many emails that are either a form email sent to many people, an automated email from an alert system, marketing emails, and emails that basically  serve as notifications.\n\nWith all of this noise what I really really wish for is a way to detect (via machine learning) rather an email is actually  written  by a human specifically meant for me. Obviously  no one can know for sure but does anyone  here have any ideas on how we can at least have better confidence  that an email was written  by a human?"}, {"id": "ohqe76", "title": "[R] Cornell and Harvard University Researchers Develops Correlation Convolutional Neural Networks (CCNN): To Determine Which Correlations Are Most Important", "score": 2, "url": "https://www.reddit.com/r/MachineLearning/comments/ohqe76/r_cornell_and_harvard_university_researchers/", "author": "techsucker", "subreddit": "r/MachineLearning", "description": "A team of researchers from Cornell and Harvard University introduces a novel approach to parse quantum matter and make crucial data distinctions. This proposed technique will enable researchers to decipher the most perplexing phenomena in the subatomic realm.\n\nIn their paper, \u201cCorrelator Convolutional Neural Networks as an Interpretable Architecture for Image-like Quantum Matter Data,\u201d the team discusses ways to extract new information about quantum systems from snapshots of image-like data. They are now thus developing ML tools to identify relationships between microscopic properties in data that would otherwise be impossible to determine at that scale.\n\nSummary: [https://www.marktechpost.com/2021/07/10/cornell-and-harvard-university-researchers-develops-correlation-convolutional-neural-networks-ccnn-to-determine-which-correlations-are-most-important/](https://www.marktechpost.com/2021/07/10/cornell-and-harvard-university-researchers-develops-correlation-convolutional-neural-networks-ccnn-to-determine-which-correlations-are-most-important/) \n\nPaper: https://www.nature.com/articles/s41467-021-23952-w.pdf"}, {"id": "ohg1dw", "title": "[D] Gradio vs Streamlit", "score": 3, "url": "https://www.reddit.com/r/MachineLearning/comments/ohg1dw/d_gradio_vs_streamlit/", "author": "IborkedyourGPU", "subreddit": "r/MachineLearning", "description": "Gradio is getting quite marketed quite a bit on social media, but I see much less attention for Streamlit, which seems to me to be quite similar (and possibly more flexible). Do you have any experience using them? What are the pros and cons of each, in your opinion?\n\nPS I heard good things about [opyrator](https://github.com/ml-tooling/opyrator) (based on Streamlit and Pydantic), but it's obviously a less mature tool."}, {"id": "ohhvzf", "title": "[D] Am I understanding correctly how the Performer Transformer variant should be used for time series data?", "score": 3, "url": "https://www.reddit.com/r/MachineLearning/comments/ohhvzf/d_am_i_understanding_correctly_how_the_performer/", "author": "EdvardDashD", "subreddit": "r/MachineLearning", "description": "Hey all! Sorry in advance for any noobishness in what I'm asking, I'm still fairly new to machine learning.\n\nI'd like to use [Performer networks](https://github.com/google-research/google-research/tree/master/performer/fast_attention) to do time series analysis. While going through the implementation provided, I was having a hard time understanding exactly how the data that gets passed to it should be structured.\n\nAs an example, let's say the input has a shape of (240, 30). 240 being the number of time series steps and 30 being the number of features for each step. In this case, `attention_head_size` would be 30, and `attention_head_count` would be 16 since I want that many attention heads, right? And then the input data needs to be passed with a shape of (240, 480)? 480 being 30 \\* 16. If that's the case, does that mean that I need to duplicate the 30 features 16 times for each time series step so that the tensor is the appropriate size?\n\nThanks in advance for any insight you can provide!"}, {"id": "ogydhz", "title": "[N] Baidu AI Researchers Introduce An Autonomous Excavator System (AES) To Perform Material Loading Tasks Without Any Human Intervention", "score": 130, "url": "https://www.reddit.com/r/MachineLearning/comments/ogydhz/n_baidu_ai_researchers_introduce_an_autonomous/", "author": "techsucker", "subreddit": "r/MachineLearning", "description": "Baidu Research Robotics and RAL (Auto-Driving Lab), along with the University of Maryland, College Park, have introduced a new benchmark in robotics. [An autonomous excavator system (AES)](http://research.baidu.com/Blog/index-view?id=159) has been manufactured that is claimed to give excellent results. The AES would be able to:\n\n* Work efficiently without any form of human intervention.\n* Would not compromise on the quality of services; that is, the performance levels have been found to mimic those of an experienced human worker closely.\n* It can work for more than 24 hours and has been tested for the same in real-world scenarios (more than ten different scenarios have been tested to understand the working properly).\n* It increases the productivity of the work manifold without any hazards attached.\n\nFull Story: [https://www.marktechpost.com/2021/07/09/baidu-ai-researchers-introduce-an-autonomous-excavator-system-aes-to-perform-material-loading-tasks-without-any-human-intervention/](https://www.marktechpost.com/2021/07/09/baidu-ai-researchers-introduce-an-autonomous-excavator-system-aes-to-perform-material-loading-tasks-without-any-human-intervention/) \n\nPaper: [https://robotics.sciencemag.org/content/6/55/eabc3164](https://robotics.sciencemag.org/content/6/55/eabc3164)"}, {"id": "ohj4va", "title": "[R] An MLP can classify nodes better than GNNs even without knowing adjacent nodes. Code is available at https://github.com/cf020031308/LinkDist", "score": 0, "url": "https://www.reddit.com/gallery/ohj4va", "author": "cf020031308", "subreddit": "r/MachineLearning", "description": ""}, {"id": "ohmlpf", "title": "[D] Best algorithms for video recognition", "score": 0, "url": "https://www.reddit.com/r/MachineLearning/comments/ohmlpf/d_best_algorithms_for_video_recognition/", "author": "willspag", "subreddit": "r/MachineLearning", "description": "I\u2019m looking into building a neural network to recognize people based on posture/body movements, and wanted to get some feedback on what algorithms would be best for this.\n\nFor backstory, a gas station cashier in my town was murdered, and there\u2019s been a case going on for months to find the killer with no progress. They caught it on camera, but the suspect was fully covered with gloves, a mask, and a hoodie. Given that murders are rarely every random, the suspect has most likely been to this gas station before. In an attempt to use my AI skills for good, I\u2019m looking into what it would take to build an AI to recognize people by their posture/body motion to help the police solve the case.\n\nI haven\u2019t worked with much video recognition in the past, but here\u2019s my initial thoughts on how it could be done:\n\nUse the YOLO algorithm to create bounding boxes of people at the cash register, linking their identities based on the credit card timestamps. Then, train an algorithm similar to facial recognition, but I\u2019m not sure what\u2019s the best approach to handle the temporal data. Off the top of my head, here\u2019s my first ideas:\n1) 3D CNN, using the time as the third dimension\n2) 2D CNN using a rolling average of the embeddings\n3) Feed a 2D CNN embedding into an LSTM\n\nI\u2019d love some recommendations/papers from anyone that\u2019s worked with anything similar before. Thanks!"}, {"id": "ohiayp", "title": "[D] What ML topic is this: Using autoencoders to learn feature maps and then getting the encoder's backbone for object detection?", "score": 1, "url": "https://www.reddit.com/r/MachineLearning/comments/ohiayp/d_what_ml_topic_is_this_using_autoencoders_to/", "author": "sarmientoj24", "subreddit": "r/MachineLearning", "description": "If I have an unlabelled ***dataset A*** on a specific domain, I can use an ***autoencoder*** with a backbone, say, **ResNet-50** on the encoder to learn feature maps for dataset A on the domain.\n\nSay, I took out the same (pretrained) **ResNet-50** from the ***autoencoder***  and use it as the pre-trained model backbone for my **object detection** task with **dataset B** on the same domain.\n\nWhat ML topic is this? I was thinking **Representation Learning** but I feel like there is a more suitable topic. How about **domain adaptation?**"}, {"id": "oh61yh", "title": "Could AI be used to clean up and enhance lo-fi audio transforming it to hi-fi audio? [D]", "score": 6, "url": "https://www.reddit.com/r/MachineLearning/comments/oh61yh/could_ai_be_used_to_clean_up_and_enhance_lofi/", "author": "jazmaan", "subreddit": "r/MachineLearning", "description": "Would it be possible to train an AI to transform lo-fi audience cassette recordings into high fidelity recordings? Suppose you fed the AI a few seconds of a well-recorded hi-fi soundboard of say Jimi Hendrix at the Fillmore East. And suppose you had a lo-fi audience cassette recording of exactly the same performance. Could you give the AI the goal of \"Teach yourself to make this lo-fi audience recording sound like that hi-fi soundboard?\" And if you trained the AI on a sufficient number of matching clips might it eventually be able to transform a lo-fi sounding Jimi Hendrix bootleg into something that sounds like a soundboard without even needing a matching hi-fi sample for comparison?"}, {"id": "ogyjdq", "title": "[R] Back to School: MIT & UWaterloo Model Gets an \u2018A\u2019 on ML Course Problems", "score": 13, "url": "https://www.reddit.com/r/MachineLearning/comments/ogyjdq/r_back_to_school_mit_uwaterloo_model_gets_an_a_on/", "author": "Yuqing7", "subreddit": "r/MachineLearning", "description": "MIT and University of Waterloo researchers propose a machine learning model that outperforms the average student on problems from MIT\u2019s 6.036 Introduction to Machine Learning course. \n\nHere is a quick read: Back to School: [MIT & UWaterloo Model Gets an \u2018A\u2019 on ML Course Problems.](https://syncedreview.com/2021/07/09/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-58/)\n\nThe paper *Solving Machine Learning Problems* is on [arXiv](https://arxiv.org/abs/2107.01238)."}, {"id": "ogliui", "title": "[D] EleutherAI One Year Retrospective", "score": 164, "url": "https://www.reddit.com/r/MachineLearning/comments/ogliui/d_eleutherai_one_year_retrospective/", "author": "regalalgorithm", "subreddit": "r/MachineLearning", "description": "EleutherAI just released[\" What A Long, Strange Trip It's Been: EleutherAI One Year Retrospective \"](https://blog.eleuther.ai/year-one/)\n\nIt's quite a fun read, and taught me a good deal about the ambitions of EleutherAI. And it made me check out their Discord! What do people on here think about the retrospective / their efforts?"}, {"id": "ogznxu", "title": "[R] Can You Learn an Algorithm? Generalizing from Easy to Hard Problems with Recurrent Networks", "score": 12, "url": "https://arxiv.org/abs/2106.04537", "author": "hardmaru", "subreddit": "r/MachineLearning", "description": ""}, {"id": "ohc7r2", "title": "[N] Google Cloud Announces Public Review of New Anomaly Detection Capabilities In BigQuery ML That Leverage Unsupervised Machine Learning To Detect Anomalies Without Needing Labeled Data", "score": 0, "url": "https://www.reddit.com/r/MachineLearning/comments/ohc7r2/n_google_cloud_announces_public_review_of_new/", "author": "techsucker", "subreddit": "r/MachineLearning", "description": "One of the critical challenges in anomaly detection that many organizations face is that it can be challenging to define an anomaly.\n\nGoogle Cloud has recently announced the public preview of new anomaly detection capabilities in BigQuery ML that utilizes unsupervised machine learning to help users detect anomalies without needing the labeled data. Therefore, users can now detect anomalies in training data or new input data using a new ML.DETECT\\_ANOMALIES function with the models like Autoencoder model, K-means model, and ARIMA\\_PLUS time series model.\n\nSummary: [https://www.marktechpost.com/2021/07/09/google-cloud-announces-public-review-of-new-anomaly-detection-capabilities-in-bigquery-ml-that-leverage-unsupervised-machine-learning-to-detect-anomalies-without-needing-labeled-data/](https://www.marktechpost.com/2021/07/09/google-cloud-announces-public-review-of-new-anomaly-detection-capabilities-in-bigquery-ml-that-leverage-unsupervised-machine-learning-to-detect-anomalies-without-needing-labeled-data/)"}, {"id": "ogxnku", "title": "[R] Self-supervised Video Representation Learning with Cross-Stream Prototypical Contrasting (SOTA on UCF101 and HMDB51 video retrieval)", "score": 11, "url": "https://www.reddit.com/r/MachineLearning/comments/ogxnku/r_selfsupervised_video_representation_learning/", "author": "jonestown_aloha", "subreddit": "r/MachineLearning", "description": "https://arxiv.org/pdf/2106.10137.pdf\n\ncode is on github: https://github.com/martinetoering/ViCC"}, {"id": "oh1ed6", "title": "[R] Simple Correction for Label Smoothing with Multi-Label Classification", "score": 0, "url": "https://www.reddit.com/r/MachineLearning/comments/oh1ed6/r_simple_correction_for_label_smoothing_with/", "author": "SnarkyVelociraptor", "subreddit": "r/MachineLearning", "description": "I was browsing around for a simple formula for doing label smoothing with multiple labels and it seems like people were having trouble coming up with one (judging from lack of answers here [https://stats.stackexchange.com/questions/483597/is-there-a-label-smoothing-version-for-multi-label-classification](https://stats.stackexchange.com/questions/483597/is-there-a-label-smoothing-version-for-multi-label-classification), and discussion [https://forums.fast.ai/t/can-label-smoothing-be-used-for-multi-label-images/46673/4](https://forums.fast.ai/t/can-label-smoothing-be-used-for-multi-label-images/46673/4)). \n\n&#x200B;\n\nI'm sure this formula has been seen somewhere in the literature already, but for those who (like me) can't find it, here it is.\n\nLet y be a vector of labels (so, for example: y = \\[1 0 0\\]). Then the length of the label vector is denoted by N (so here N = 3) and we can index a generic entry in the vector as y\\_n. \n\nThe original formula is this: y\\_ls = y\\_n (1-a) + a/N (where y\\_ls is the i'th smoothed label). \n\nIf you apply this formula to something with multiple labels (say, y = \\[1 0 1\\]) then overall \"probability mass\" of the smoothed vector (i.e. sum(y)) will not equal the probability mass of the original vector (i.e. sum(y) != sum(y\\_ls)). \n\nJust change that a/N term to (a \\* sum(y))/N instead and you're done. The final formula is \n\n**y\\_ls = y\\_n (1-a) + (a \\* sum(y))/N.** \n\nWhen you only have a single label, sum(y) = 1 and so it reduces to the original case. \n\n&#x200B;\n\n(Apologies for the lack of LaTeX, not sure if that's available on reddit)."}, {"id": "og48q2", "title": "[D] AI ethics research is unethical", "score": 354, "url": "https://www.reddit.com/r/MachineLearning/comments/og48q2/d_ai_ethics_research_is_unethical/", "author": "yusuf-bengio", "subreddit": "r/MachineLearning", "description": "I have been observing AI/ML ethics research and discussions for over a year now and I have come to the conclusion that most work conducted in this area is deeply unethical.\n\nAll entities, let it be companies, institutions, and individuals, are subject to inherent **conflict-of-interests** that render any discussion meaningless.\n\nAI/ML ethics does not generate any profits, making funding source for research or even ethics policies scarce. As a result, there are only a handful of entities working on this domain, which in turn have full control over how the entire field is moving. For instance, the ethics PC of NeurIPS 2020 was a single person (a British man) employed by DeepMind, making him/DM the ultimate arbiter of truth on AI ethics.\n\nAI/ML ethics discussions are centered on domestic problems of the US. For instance, computer vision is becoming dominated by Chinese researchers (just look at this year's CVPR papers), whose approach to ethical values completely differ from the first. However, their views (and those of people from many other demographic groups) are not reflected by any AI/ML ethics rulings.\n\nFinally, the way Timnit Gebru was treated by Google before and after she was kicked out is just unbearable for me. First of all, her paper is not a big deal, her claims are valid and do not threaten Google in any way. The way Google overreacted and even [published a counter paper](https://arxiv.org/pdf/2104.10350v1.pdf) reveals that the conflict-of-interest I mentioned above runs much much deeper than I previously thought.\n\nNowadays when we see an AI/ML ethics paper funded by a company, we have to assume it went through several layers of filtering and censoring, putting it on a trustworthiness level on par with CCP propaganda. On top of that, even for papers without any company funding, we have to assume that a paper only resembles the views of a very tiny subset of the global population, because as I wrote, most demographical groups do not have access to funding for this topic and are therefore disregarded.\n\n**TL;DL** an AI/ML ethics paper either reflects a company's interest or the beliefs of a very tiny subset of the earth's population\n\n&#x200B;\n\nI would like to hear your thought on this topic"}, {"id": "ogq4vv", "title": "[D] BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer (Research Paper Summary)", "score": 6, "url": "https://www.reddit.com/r/MachineLearning/comments/ogq4vv/d_bert4rec_sequential_recommendation_with/", "author": "prakhar21", "subreddit": "r/MachineLearning", "description": "This paper from Alibaba Group introduces BERT for doing sequential recommendations. \ud83d\udd25 \n\nThe goal of  sequential recommender system is to recommend the next item that a user I likely to buy based on the historical transactions. \ud83e\udd29\n\nInteresting? Then checkout, Paper Summary: https://youtu.be/4pYHEzwTa78"}, {"id": "ogpngp", "title": "[R] Astrology Dataset for Correlation Research", "score": 7, "url": "https://www.reddit.com/r/MachineLearning/comments/ogpngp/r_astrology_dataset_for_correlation_research/", "author": "Meow7788", "subreddit": "r/MachineLearning", "description": "Dear Redditors,\n\nI took the time to put together this dataset listing all the dates the planet Mercury was in retrograde motion from 1990 to 2020.\nWhether you want to prove or disprove the claims of astrology this is an interesting dataset to start from.\n\nAstrology claims that a Retrograde Mercury increases the probability of travel delays, crimes, computer malfunctions and many more.\n\nThis is a dataset compiled to inspire statistics and Machine Learning related research especially when it comes to correlation studies. \nI hope you make some awesome projects with this.\n\n*Dataset Link:*\nhttps://github.com/Paris778/Retrograde-Mercury-Dates-Dataset-1990-2020"}, {"id": "oh1hc1", "title": "[Research] Input Arbitrary PDE -> Output Approximate Solution", "score": 0, "url": "https://www.reddit.com/r/MachineLearning/comments/oh1hc1/research_input_arbitrary_pde_output_approximate/", "author": "zstreet_42", "subreddit": "r/MachineLearning", "description": "I want to build a code base that takes in a user defined PDE and outputs an approximate simulation of that PDE.  Is there any research that, perhaps, uses machine learning that trains on an arbitrary PDE and outputs a solution at time t?  A very high level coarse solution would suffice, just wondering if there is any work that looks at applied mathematics field at this angle (give arbitrary PDE -> gives approximate solution)."}, {"id": "ogawk9", "title": "[D] if you're an ML researcher, how do you move away from it ?", "score": 49, "url": "https://www.reddit.com/r/MachineLearning/comments/ogawk9/d_if_youre_an_ml_researcher_how_do_you_move_away/", "author": "SaltyStackSmasher", "subreddit": "r/MachineLearning", "description": "I'm a master's student and I'll be graduating soon (defended my thesis today). I worked exclusively on machine learning in my master's but it turns out I don't like it a lot \n\nML research (in my short experience with it) was a lot more competitive with somewhat less impact than I thought it would have. This isn't me being salty, I have some publications at top conferences (NeurIPS, ICLR & ICML) but rather me not finding the \"just get it accepted to conference\" approach at ML \n\nFor some reason, I never had opportunity to explore anything other than ML in my master's and I definitely want to switch sides and try research in theoretical CS/graphics/compilers/computer security \n\nThat's a lot of topics I want to explore, how should I get started with them ? I'm looking to pursue a PhD in a topic I genuinely like and I want to explore the above 4 areas \n\nI started applying to relevant labs in the areas I wanted to explore but none of them seem to take me as an RA / PhD student considering I have absolutely zero experience in anything other ML. This is kinda sad because I'm a CS graduate and I learned basics of all the topics I'm currently interested in in my undergrad and I feel like I could've cleared interviews of some labs if I had done some revision\n\nSo how can I move away from my ML research and explore other topics ? If so, how does such an SoP look like ? I also want to see and use ML in the said topics if possible (I don't hate ML, I have just gone bored with it) \n\nAny tips ? How do I convince admission committee that I can do research in other topics when I have no research background in them ?"}, {"id": "ogu49n", "title": "[R] A review of \"The Values Encoded in Machine Learning Research\"", "score": 0, "url": "https://www.reddit.com/r/MachineLearning/comments/ogu49n/r_a_review_of_the_values_encoded_in_machine/", "author": "Flying_Scholars", "subreddit": "r/MachineLearning", "description": "**Paper:** \"The Values Encoded in Machine Learning Research\"  \n *Abeba Birhane, Pratyusha Kalluri, Dallas Card, William Agnew, Ravit Dotan, Michelle Bao*  \n [https://arxiv.org/abs/2106.15590v1](https://arxiv.org/abs/2106.15590v1)\n\n**TL;DR of the Review:**   Technical objectives typically pursued in ML, such as  performance and efficiency, are commonly believed to be neutral for society. This study powerfully raises awareness on the possibility that  these objectives might in fact be aligned with the further concentration of power by large technology corporations. However, the argumentation  is one-sided and fails to acknowledge the potential benefits for society of these same technical objectives. **Link to full review in the comments.**"}, {"id": "og4rga", "title": "[D] Why aren't workflow management tools used to ensure reproducibility in the ML world?", "score": 41, "url": "https://www.reddit.com/r/MachineLearning/comments/og4rga/d_why_arent_workflow_management_tools_used_to/", "author": "elanmart", "subreddit": "r/MachineLearning", "description": "One can hear a lot about reproducibility crisis in the ML community. \n\nI've been recently discussing this with some co-workers who have bioinformatics / genomics background, and they said that things like [Common Workflow Language](https://www.commonwl.org/) and [Research Objects](https://www.researchobject.org/) are commonly used by research teams in their fields to make sure their work is reliable and reproducible. \n\nDo you have any opinions on why these tools are not adopted by the ML community? \n\nIt seems that they do solve a lot of problems related to reproducibility (like capturing provenance information)."}, {"id": "ogkr24", "title": "[R] AutoFormer: Searching Transformers for Visual Recognition", "score": 2, "url": "https://www.reddit.com/r/MachineLearning/comments/ogkr24/r_autoformer_searching_transformers_for_visual/", "author": "henryacm", "subreddit": "r/MachineLearning", "description": "[https://arxiv.org/abs/2107.00651](https://arxiv.org/abs/2107.00651)"}, {"id": "ogamdv", "title": "[D] CVPR 2021 Best Paper (GIRAFFE) explained: Representing Scenes as Compositional Generative Neural Feature Fields by Michael Niemeyer et al.", "score": 8, "url": "https://www.reddit.com/r/MachineLearning/comments/ogamdv/d_cvpr_2021_best_paper_giraffe_explained/", "author": "KirillTheMunchKing", "subreddit": "r/MachineLearning", "description": "[Multi-object generation](https://i.redd.it/47danckuj0a71.gif)\n\n[Controlled translation](https://i.redd.it/755jlhiij0a71.gif)\n\n[Controlled rotation](https://i.redd.it/8r4av0jqj0a71.gif)\n\nIf you thought GRAF did a good job at 3d-aware image synthesis just wait until you see the samples from this model by Michael Niemeyer and colleagues at the Max Planck Institute. While generating 256x256 resolution images does not sound that impressive in 2021, leveraging knowledge about the 3D nature of real world scenes to explicitly control the position, shape, and appearance of objects on the generated images certainly is exciting. So, did GIRAFFE deservedly win the best paper award at the recent CVPR 2021?  \n\nRead the [full paper digest](https://t.me/casual_gan/63) (reading time \\~5 minutes) to learn about latent object representation that allows for controlled 3d-aware multi-object synthesis (rotation, translation, shape, appearance), and how to combine techniques from neural volume and image rendering to work with 256x256 Neural Feature Fields in a memory constrained setting.\n\nMeanwhile, check out the paper digest poster by [Casual GAN Papers](https://t.me/casual_gan)!\n\n[GIRAFFE](https://preview.redd.it/nkvxhgp6l0a71.png?width=2218&format=png&auto=webp&s=8488d4972d86bfcf4ef597d7f175c93ce4e7319b)\n\n \n\n\\[[Full Explanation Post](https://t.me/casual_gan/63)\\] \\[[Arxiv](http://www.cvlibs.net/publications/Niemeyer2021CVPR.pdf)\\] \\[[Code](https://github.com/autonomousvision/giraffe)\\]\n\nMore recent popular computer vision paper breakdowns:\n\n>\\[[Alias-free GAN](https://t.me/casual_gan/58)\\]  \n>  \n>\\[[GFPGAN](https://t.me/casual_gan/54)\\]  \n>  \n>\\[[GRAF](https://t.me/casual_gan/61)\\]"}, {"id": "ogmzwv", "title": "[D] *Quantitative* evaluation of upscaled images when ground truth is unavailable?", "score": 0, "url": "https://www.reddit.com/r/MachineLearning/comments/ogmzwv/d_quantitative_evaluation_of_upscaled_images_when/", "author": "TheCockatoo", "subreddit": "r/MachineLearning", "description": "Seems unlikely, but thought I would ask.\n\nDataset A has both low-resolution images and their high-resolution targets.\n\nDataset B only has low-resolution images.\n\n**Quantitative** evaluation of A is straightforward (e.g., PSNR, SSIM), but is it possible with B?"}, {"id": "ofwtp2", "title": "[R] Evaluating Large Language Models Trained on Code (paper on OpenAI Codex)", "score": 111, "url": "https://arxiv.org/abs/2107.03374", "author": "tmabraham", "subreddit": "r/MachineLearning", "description": ""}, {"id": "og71zn", "title": "[D] What does it mean for a distribution to induce an ordering on a vector space?", "score": 13, "url": "https://www.reddit.com/r/MachineLearning/comments/og71zn/d_what_does_it_mean_for_a_distribution_to_induce/", "author": "CS_Student95", "subreddit": "r/MachineLearning", "description": "I'm reading [this](https://openreview.net/pdf?id=rJzLciCqKm) paper and at the end of page 1 they say \"We assume that p(o=+1|**x**, y=+1) and p(y=+1|**x**) induce the same ordering on the input space X\"\n\nI understand what these two distributions they mention are describing, but I don't really understand what they mean by them 'inducing an ordering' on the input space. I did some googling to learn about ordering in vector spaces, preorders, partial orders, etc, so I think I have a basic idea of what ordering looks like for a vector space, but I'm really not seeing the connection between probability distributions and this ordering.\n\n&#x200B;\n\nAny thoughts?"}, {"id": "oga27k", "title": "[R] Meet CLIPDraw: Text-to-Drawing Synthesis via Language-Image Encoders Without Model Training", "score": 5, "url": "https://www.reddit.com/r/MachineLearning/comments/oga27k/r_meet_clipdraw_texttodrawing_synthesis_via/", "author": "Yuqing7", "subreddit": "r/MachineLearning", "description": "A research team from Cross Compass Ltd, Massachusetts Institute of Technology, Tokyo Institute of Technology and University of Tokyo presents CLIPDraw, an algorithm that synthesizes drawings based on natural language input without the need for any training. \n\nHere is a quick read: [Meet CLIPDraw: Text-to-Drawing Synthesis via Language-Image Encoders Without Model Training.](https://syncedreview.com/2021/07/08/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-57/)\n\nThe ClipDraw code is available in this [Colab](https://colab.research.google.com/github/kvfrans/clipdraw/blob/main/clipdraw.ipynb) notebook. The paper *CLIPDraw: Exploring Text-to-Drawing Synthesis Through Language-Image Encoders* is on [arXiv](https://arxiv.org/abs/2106.14843)."}, {"id": "og91cq", "title": "[D] Preference of normalisation approach for computing similarity.", "score": 4, "url": "https://www.reddit.com/r/MachineLearning/comments/og91cq/d_preference_of_normalisation_approach_for/", "author": "PaganPasta", "subreddit": "r/MachineLearning", "description": "To understand the notion of similarity or closeness in an embedding space between different samples what is the recommended approach of normalising the feature vectors ?\n\nI am using dot product between 2 vectors to denote similarity. Is doing L2 norm of features a better approach ? Or doing a zero mean unit variance across sample dimension the way to go?\n\nI'd appreciate if you can also provide some reasoning or resource I can I go through.\n\n\nThanks."}, {"id": "ogew3f", "title": "[R] Machine Learning for Fraud Detection in E-Commerce: A Research Agenda", "score": 0, "url": "https://www.reddit.com/r/MachineLearning/comments/ogew3f/r_machine_learning_for_fraud_detection_in/", "author": "ThrowDataScientist", "subreddit": "r/MachineLearning", "description": "**Abstract:** Fraud detection and prevention play an important part in ensuring the sustained operation of any e-commerce business. Machine learning (ML) often plays an important role in these anti-fraud operations, but the organizational context in which these ML models operate cannot be ignored. In this paper, we take an organization-centric view on the topic of fraud detection by formulating an operational model of the anti-fraud departments in e-commerce organizations. We derive 6 research topics and 12 practical challenges for fraud detection from this operational model. We summarize the state of the literature for each research topic, discuss potential solutions to the practical challenges, and identify 22 open research challenges.\n\n**Paper:** [https://arxiv.org/abs/2107.01979](https://arxiv.org/abs/2107.01979)\n\n**Disclaimer:** I am one of the authors of the paper.\n\n**What kind of input we are looking for:** If you work on machine learning for fraud detection, we would love to hear your feedback our your ideas on the organizational model that we put forward in this paper (i.e., Figure 1). Furthermore, we  would love to hear if anyone believes that we might have missed something regarding relevant literature."}, {"id": "og1tci", "title": "[R] A Survey on Data Augmentation for Text Classification", "score": 8, "url": "https://www.reddit.com/r/MachineLearning/comments/og1tci/r_a_survey_on_data_augmentation_for_text/", "author": "Kaesebrot109", "subreddit": "r/MachineLearning", "description": "  [http://arxiv.org/abs/2107.03158](http://arxiv.org/abs/2107.03158)"}, {"id": "ogdf7m", "title": "[N] IBM Open Sources \u2018CodeFlare\u2019, A Machine Learning Framework That Simplifies AI Workflows Onto The Hybrid Cloud", "score": 1, "url": "https://www.reddit.com/r/MachineLearning/comments/ogdf7m/n_ibm_open_sources_codeflare_a_machine_learning/", "author": "techsucker", "subreddit": "r/MachineLearning", "description": "Data and machine-learning analytics are becoming more widespread, but they grow in complexity with larger datasets requiring much time for configuration. Researchers spend less time actually in data science than getting their systems up to date which can prove difficult at times.\n\nIBM has [open-sourced CodeFlare](https://www.research.ibm.com/blog/codeflare-ml-experiments?lnk=ushpv18f2), a machine learning framework that will allow developers to train their models more efficiently onto the hybrid cloud. This new framework is an exciting concept for those who are looking to simplify their workflow and shorten the time it takes. The idea behind this design is that when users have 10,000 work pipelines running, they wait up to 4 hours before receiving a result. While using this new framework, its implementation into these machines will require only 15 minutes.\n\nSummary: [https://www.marktechpost.com/2021/07/08/ibm-open-sources-codeflare-a-machine-learning-framework-that-simplifies-ai-workflows-onto-the-hybrid-cloud/](https://www.marktechpost.com/2021/07/08/ibm-open-sources-codeflare-a-machine-learning-framework-that-simplifies-ai-workflows-onto-the-hybrid-cloud/)"}, {"id": "ofivs2", "title": "[D] Difference between representation vs. latent vs. embedding space", "score": 198, "url": "https://www.reddit.com/r/MachineLearning/comments/ofivs2/d_difference_between_representation_vs_latent_vs/", "author": "mortadelass", "subreddit": "r/MachineLearning", "description": "I am writing a paper and a reviewer pointed out some problems in my wording regarding the differentiation between representation vs. latent vs. embedding space. The more I read, the more I get confused, since I have the overall feeling that very often these words are used as synonyms and I am currently highly unsure on when / how to use each.\n\nMy understanding: let's say we train an encoder (using for example self supervised contrastive learning like SimCLR). If I input a image to this encoder I will get a lower dimensional representation of that image, i.e., its EMBEDDINGS or its LATENT REPRESENTATION in that particular encoder. The space created by all the possible images that the encoder can represent is more generically called the REPRESENTATION SPACE. I can similarly call this the EMBEDDING SPACE to refer to all possible embedded images (i.e. a synonym).\n\nIs something wrong with the above illustration or am I mixing heterogeneous concepts?"}, {"id": "ogcdek", "title": "[D] Clarification regarding Empirical Distribution & its relationship to Training Set", "score": 0, "url": "https://www.reddit.com/r/MachineLearning/comments/ogcdek/d_clarification_regarding_empirical_distribution/", "author": "strategist-roy", "subreddit": "r/MachineLearning", "description": "Section 3.9.5 of Deep Learning book (Goodfellow, Bengio et. al) states:-\n\n[Last paragraph](https://preview.redd.it/ptovhl0e01a71.png?width=971&format=png&auto=webp&s=e82a1818b9cc0d6b6f0e1a3e6ec23e8fdcac28b6)\n\nI have been studying *data-generating distributions* and how ML models train themselves to learn this original distribution, but I have got very much confused with what is the role of an empirical distribution in all this. In short, I don't understand it well enough to explain to someone else? I request the altruist members of this group to kindly elaborate on this concept, and preferably with examples."}, {"id": "og293s", "title": "[D] Questions on Semi-Supervised Learning on Object Detection using Video Footages", "score": 5, "url": "https://www.reddit.com/r/MachineLearning/comments/og293s/d_questions_on_semisupervised_learning_on_object/", "author": "sarmientoj24", "subreddit": "r/MachineLearning", "description": "While there had been advances on the Semi-Supervised Learning area, there are just few in SSL on Object Detection. There is one on Unbiased Teacher for SSL.\n\nI would like to ask for some perspectives on SSL on Object Detection.\n\n**Semi-Supervised Learning in a nutshell**\n\nFrom what I know, SSL utilizes labeled and unlabeled datasets to come up with better results than, just having the labeled datasets. Of course that is an oversimplification of things. But usually, the number of unlabeled datasets outnumber the number of labeled datasets. Most of the time, there are plentiful of unlabeled datasets around as well.\n\n**SSL using Video Footages**\n\nOne of the most tedious process in acquiring datasets for Object Detection (OD) is labelling datasets. But if I have a video footage that runs, say 30FPS, I could just ***label a few, say a couple,*** on that short burst of 1second clip which has about 30 frames of images containing the object on slight variations. \n\nTheoretically, for a 1 second clip, I can annotate ***n*** data and then use ***30 - n***  images as unlabelled dataset, right? Could I? And should I?\n\n**SSL on Variations of Objects in the Video/Image**\n\nOur dataset will be collected using a ***moving video camera.*** It is a video footage of road survey. Hence, frame by frame comparison of images could have slight to moderate difference from occlusions to motion blur, to shadows, perspectives, etc. \n\n**Questions:**\n\n1. Has there been papers dealing with similar problem? I know that SSL just assumes unlabeled datasets are present. But for these ones, datasets would be much closer to their annotated counterpart on one point in the video.\n2. Any advice or perspectives on approaching the problem as well? We could manually find frames in the video then get the frames from time **t,** with **t-n and t+n** to get the unlabelled dataset and one to two annotations at that time **t.** \n3. What approaches in SSL in terms of algorithm would be preferable? Consistency Regularizations, Proxy Methods, etc?"}, {"id": "ofs9i0", "title": "[D] Paper regarding all ML problems reducing to diff eq..?", "score": 21, "url": "https://www.reddit.com/r/MachineLearning/comments/ofs9i0/d_paper_regarding_all_ml_problems_reducing_to/", "author": "Smiliey", "subreddit": "r/MachineLearning", "description": "I remember reading a while back about someone releasing a paper of someone stating that all ML-based problems could be reduced to, or modeled by a differential equation (can't remember if it was PDE or ODE).. Does anyone else remember reading about something like that and if so, would you be able to share the link? Thanks."}, {"id": "ofkn7k", "title": "[R] ACL 2021 Best Paper: Finding the Optimal Vocabulary for Machine Translation via an Optimal Transport Approach", "score": 38, "url": "https://www.reddit.com/r/MachineLearning/comments/ofkn7k/r_acl_2021_best_paper_finding_the_optimal/", "author": "Yuqing7", "subreddit": "r/MachineLearning", "description": "A research team from ByteDance AI Lab, University of Wisconsin\u2013Madison and Nanjing University wins the ACL 2021 best paper award. Their proposed Vocabulary Learning via Optimal Transport (VOLT) approach leverages optimal transport to automatically find an optimal vocabulary without trial training. \n\nHere is a quick read: [ACL 2021 Best Paper: Finding the Optimal Vocabulary for Machine Translation via an Optimal Transport Approach.](https://syncedreview.com/2021/07/07/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-56/)\n\nThe associated codes are available on the project GitHub. The paper *Vocabulary Learning via Optimal Transport for Neural Machine Translation* is on [arXiv](https://arxiv.org/abs/2012.15671)."}, {"id": "og4zz7", "title": "[D] In LDA model topics which are created what would be the best way to derive what each topic is actually about.", "score": 1, "url": "https://www.reddit.com/r/MachineLearning/comments/og4zz7/d_in_lda_model_topics_which_are_created_what/", "author": "AROY18", "subreddit": "r/MachineLearning", "description": "For example if a topics has keywords like \\[artificial intelligence, information, microsoft, machine learning\\] how to derive that this topic is actually related to artificial intelligence or maybe IT. What would be the best way to draw some correlation between the topic keywords and arrive at a final word which could summarize what all the topic keywords are related to."}, {"id": "ofiid0", "title": "[D] Alien Dreams: An Emerging Art Scene. Blog post about the recent trend of art produced using OpenAI's CLIP model.", "score": 35, "url": "https://www.reddit.com/r/MachineLearning/comments/ofiid0/d_alien_dreams_an_emerging_art_scene_blog_post/", "author": "hardmaru", "subreddit": "r/MachineLearning", "description": "**Blog post**&nbsp;&nbsp;https://ml.berkeley.edu/blog/posts/clip-art/\n\n**Author**&nbsp;&nbsp;Charlie Snell\n\n**Excerpt**&nbsp;&nbsp;In recent months there has been a bit of an explosion in the AI generated art scene.\n\nEver since OpenAI released the weights and code for their CLIP model, various hackers, artists, researchers, and deep learning enthusiasts have figured out how to utilize CLIP as a an effective \u201cnatural language steering wheel\u201d for various generative models, allowing artists to create all sorts of interesting visual art merely by inputting some text \u2013 a caption, a poem, a lyric, a word \u2013 to one of these models."}, {"id": "og2zy6", "title": "[D]Question on Recommendation for transfer learning studying material (clinical data science related)", "score": 0, "url": "https://www.reddit.com/r/MachineLearning/comments/og2zy6/dquestion_on_recommendation_for_transfer_learning/", "author": "amazingclaude", "subreddit": "r/MachineLearning", "description": "I encountered a transfer learning problem, and would like to ask for some recommendations for methodologies in solving this problem.\n\n**\\[Background\\]** Some patients are sent to hospital Emergency Room (ER) because of the side effects from medications. Some patients went back home after treatment in ER (group A), the other patients were clinical admitted after ER, that is, went to hospital ward after ER (group B). Doctors labeled all the group B patients (binary label of whether the patient's medication side effects can be prevented). A supervised machine learning classification model (random forest) is trained from group B.\n\n**\\[Problem\\]** Now we want to estimate if we can use the trained model directly, or transferred to group A. Compared to group B, Group A lack clinical measurements data (such as heart rate, blood pressure) because Group A were not clinical admitted. Other than this, Group A and B share the same data structure in the data base.\n\n**\\[Question\\]** Are there books / articles / methodologies that can be used for estimating the feasibility of transferring the model?"}, {"id": "of57qe", "title": "[D] PhD students: what is your process to generate new ideas?", "score": 243, "url": "https://www.reddit.com/r/MachineLearning/comments/of57qe/d_phd_students_what_is_your_process_to_generate/", "author": "nojplize", "subreddit": "r/MachineLearning", "description": "I am wondering what is your usual process to generate new research ideas.\n\nSome thoughts about how to come up with new ideas:\n\n\\-discuss and brainstorm with other students\n\n\\-your supervisor suggested research ideas\n\n\\-you read a paper and realized there is something interesting to dwelve into\n\n\\-solo sessions of brainstorming / daydreaming\n\nDo you have a consistent way to produce new research ideas or is it more like a \"hazardous\" process?"}, {"id": "ofj677", "title": "[D] NeRF GAN paper explained in 5 minutes - GRAF: Generative Radiance Fields for 3D-Aware Image Synthesis by Katja Schwarz et al.", "score": 14, "url": "https://www.reddit.com/r/MachineLearning/comments/ofj677/d_nerf_gan_paper_explained_in_5_minutes_graf/", "author": "KirillTheMunchKing", "subreddit": "r/MachineLearning", "description": "[ 3D GRAF samples learned from 2D data](https://i.redd.it/hhupbq9zns971.gif)\n\nNeRF models blew up last year spawning an endless stream of variations and modifications addressing important issues with the original design. One of the more unique ideas that came from this NeRF Explosion (coined by Frank Dellaert) is this paper by researchers from the Max Planck Institute for Intelligent Systems. The authors of GRAF combined NeRFs and GANs to design a pipeline for generating conditional Neural Radiance Fields that can generate consistent 3d models with various shapes and appearances despite only being trained on 2d unposed images.\n\nRead the [full paper digest](https://t.me/casual_gan/61) (reading time \\~5 minutes) to learn about NeRF models, the motivation for combining NeRF models with the GAN framework, and all of the tricks used in the radiance field generator to synthesize 3d aware images from a set of unposed 2d images.\n\nMeanwhile, check out the paper digest poster by [Casual GAN Papers](https://t.me/casual_gan)!\n\n[GRAF explained](https://preview.redd.it/pzm62j29os971.png?width=615&format=png&auto=webp&s=65d0848d71630c71de0e02ff05b6a6f45bb85f4f)\n\n\\[[Full Explanation Post](https://t.me/casual_gan/61)\\] \\[[Arxiv](http://www.cvlibs.net/publications/Schwarz2020NEURIPS.pdf)\\] \\[[Code](https://github.com/autonomousvision/graf)\\]\n\nMore recent popular computer vision paper breakdowns:\n\n>\\[[Alias-free GAN](https://t.me/casual_gan/58)\\]  \n>  \n>\\[[GFPGAN](https://t.me/casual_gan/54)\\]  \n>  \n>\\[[PTI](https://t.me/casual_gan/60)\\]"}, {"id": "ofh8uy", "title": "[R] Challenge: Out-of-Distribution Detection IRL", "score": 6, "url": "https://www.reddit.com/r/MachineLearning/comments/ofh8uy/r_challenge_outofdistribution_detection_irl/", "author": "NecessaryEchidna", "subreddit": "r/MachineLearning", "description": "Interested or working on (\"unsupervised\") Out-of-Distribution Detection?\n\nDo you want to compare your OoD algorithms in a controlled setting (#NoWeakBaselines) and/or see how it stacks up \"in real life\"?\n\nWe would like to invite you to the *Medical Out-of-Distribution Analysis Challenge 2021* : \n\n[http://medicalood.dkfz.de/web/](http://medicalood.dkfz.de/web/) \n\nThe challenge spans two datasets, one brain MRI-dataset and one abdominal CT-dataset, the training set has no anomalies. The goal is to detect OoD samples and potentially as a second step segment anomalies.\n\nData loaders, ready2go examples, ... are provided: [https://github.com/MIC-DKFZ/mood](https://github.com/MIC-DKFZ/mood) and last year's top team papers and code are available as well [http://medicalood.dkfz.de/web/2020/#leaderboard](http://medicalood.dkfz.de/web/2020/#leaderboard) .\n\nThe top teams will be invited to present their results at MICCAI 2021 and submit a proceedings paper.\n\nDeadline: 07 September 2021.\n\nLooking forward to your submission :D #ShamelessPlug"}, {"id": "og1mat", "title": "[D] An interview with the Forbes AI writer, Tom Taulli", "score": 0, "url": "https://www.reddit.com/r/MachineLearning/comments/og1mat/d_an_interview_with_the_forbes_ai_writer_tom/", "author": "DonjiDonji", "subreddit": "r/MachineLearning", "description": "This week we have a really interesting interview for you, its not that often we get a look into the publication aspect of the AI/ML space.  \nThere are some stamped links below for your convenience.  \n\n\nhttps://preview.redd.it/64k483cknx971.jpg?width=2042&format=pjpg&auto=webp&s=a63b51789c82168f621b7a54b5aa552202358ddf\n\n[0:00](https://www.youtube.com/watch?v=v_cHt8UMGgo&t=0s) Getting to Know Tom!  \n[3:05](https://www.youtube.com/watch?v=v_cHt8UMGgo&t=185s) The Early Days of Tech Empires  \n[5:22](https://www.youtube.com/watch?v=v_cHt8UMGgo&t=322s) Forgotten Importance of the Tech Event  \n[6:28](https://www.youtube.com/watch?v=v_cHt8UMGgo&t=388s) From Hobby to Career  \n[10:17](https://www.youtube.com/watch?v=v_cHt8UMGgo&t=617s) Writing for Forbes  \n[16:31](https://www.youtube.com/watch?v=v_cHt8UMGgo&t=991s) The AI Space  \n[24:01](https://www.youtube.com/watch?v=v_cHt8UMGgo&t=1441s) Humans are Underrated  \n[24:02](https://www.youtube.com/watch?v=v_cHt8UMGgo&t=1442s) AI Pitfalls  \n[29:40](https://www.youtube.com/watch?v=v_cHt8UMGgo&t=1780s) Deeptech Pitfalls  \n[31:07](https://www.youtube.com/watch?v=v_cHt8UMGgo&t=1867s) The Robot-Job Apocalypse?  \n[36:01](https://www.youtube.com/watch?v=v_cHt8UMGgo&t=2161s) Modern Day Luddites  \n[38:56](https://www.youtube.com/watch?v=v_cHt8UMGgo&t=2336s) 100 Years into The Future  \n[42:08](https://www.youtube.com/watch?v=v_cHt8UMGgo&t=2528s) Tom's AI Takes  \n[43:55](https://www.youtube.com/watch?v=v_cHt8UMGgo&t=2635s) Strong v. Weak AI  \n[44:38](https://www.youtube.com/watch?v=v_cHt8UMGgo&t=2678s) Hardware & Energy  \n[47:07](https://www.youtube.com/watch?v=v_cHt8UMGgo&t=2827s) What Tom is Excited by in AI  \n[48:55](https://www.youtube.com/watch?v=v_cHt8UMGgo&t=2935s) Tips for Integrating AI into your Business  \n[52:36](https://www.youtube.com/watch?v=v_cHt8UMGgo&t=3156s) Tom's Current Projects  \n[53:46](https://www.youtube.com/watch?v=v_cHt8UMGgo&t=3226s) Tom's Process and Outreach!  \n[57:21](https://www.youtube.com/watch?v=v_cHt8UMGgo&t=3441s) Tips for Being Published in Forbes\n\nRegular link to episode: [https://www.youtube.com/watch?v=v\\_cHt8UMGgo&ab\\_channel=Metabob](https://www.youtube.com/watch?v=v_cHt8UMGgo&ab_channel=Metabob)\n\nLet us know if there is anyone in the ML space you would like to see us interview!!!"}, {"id": "ofc2dm", "title": "[R] Variational Diffusion Models", "score": 16, "url": "https://arxiv.org/abs/2107.00630", "author": "hardmaru", "subreddit": "r/MachineLearning", "description": ""}, {"id": "ofa0u8", "title": "[R] Rethinking Positional Encoding", "score": 25, "url": "https://arxiv.org/abs/2107.02561", "author": "hardmaru", "subreddit": "r/MachineLearning", "description": ""}, {"id": "ofkbh9", "title": "[D] Regarding the training sets and output of GANs", "score": 2, "url": "https://www.reddit.com/r/MachineLearning/comments/ofkbh9/d_regarding_the_training_sets_and_output_of_gans/", "author": "crwcomposer", "subreddit": "r/MachineLearning", "description": "If you have a training set that includes both dogs and cats, and you want it to generate an image based on the training set, will it generate discrete images of dogs OR cats, or will it generate images of a creature somewhere between a dog and cat?\n\nIt seems to me that the network that performs the discrimination would be able to learn the difference between dogs and cats, and therefore the output of the network that generates images would be either a dog or a cat, and not a mix.\n\n1. Is that correct?\n2. If it is, is there a way to adjust the network to adjust the mixture?\n3. Is there a better non-GAN configuration of networks for that purpose?\n\nThanks"}, {"id": "of8z7d", "title": "[R] Long-Short Transformer: Efficient Transformers for Language and Vision", "score": 18, "url": "https://arxiv.org/abs/2107.02192", "author": "downtownslim", "subreddit": "r/MachineLearning", "description": ""}, {"id": "oev35g", "title": "[P] MLOps Podcast \u2013 \ud83e\udd17 HuggingFace CTO Julien Chaumond on large models in production", "score": 142, "url": "https://www.reddit.com/r/MachineLearning/comments/oev35g/p_mlops_podcast_huggingface_cto_julien_chaumond/", "author": "PhYsIcS-GUY227", "subreddit": "r/MachineLearning", "description": "Hi r/MachineLearning! Thought this might be interesting for people here\n\n[https://www.youtube.com/watch?v=NKw\\_fPMRe4o](https://www.youtube.com/watch?v=NKw_fPMRe4o)\n\nI sat down to interview Julien, Co-Founder & CTO at HuggingFace a few weeks ago. It was really fun. We talk about a bunch of topics, including the HuggingFace origin story, their models in production, and the \"CERN\" of machine learning.\n\nWe're still in the early stages of the podcast, so I would honestly love feedback, thoughts on how future episodes can be better, and suggestions for other guests I should reach out to.\n\nIf you want to listen in audio form:\n\n* [Apple podcasts](https://podcasts.apple.com/il/podcast/huggingface-cto-julien-chaumond-on-large-models-in/id1565390757?i=1000527770905)\n* [Google podcasts](https://podcasts.google.com/feed/aHR0cHM6Ly9hbmNob3IuZm0vcy81OGNkYmI4Yy9wb2RjYXN0L3Jzcw/episode/M2Y3MWFiZjAtMmM2Zi00M2M3LTljN2MtYTAwNjFlY2ZiNDUw?sa=X&ved=0CAUQkfYCahcKEwiIrrCIx87xAhUAAAAAHQAAAAAQAQ)\n* [Spotify](https://open.spotify.com/episode/1JU5zN2vhhOI0xkUqxsvJV?si=vv6n3LLyQZ-LqZDfLWwwMQ&dl_branch=1)\n* [Podcast addict](https://podcastaddict.com/episode/125318836)"}, {"id": "ofk62c", "title": "[D] Machine Learning with Graphs (Research Paper Summaries)", "score": 0, "url": "https://www.reddit.com/r/MachineLearning/comments/ofk62c/d_machine_learning_with_graphs_research_paper/", "author": "prakhar21", "subreddit": "r/MachineLearning", "description": "Here\u2019s an evolving playlist for Graph ML papers explanations.\n\nPaper Walkthroughs: https://youtube.com/playlist?list=PLsAqq9lZFOtU7tT6mDXX_fhv1R1-jGiYf"}, {"id": "ofk5e5", "title": "[R] [N] Conference for Truth and Trust Online Calls for Paper and Talk Proposal Submissions", "score": 0, "url": "https://www.reddit.com/r/MachineLearning/comments/ofk5e5/r_n_conference_for_truth_and_trust_online_calls/", "author": "kochkinael", "subreddit": "r/MachineLearning", "description": "We invite you to submit a paper or talk proposal to this year\u2019s\u00a0**Truth and Trust Online (TTO) conference (October 7-8, 2021, online).**\n\nWe are excited to announce that\u00a0**a selection of the best technical papers will be invited to a special issue of the ACM Journal of Data and Information Quality.**  \n\n\nThe submission deadlines are just around the corner but its not too late to contribute :)\u00a0\n\nTechnical papers deadline: **July 30, 2021;**\u00a0\u00a0talk proposals deadline:\u00a0**August 13, 2021.**  \n\n\n**Call for Papers and Talk Proposals**\n\nWe invite submissions of\u00a0**technical papers**\u00a0and\u00a0**talk proposals**\u00a0on technical solutions for addressing current challenges facing social media platforms on the following\u00a0topics:  \n\n\n\u00a0 \u00a0 \u2022 Misinformation and disinformation\u00a0  \n\u00a0\u00a0 \u00a0\u2022 Trustworthiness of COVID-19 news and guidance  \n\u00a0\u00a0 \u00a0\u2022 Hate speech  \n\u00a0\u00a0 \u00a0\u2022 Online harassment and cyberbullying  \n\u00a0\u00a0 \u00a0\u2022 Credibility and fake reviews  \n\u00a0\u00a0 \u00a0\u2022 Hyper-partisanship and bias  \n\u00a0\u00a0 \u00a0\u2022 Image/video/audio verification  \n\u00a0\u00a0 \u00a0\u2022 Fake amplification, polarization, and echo chambers  \n\u00a0\u00a0 \u00a0\u2022 Transparency in content and source moderation\u00a0  \n\u00a0\u00a0 \u00a0\u2022 Privacy and anonymity requirements  \n\n\nWe welcome\u00a0**technical papers**\u00a0of the following types:\u00a0*surveys, methods, reproduction papers, resource papers, case studies*.\u00a0\n\n**More information**\u00a0about the call for papers, talk proposals and the JDIQ special issue\u00a0can be found here:\u00a0[https://truthandtrustonline.com/call-for-papers-2/](https://eur01.safelinks.protection.outlook.com/?url=https%3A%2F%2Ftruthandtrustonline.us17.list-manage.com%2Ftrack%2Fclick%3Fu%3Da4f23076f7054c8e9a1620da3%26id%3Dec373059d5%26e%3D5db6c1f255&data=04%7C01%7C%7Caf3244fe4eec453258d308d9399230ad%7C569df091b01340e386eebd9cb9e25814%7C0%7C0%7C637604122318972844%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=TGRUQTNUmmIzUszXan42baSAF2KOlPE9psaw3EwKP90%3D&reserved=0)  \n\n\nWe encourage wide participation from all interested parties and stakeholders on online media, including academics, startups and large industry, non-profit organizations and governmental institutions.\n\n**About The Conference on Truth and Trust Online**\n\nThe Conference on Truth and Trust Online is an annual forum for academia, industry, non-profit organizations, and other stakeholders to discuss the problems facing (social) media platforms and technical solutions to understand and address them. TTO\u2019s mission is to bring together all parties working toward improving the truthfulness and trustworthiness of online communications. More information about the conference is\u00a0[on the TTO website](https://truthandtrustonline.com/).\u00a0  \n\n\n**Feel free to pass this on to anyone you think might be interested!**\n\n**Contact us**\u00a0if you have any questions at\u00a0[admin@truthandtrustonline.com](mailto:admin@truthandtrustonline.com)\n\nFollow us on **Twitter**: u/TTOConference  \nSign up to **TTO mailing list**:\u00a0[eepurl.com/haDf5H](https://t.co/BFvIgPjw9o?amp=1)"}, {"id": "of4jq2", "title": "[N] A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification", "score": 18, "url": "https://www.reddit.com/r/MachineLearning/comments/of4jq2/n_a_gentle_introduction_to_conformal_prediction/", "author": "aangelopoulos", "subreddit": "r/MachineLearning", "description": "Check out our new Gentle Intro to Conformal Prediction and distribution-free UQ tutorial + video. \n\n[http://people.eecs.berkeley.edu/\\~angelopoulos/blog/posts/gentle-intro/](http://people.eecs.berkeley.edu/~angelopoulos/blog/posts/gentle-intro/)\n\nThis tutorial will give you instructions for quantifying the uncertainty of any machine learning algorithm, accompanied by Python/PyTorch code, lots of explanatory illustrations, and worked examples.  It is meant to be a hands-on introduction for a reader interested in the practical implementation of distribution-free UQ, who is not necessarily a statistician.  At the end, you will be able to generate valid confidence intervals/sets for any model for any (unknown) distribution, without ever retraining the model. \n\nPlease let me know if you have any thoughts, questions, or feedback, and I hope you enjoy watching and reading. If you want to learn more about this research area, come to the [ICML Workshop on Distribution-Free Uncertainty Quantification](https://sites.google.com/berkeley.edu/dfuq21/)!\n\nhttps://preview.redd.it/n4j8ip6dtn971.jpg?width=1756&format=pjpg&auto=webp&s=348ef451307158a6396ba2cf5941b4b9f670e292"}, {"id": "ofkiti", "title": "[D] SAILea AI competition problem ideas", "score": 0, "url": "https://www.reddit.com/r/MachineLearning/comments/ofkiti/d_sailea_ai_competition_problem_ideas/", "author": "Intrepid_Bill3482", "subreddit": "r/MachineLearning", "description": "Hi r/MachineLearning,\n\nThis fall, I will be co-organizing a ML competition that both beginners and advanced people can participate in! The competition is broken into two rounds, namely optimization (solo) and team vs team. Just as it sounds, optimization problems will have a range of solutions. For example [this](https://en.wikipedia.org/wiki/Packing_problems) circle packing in a rectangle is an optimization problem. Similarly the team vs team round will follow a tournament format where teams are pitted against each other. An example of a problem can be team performance on [this](http://m.onemorelevel.com/trapthetiger/) game. Naturally, there won't be these exact problems on the competition. There will also be cash prizes for the top competitors. I will also be linking the competition website once it is fully developed :)\n\nI was wondering if you guys had any good optimization/ tvt problems that I could draw some inspiration from?"}, {"id": "of7vtf", "title": "[D] best practices for using humans to verify labels", "score": 5, "url": "https://www.reddit.com/r/MachineLearning/comments/of7vtf/d_best_practices_for_using_humans_to_verify_labels/", "author": "hswerdfe_2", "subreddit": "r/MachineLearning", "description": "I have 4 million sentence about 0.2% of which are labeled as a positive class. those 0.2% can then belong to any one of a number of sub-classes. I was wondering what is the best way to utilize the time of labelers that will verify the models labels and then retrain on those labels as well. Is there literature on how to best sample from the models predictions and obtain additional labels?"}, {"id": "of6m5t", "title": "[D] Apart from the Unet family which other models are used for semantic segmentation?", "score": 6, "url": "https://www.reddit.com/r/MachineLearning/comments/of6m5t/d_apart_from_the_unet_family_which_other_models/", "author": "Tis-is-the-way", "subreddit": "r/MachineLearning", "description": "Kindly recommend some models apart from PSPnet or Unet family which can be used for Lane Segmentation"}, {"id": "ofkxhc", "title": "[D] A detailed critique of DeepMind's intelligence theory", "score": 0, "url": "https://www.reddit.com/r/MachineLearning/comments/ofkxhc/d_a_detailed_critique_of_deepminds_intelligence/", "author": "bendee983", "subreddit": "r/MachineLearning", "description": "In a new essay by Herbert Roitblat, author of *Algorithms Are Not Enough,* refutes claims by Silver et al in the recent paper \"Reward is Enough.\"\n\n[https://bdtechtalks.com/2021/07/07/ai-reward-is-not-enough-herbert-roitblat/](https://bdtechtalks.com/2021/07/07/ai-reward-is-not-enough-herbert-roitblat/)\n\nKey points:\n\n* The reward hypothesis is very much like behaviorism (B. F. Skinner) and past-tense learning (Rumelhart and McClelland), both of which suffered from confirmation bias and have failed. It is a circulary hypothesis.\n* Reward by itself is not really enough, and at a minimum, the environment also plays a role by enforcing adaptation. Adaptation requires a source of variability from which certain traits can be selected.\n* Reinforcement learning is a purely selective process. Reinforcement learning is the process of increasing the probabilities of actions that together form a policy for dealing with a certain environment. Those actions must already exist for them to be selected. At least for now, those actions are supplied by the genes in evolution and by the program designers in artificial intelligence.\n* Rewards select among available alternatives but they cannot create those alternatives. Behaviorist rewards work so long as one does not look too closely at the phenomena and as long as one assumes that there must be some reward that reinforces some action. They are good after the fact to \u201cexplain\u201d any observed actions, but they do not help outside the laboratory to predict which actions will be forthcoming. These phenomena are consistent with reward, but it would be a mistake to think that they are caused by reward."}, {"id": "of9sl6", "title": "[D] What are some interesting works in GraphDL x RL?", "score": 0, "url": "https://www.reddit.com/r/MachineLearning/comments/of9sl6/d_what_are_some_interesting_works_in_graphdl_x_rl/", "author": "rish-16", "subreddit": "r/MachineLearning", "description": "Hello everyone!\n\nI'm an undergrad looking at areas to focus on for school and am planning to look into the intersection of Graph Deep Learning and Reinforcement Learning. I've found some interesting papers but am not able to find any more off of simple Google searches.\n\nCan I please know what are some really cool GDL x RL papers you've come across the past few years that achieve SOTA on various environments? I'm interested in anything under the sun.\n\nThanks :D"}, {"id": "ofd505", "title": "[D] what tool you use for data dictionary", "score": 0, "url": "https://www.reddit.com/r/MachineLearning/comments/ofd505/d_what_tool_you_use_for_data_dictionary/", "author": "ajinkyajawale", "subreddit": "r/MachineLearning", "description": "We are heavy data engineering and data science team who regularly interacts with product and Marketing teams\n\nWe don't want to Product team and marketing team to get involved into dwh or schema or any data level asking hell what's this what's that... Rather we providing the proper data dictionary like what's the table what's it's meaning fields at High level view..\n\nIt would be highly appreciated if you have used any similar relevant tools..\n\nPs.. we are not looking for data governance tools more as a data dictionary documention tool"}, {"id": "oewa7d", "title": "[R] Facebook & UC Berkeley Substitute a Convolutional Stem to Dramatically Boost Vision Transformers\u2019 Optimization Stability", "score": 8, "url": "https://www.reddit.com/r/MachineLearning/comments/oewa7d/r_facebook_uc_berkeley_substitute_a_convolutional/", "author": "Yuqing7", "subreddit": "r/MachineLearning", "description": "A research team from Facebook AI and UC Berkeley finds a solution for vision transformers\u2019 optimization instability problem by simply using a standard, lightweight convolutional stem for ViT models. The approach dramatically increases optimizer stability and improves peak performance without sacrificing computation efficiency. \n\nHere is a quick read: [Facebook & UC Berkeley Substitute a Convolutional Stem to Dramatically Boost Vision Transformers\u2019 Optimization Stability.](https://syncedreview.com/2021/07/06/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-55/)\n\nThe paper *Early Convolutions Help Transformers See Better* is on [arXiv](https://arxiv.org/abs/2106.14881)."}, {"id": "oemieo", "title": "[D] I am a research intern with very little ML knowledge and have not learnt much on the job as of yet. I would appreciate to hear what it is like working in the field and of any recommendations on ML resources.", "score": 58, "url": "https://www.reddit.com/r/MachineLearning/comments/oemieo/d_i_am_a_research_intern_with_very_little_ml/", "author": "ChrisDrake", "subreddit": "r/MachineLearning", "description": "I am going into my final year of computer science and I am on a summer internship. I originally came in on this project as an android developer and it has taken a turn into ML . Im glad it did because its such a fascinating field! \n\nA huge amount of time went into finding suitable projects we could use for our model,  instead of creating one from scratch. The disappointing result of thus is how little I have learned so far. Since all I have done is train a model using a default dataset on a SLURM cluster.\n\nGoing forward with the project I would hate to get little ML experience out of my internship or get hit with a giant roadblock down the line.\n\nI would really appreciate any suggestions in material anybody has. I would also love to hear if anyone has ever been in my situation or even tell me what working in the field is like !\nThanks guys."}, {"id": "ofe3ji", "title": "[D] The Secrets of Machine Learning: Ten Things You Wish You Had Known Earlier to be More Effective at Data Analysis", "score": 0, "url": "https://arxiv.org/abs/1906.01998", "author": "Philo167", "subreddit": "r/MachineLearning", "description": ""}, {"id": "oearv9", "title": "[R] [P] - Master's thesis on Investigating Sparsity in Recurrent Neural Networks", "score": 259, "url": "https://www.reddit.com/r/MachineLearning/comments/oearv9/r_p_masters_thesis_on_investigating_sparsity_in/", "author": "harshildarji", "subreddit": "r/MachineLearning", "description": "Today, my Master's thesis got graded. So now, I can make my thesis repository public.\n\nMy Master's thesis focuses on examining the effects of sparsity in Recurrent Neural Networks. This is an important research area that focuses on the optimization of Recurrent Neural Networks by reducing the complexity of their internal architecture while maintaining the original performance.\n\nComplete thesis report and full implementation are available on the following GitHub repository.\n\n[https://github.com/harshildarji/thesis](https://github.com/harshildarji/thesis)"}, {"id": "oekh7h", "title": "[R] A topological solution to object segmentation and tracking", "score": 41, "url": "https://arxiv.org/abs/2107.02036", "author": "hardmaru", "subreddit": "r/MachineLearning", "description": ""}, {"id": "oer10g", "title": "[P] Gait Recognition in the wild", "score": 7, "url": "https://www.reddit.com/r/MachineLearning/comments/oer10g/p_gait_recognition_in_the_wild/", "author": "cosmadrian", "subreddit": "r/MachineLearning", "description": "Hey r/MachineLearning, \n\nI wanted to share this demo video from my master thesis called \"Learning Gait Representations from Raw Surveillance Streams\". It's an example of how gait recognition works in unstructured scenes. The model recognises me and my girlfriend reliably (green color), amongst other people in the scene, using the way we walk (i.e. skeleton sequences).\n\nThe model is an [ST-GCN](https://arxiv.org/abs/1801.07455) trained in a completely unsupervised manner, from a LOT of skeleton sequences. Pose estimation was performed with [AlphaPose](https://github.com/MVIG-SJTU/AlphaPose), and tracked using [SORT](https://github.com/abewley/sort).\n\nI'm curious of your opinions & suggestions :)\n\n[https://youtu.be/NoH0vL1H08E](https://youtu.be/NoH0vL1H08E)"}, {"id": "oeo38o", "title": "[D] What are some great newsletters for Machine Learning and AI?", "score": 9, "url": "https://www.reddit.com/r/MachineLearning/comments/oeo38o/d_what_are_some_great_newsletters_for_machine/", "author": "benthecoderX", "subreddit": "r/MachineLearning", "description": "Hi all, I compiled a list of reputable newsletters with quality content on DS, DE, ML, and AI and was wondering if you all agree with the list, or there's any other ones I missed out. Cheers.\n\nLink: [https://towardsdatascience.com/20-must-subscribe-data-and-ai-newsletters-in-2021-7c5ddb9b3c19](https://towardsdatascience.com/20-must-subscribe-data-and-ai-newsletters-in-2021-7c5ddb9b3c19)"}, {"id": "oe6paj", "title": "[D] GPT-J for text generation: hardware requirements", "score": 83, "url": "https://www.reddit.com/r/MachineLearning/comments/oe6paj/d_gptj_for_text_generation_hardware_requirements/", "author": "juliensalinas", "subreddit": "r/MachineLearning", "description": "Hi everyone.\n\nSince the release of GPT-J, I worked hard in order to add it to [NLPCloud.io](https://nlpcloud.io/?utm_source=reddit&utm_campaign=b311103c-dd8e-11eb-ba80-0242ac130004) for text generation.\n\nThis is done now and the infrastructure is stabilized but that was tricky. So I thought I would share here my key takeaways, in case it can help some of you:\n\n* On CPU, the model needs around 40GB of memory to load, and then around 20GB during runtime.\n* On CPU, a standard text generation (around 50 words) takes approximately 12 CPUs for 11 seconds\n* On a GPU, the model needs around 40GB of memory to load, and then around 3GB during runtime + 24GB of GPU memory. For a standard text generation (around 50 words), the latency is around 1.5 secs\n\nThe 2 main challenges are the high amount of RAM needed for startup, and then high amount of GPU memory needed during runtime which is quite   impractical as most affordable NVIDIA GPUs dedicated to inference, like Tesla T4, only have 16GB of memory...\n\nIt's very interesting to note that, during my tests, the latency was pretty much the same as GPT-Neo 2.7B on the same hardware, but accuracy seems of course much better.\n\nIf some of you also ran these kinds of benchmarks on GPT-J I'd love to see if we're aligned or not!"}, {"id": "oeqd0y", "title": "[D] Can data-centric approach be applied to feature-centric approach", "score": 3, "url": "https://www.reddit.com/r/MachineLearning/comments/oeqd0y/d_can_datacentric_approach_be_applied_to/", "author": "vietlinh12hoa", "subreddit": "r/MachineLearning", "description": " \n\nRecently, Andrew Ng introduced the data-centric approach in which the model and its parameters are kept the same but the data will vary. I wonder if we can apply the same principle for feature-centric, to see which feature is important.\n\nFor example:\n\n* initialize a random forest model\n* run a list of uncorrelated features like: only feature A, then only feature B ... with a that model and doesn't change any param.\n* Rank best features based on the model metrics. (try to understand why this feature is most important)\n* Test that model based on those features, start with combining 2, 3, 4 .... until the metric is saturated.\n* Finally, start tuning model or even testing other model types with those features.\n\nRule:\n\n* Try the moderate-complex model (not too much simple like linear, not too much complex and black box like neural net)\n* Try numerical features first\n* Try categorical features with 5 - 10 categories (or 10-20, 20-30... depends on data dimension). The variable with too many or too few categories, IMO, might be not so important. For those categorical variables, applying the same encoding method\n* Don't grid search or random search (so it'll be quick)\n\nChallenge (vary on problems):\n\n* Which threshold/method to decide features are not correlated?\n* Which metric of model output to decide one feature is better than the others?\n* What happens if too many uncorrelated features? Should loop over all?\n\nWhat do you guys think?"}, {"id": "oer5um", "title": "[D] Any ideas why Apple's tensorflow_macos is being archived?", "score": 2, "url": "https://www.reddit.com/r/MachineLearning/comments/oer5um/d_any_ideas_why_apples_tensorflow_macos_is_being/", "author": "CodaholicCorgi", "subreddit": "r/MachineLearning", "description": "Hi folks, TensorFlow for M1 still has not been fully functional yet, so I have many bugs to raise for them. But it seems that this repository is achieved for now, which means we cannot raise any issues. Does anyone know why they achieve the repo and when will they open it again? Thank you a lot.\n\n  \nLink: [https://github.com/apple/tensorflow\\_macos](https://github.com/apple/tensorflow_macos)"}, {"id": "oeaxyj", "title": "[D] Is big spending on AI ethics and regulation research premature?", "score": 17, "url": "https://www.reddit.com/r/MachineLearning/comments/oeaxyj/d_is_big_spending_on_ai_ethics_and_regulation/", "author": "Azmisov", "subreddit": "r/MachineLearning", "description": "My observation is that the surge in spending in recent years is due to this obsessive fear of an AI war and uprising, or oppression of minorities. To me, it seems governments and companies are spending disproportionately to the actual observable problems. A biased/unfair dataset yields biased results... do we really need so much research to repeat that same self-evident conclusion over and over? Sure, it helps to have secondary research pointing out biases in SOTA models, but surely that should only require a tiny fraction of current spending, no?\n\nI think ethics may become a more interesting topic when we are further along the road to general AI. Then, questions like \"how can we teach AI morality?\" can actually usefully be answered\u2014 not in a vacuous abstract way from 3rd parties, but by the actual ML researchers. At this point with everything being weak AI, it seems better to just think of everything as sophisticated computer programs. Targeted advertising, discounts, or loans all have a much bigger, pervasive impact on society right now, and yet there is little to no ethics research and publicity on these because they don't have the \"AI\" buzzword attached.\n\nWhat do you think? Do you disagree and feel it is still money well spent?\n\n*(I realize there is an AI ethics subreddit, but I'd like to hear opinions from ML researchers themselves)*\n\n[View Poll](https://www.reddit.com/poll/oeaxyj)"}, {"id": "oe2xs0", "title": "[R] R-Drop: Regularized Dropout for Neural Networks", "score": 68, "url": "https://arxiv.org/abs/2106.14448", "author": "GratisSlagroom", "subreddit": "r/MachineLearning", "description": ""}, {"id": "oehajq", "title": "[Discussion] Can a PhD correct mistakes of the past education?", "score": 6, "url": "https://www.reddit.com/r/MachineLearning/comments/oehajq/discussion_can_a_phd_correct_mistakes_of_the_past/", "author": "AgrippaMorti", "subreddit": "r/MachineLearning", "description": "Hello, I am a PhD offer holder at a good UK university (not top tier but is every year at ICLR, NeurIPS, ICML, etc) and wanted to have a discussion with people currently doing their PhD in AI/ML or who have graduated recently. My friends/family are not in academia so I turn to you, fellow trusted Redditors to ask you for your stories. My aim is to hear personal experiences **(especially your horror stories)** rather than suggestions if I should or shouldn't do it, however, if you identify red flags in my story please let me know.\n\nMy BSc is in CompSci (Europe low tier school) and I have been working in the past couple of years in the industry and did a stint as a research assistant. I was a mediocre student who never really learned how to code (still am kinda bad at it) but the research stint made me want to pursue an ML master's (in the UK) which I am currently finishing. It was a great experience but I feel I am still lacking the fundamentals needed to be successful in the field (not very good at math either-took calculus/stats/linalg classes in my undergrad but it was 5 years ago and I struggled on those topics during the MSc).\n\n My questions are the following:\n\n1. Does the intensive programme of a 3-4 year PhD allow for someone to double down on things that they are bad at (in my case become a competent Soft Eng and improve my understanding of seminal math/stat concepts)?\n2. For those that pursued a career in the industry what did you end up doing? Is it possible to land a role where you design solutions to problems (like an Uber AI, Twitter etc research scientist role) if you're not Karpathy or Goodfellow (you might be...I am not!)?\n3. Is the fact that I see the PhD as an opportunity to pursue research and at the same time enhance the aforementioned skills a red flag? Currently, the ML jobs that I find fulfilling are either ML Engineer-that requires good coding skills or ML Scientist positions where 9 times out of 10 a PhD is a prerequisite. Would an alternative route of taking some months to improve on the aforementioned skills make more sense than committing to a PhD?\n\nI am particularly interested in stories that dissuade mere mortals like me to follow the PhD route and how you guys handle the mental pressure. Long replies are hugely encouraged! \n\nThank you and nice to meet you!"}, {"id": "oep2gh", "title": "[Research] Is there a name for \"passive\" LIME?", "score": 2, "url": "https://www.reddit.com/r/MachineLearning/comments/oep2gh/research_is_there_a_name_for_passive_lime/", "author": "solresol", "subreddit": "r/MachineLearning", "description": "Generally when people talk about using LIME to explain some classifier or regressor, they have access to the predictor as a black-box. So while you don't know how the black-box works (obviously, you want to explain it, that's why you're using LIME) you can put in different values and see what comes out.\n\nWhat do you call it if you don't have access to the black-box predictor? All you have access to is a set of inputs (chosen by someone else) and their corresponding outputs. You want to build an explanation of what the black-box predictor appears to have done.\n\nIs there a name for this?"}, {"id": "oeofsg", "title": "[P] Multiple text summarization with a twist", "score": 1, "url": "https://www.reddit.com/r/MachineLearning/comments/oeofsg/p_multiple_text_summarization_with_a_twist/", "author": "eneskristo", "subreddit": "r/MachineLearning", "description": "Hey, I have the following problem at hand. I have multiple trivia questions, called tossups. The format of a tossup is that it pyramidal, which means that the clues get progressively easier, and you can buzz at any point, and attempt to answer the question. Here is an example of a tossup:\n\nWhen Heracles visited the underworld, the shades of Meleager and this character were the only two who did not flee his presence. This character\u2019s blood was used as a panacea by Asclepius to raise the dead. The quest to locate and kill this character was undertaken by a hero who had no horse to offer as a gift during Hippodamia\u2019s wedding; that man found this figure by stealing an eye from three blind sisters. After this figure died, Chrysaor and Pegasus emerged from her body. This mortal sibling of Stheno and Euryale [yur-ee-AH-lee] was cursed with her appearance after making love to Poseidon in a temple of Athena, who used a shield with this person\u2019s face on it as a weapon. For 10 points, name this Gorgon beheaded by Perseus to stop her from turning people to stone.\n\nAnswer: Medusa\n\nAs you can see, the initial clues are very hard, while the later ones are things that would be known to people even if they have no particular interest at myth.\n\nNow, there are only so many possible answerlines, but there are plenty of clues. To give an example, here is a second question on the Medusa.\n\nAccording to Ovid, this figure's blood was said to have caused the formation of corals in the Red Sea. Unlike her siblings Stheno and Euryale, this daughter of Keto and Phorcys was mortal. In one story, this figure was cursed after she was caught making love with Poseidon in Athena's temple. This figure's children included Chrysaor and Pegasus, who were born after she was beheaded by Perseus. For 10 points, name this Gorgon with snake-like hair whose gaze could turn people to stone.\n\nYou see, the later clues are similar, while the earlier ones aren't. Hence you are rewarded for having deep knowledge, while not locked out of the question if the opponent doesn't buzz.\n\nNow, if you could collate twenty tossups into one tossup with the important info, that would be very helpful. Essentially im looking for a way to do multiple text summarization. However, there is a problem when using more traditional methods; the summary would just consist of the For Ten Points parts of various questions! This is useful, but not great info. Ideally, I would like 4-5 sentences with reasonable summaries without repeating clues, which probably means an RNN solution. I could find some papers, unfortunately there wasn't much in github. Anyone know how to approach this problem?"}, {"id": "odys6c", "title": "[D] Has anyone used \"copulas\" before?", "score": 85, "url": "https://www.reddit.com/r/MachineLearning/comments/odys6c/d_has_anyone_used_copulas_before/", "author": "blueest", "subreddit": "r/MachineLearning", "description": "https://en.wikipedia.org/wiki/Copula_(probability_theory)\n\nIf I understand correctly, \"copulas\" are methods that attempt to estimate the joint probability distribution of multivariate data.\n\nSuppose I have a data with 2 variables: salary and weight. \n\nFirst dataset: I have this data for a group of people from 2010\n\nSecond Dataset: I have this data for the same group of people from 2020.\n\nCan copulas be used in this problem for some interesting purpose?  \n\n- Suppose a copula fits the first dataset well, and can then be used to find the joint probability distribution. Could we use the copula to generate new hypothetical (weight, salary) measurements having the same distribution as the first dataset?\n\n- Can copulas be used to determine how different the first dataset is from the second dataset?\n\n- Suppose for the second dataset, the goal is to predict whether an individual is employed or unemployed at the time of measurement.  Let's assume that the for the second dataset, we have : weight, salary and employment status (binary: 1 or 0). Can copulas be used to tell us something interesting in this example?\n\nOverall, are copulas even useful in this imaginary problem I invented?\n\nThanks"}, {"id": "oe374e", "title": "[D] Best practices of storing annotations for image data", "score": 28, "url": "https://www.reddit.com/r/MachineLearning/comments/oe374e/d_best_practices_of_storing_annotations_for_image/", "author": "crazyfrogspb", "subreddit": "r/MachineLearning", "description": "What are the best practices of storing annotations (bounding boxes, masks) and other kind of meta data for image datasets?\n\nThere are some options that we tried out or considering right now:\n\n1) JSON/CSV + storing train/val/test splits in the repo\n\n2) JSON/CSV + DVC\n\n3) Relational database\n\nAll of them seem to have some pros and cons. Here are the factors that we consider:\n\n1) Simplicity. Simple JSONs win here, there is no need to maintain any DB or use an external tool.\n\n2) Reproducibility of experiments. DVC leads here, whereas it might be difficult to version control a DB.\n\n3) Clear structure + Ability to easily add new annotations and information - databases seem to win here. We mainly work with medical images, and there is a large nested structure of the data (e.g., hospital - patient - study - left/right breast - CC/MLO view - annotation). In addition, there are usually multiple annotators per image. Storing and linking all this information in CSV/JSONs slowly becomes a nightmare.\n\n&#x200B;\n\nOverall, it seems like there is no clear winner. Are we missing any other options?"}, {"id": "oebe9q", "title": "[D] Using GAN to enhance VQVAE auto encoder.", "score": 5, "url": "https://www.reddit.com/r/MachineLearning/comments/oebe9q/d_using_gan_to_enhance_vqvae_auto_encoder/", "author": "chasep255", "subreddit": "r/MachineLearning", "description": "I have a VQVAE model that is producing reasonable sound using just a straight forward multi spectral loss function similar to than in the parallel wave gan ([https://arxiv.org/pdf/1910.11480.pdf](https://arxiv.org/pdf/1910.11480.pdf)).  While it does sound like the original audio it is far from perfect and kind of crackly.  To address this I have been working on adding the adversarial component to my model.  My model is as follows...\n\nRaw Audio -> Encoder -> Codebook -> Decoder -> Discriminator + Multi Spectral Loss\n\nThe loss function is a combination of the multispectal loss, and adversarial loss (I am using Least Squares GAN.)  The issue I keep running into is that one of the loss terms tends to dominate the other depending on how I weight them.  If I use a weight of 4 for the adversarial term (as suggested in the paper) my model tends to ignore its inputs and just converge on something else.  If I lower the weight of the adversarial loss my discriminator can perfectly differentiate real/fake, and the decoder never manages to overcome this.\n\nWhat I would like is to have only an adversarial loss for my decoder.  However, the issue here is that the discriminator is only being asked to tell real vs fake. What I really want it to discern is whether the generated audio matches that of the input.  So at first I thought of maybe using an autoencoder as part of my discriminator which would be fed the original sample.  This way the discriminator could be conditioned on what the generator should sound like.  Then I thought more about it and decided against this.  What I figured is the discriminator could learn something like a hash function for the original sound and use that to perfectly tell apart real/fake despite having the bottle neck.  Also I think I would need to balance the amount of information the discriminator and decoder gets since with this architecture the decoder can only be as good as what the discriminator can teach it.\n\nAfter some more thought I came up with the following idea...\n\nFirst I would train a normal non-adversarial model using just multi spectral loss like I had done before.\n\nRaw Audio -> Encoder -> Codebook -> Decoder -> Multi Spectral Loss\n\nThen I would take just the Encoder/Codebook part, and use these codes to condition both the generator and discriminator of a GAN.  Note that the weights for the encoder are frozen at this point and will no longer be updated.\n\nRaw Audio -> Encoder -> Codebook -> Generator -> Discriminator (Codes from codebook also input)  \n\n\nI kicked off training this morning and so far the generator seems to be converging on the correct thing.  Let me know what you guys think or if you have any suggestions."}, {"id": "oe4o1z", "title": "[D] What kinds of abstract mathematics are currently being used to formalize concepts in ML and Neural Networks", "score": 12, "url": "https://www.reddit.com/r/MachineLearning/comments/oe4o1z/d_what_kinds_of_abstract_mathematics_are/", "author": "brazdaph", "subreddit": "r/MachineLearning", "description": "I searched up the concept and found some documents on bayesian category theory being used to describe neural networks. However, the document is 8 years old and I'm curious if there are more current and useful abstract fields being used on the theory side of ML."}, {"id": "oeaopr", "title": "[R] Better Lee Bounds -- Machine Learning for Partial Identification", "score": 3, "url": "https://www.reddit.com/r/MachineLearning/comments/oeaopr/r_better_lee_bounds_machine_learning_for_partial/", "author": "bikeskata", "subreddit": "r/MachineLearning", "description": "Of interest for those of you who do causal inference, and have to worry about things like selection into treatment. It's a way to get partial identification bounds, and perform (statistical) inference on these bounds.\n\n\"This paper develops methods for tightening Lee (2009) bounds on average causal effects when the number of pre-randomization covariates is large, potentially exceeding the sample size. These Better Lee Bounds are guaranteed to be sharp when few of the covariates affect the selection and the outcome. If this sparsity assumption fails, the bounds remain valid. I propose inference methods that enable hypothesis testing in either case. My results rely on a weakened monotonicity assumption that only needs to hold conditional on covariates. I show that the unconditional monotonicity assumption that motivates traditional Lee bounds fails for the JobCorps training program. After imposing only conditional monotonicity, Better Lee Bounds are found to be much more informative than standard Lee bounds in a variety of settings. \"\n\nPaper: https://arxiv.org/abs/2008.12720\n\nR Package: https://github.com/vsemenova/leebounds"}, {"id": "odkdsv", "title": "[D] Growing beyond a deep learning PhD", "score": 406, "url": "https://www.reddit.com/r/MachineLearning/comments/odkdsv/d_growing_beyond_a_deep_learning_phd/", "author": "Jazzlike-Disaster-67", "subreddit": "r/MachineLearning", "description": "Hi, throwaway because everyone in my lab uses reddit.\n\nI am doing a PhD in machine learning but my field is heavily based in computer vision and also some techniques from natural language processing, so I'm mostly doing deep learning.\n\nI have some conference contributions, but none of them in major conferences. Reviewers are always fairly critical but I have not gotten a rejection yet (though last time was pretty close).\n\nI get why they are critical too. I'm not a top student, our lab is not a top lab, and what I do is mostly repurpose existing methods for different domains. Think taking a ResNet and applying it to medical imaging, or transformers for music classification (not actually my domains).\n\nI feel like compared to many others, I heavily lack in mathematical background even though I try to read up, I often immediately forget concepts that I don't actually apply. I couldn't tell you what the rank of a matrix is, let alone how to use it.\n\nThis is partly why I don't really come up with new methods. I'm better at combining existing stuff, but it doesn't feel like research but more like engineering at times.\n\nBecause my contributions are fairly underwhelming, I don't think I will be able to achieve a career in academia. So I will likely look for a job in the industry.\n\nBut there I would like to be able to show something more than \"I applied method X to data Y and got a slightly better result so I published it\".\n\nDo you have any tips for (1) growing beyond the niche of your PhD, and (2) making actual contributions that are not purely incremental and applied during your PhD?\n\nPerhaps side projects that I should do if I have some left over energy in the weekend?\n\nThanks."}, {"id": "oe6m2f", "title": "[D] What's the appropriate ML term for this seemingly \"Transfer Learning of Transfer Learning\"?", "score": 5, "url": "https://www.reddit.com/r/MachineLearning/comments/oe6m2f/d_whats_the_appropriate_ml_term_for_this/", "author": "sarmientoj24", "subreddit": "r/MachineLearning", "description": "The problem is Object Detection.\n\nSay, I have a pretrained ResNet model (for example) in ImageNet, and I want to solve a problem X.\n\nI have atleast 2 publicly available dataset on the domain of X. And I have my dataset of a smaller, decent size on the same domain X but with different classes.\n\nSay, I trained my object detection model in one of the larger publicly available dataset to learn the features on the domain X.\n\nAfter this, I use the trained model as a starting model to solve my problem with my own dataset.\n\nWhat is the ML term for this? I was thinking ***Representation Learning*** because you are basically learning the representation and features of domain X by utilizing all publicly available datasets to your advantage so that the model is better when you will use it on your problem.\n\n\\-----\n\nTo make it more tangible.\n\nSay there is a publicly available dataset of Object Detection with the classes -- trousers, shirts, pants, and sneakers. This is dataset ***A.***\n\nThen I have a smaller dataset that I can annotate which has slightly different classes and outcomes for Object Detection. Say the classes would be -- buttoned shirt, khaki pants, shorts, track pants, long sleeves, hoodies, etc. This is dataset ***B.***\n\nI train the Object Detection model with dataset A producing trained model ***M.***\n\nThen using ***M,*** as the pretrained model, to solve my task with dataset ***B.***\n\nIt does look like it's Transfer Learning of Transfer Learning since I transferred the knowledge gained from ImageNet and dataset A into solving my task with dataset B. But there might be term in ML for this."}, {"id": "oe5p0v", "title": "[D] Unsupervised instance segmentation?", "score": 4, "url": "https://www.reddit.com/r/MachineLearning/comments/oe5p0v/d_unsupervised_instance_segmentation/", "author": "DiracShore", "subreddit": "r/MachineLearning", "description": "Hi there,\n\nIs anyone aware of good methods to perform unsupervised instance segmentation?\n\nI have a large dataset of images, and each image contains multiple objects that often overlap. My goal is to perform instance segmentation, such that for every object, I'd get a different per-pixel prediction value.\n\nI know of one method that uses an adversarial approach (ReDO), but I'm looking for alternatives as well.\n\nThanks!"}, {"id": "oe6hy3", "title": "[Project] Yolo Oriented BBoxes", "score": 2, "url": "https://www.reddit.com/r/MachineLearning/comments/oe6hy3/project_yolo_oriented_bboxes/", "author": "PovoliSergio", "subreddit": "r/MachineLearning", "description": "Hello everyone, \n\nI'm trying to understand the feasibility of implementing a yolo network so that it also predicts bounding box orientation. On a theoretical level it is very feasible but on a practical level, starting from yolo implementations found online, it seems to me to be more complex (I am a newbie with coding these architectures).\n\n&#x200B;\n\nWould anyone be able to put me on the right track and help me in this process? My idea is to start from this implementation: [https://github.com/emadboctorx/yolo-tf2#usage](https://github.com/emadboctorx/yolo-tf2#usage) . Obviously the loss function would have to be modified, the IoU calculation would have to be modified and then I would have to figure out what else to modify. \n\n&#x200B;\n\nThanks to anyone who wants to help me with this project and anyone who leaves some ideas!"}, {"id": "oe1tp9", "title": "[D] Is this a reasonable assumption in machine learning?", "score": 3, "url": "https://www.reddit.com/r/MachineLearning/comments/oe1tp9/d_is_this_a_reasonable_assumption_in_machine/", "author": "ottawalanguages", "subreddit": "r/MachineLearning", "description": "1) Suppose you train a random forest model on some data and the model performs well. Now, you have new data that arrives. \n\nA) If you could somehow determine that the new data is VERY DIFFERENT from the data used to train the model - could you make a statement and say \"we are not confident about the performance of the random forest on this new data, because the new data appears to be quite different from the old data\"?\n\nB) If you could somehow determine that the new data is NOT VERY DIFFERENT from the data used to train the model - could you make a statement and say \"we are more confident about the performance of the random forest on this new data, because the new data appears to be quite similar to the old data\"?\n\nAre A) and B) reasonable assumptions? It seems to me that the ability of statistical/machine learning models to generalize to new data should (in part) depend on how similar this new data is to the data that the model has already seen? For example, if I only watched the movie \"Toy Story\" and was asked to write a true/false exam about the sequel \"Toy Story 2\" - I would probably do better on this exam compared to an exam on the movie \"Finding Nemo\"? Is my understanding correct?\n\n2) Also - can the KL Divergence be used to decide how different new data is compared to the data that the model was trained on? I am not sure if the KL Divergence can be used like this, because the KL Divergence between \"distribution p\" and \"distribution q\" is not necessarily the same between \"distribution q\" and distribution p\". (e.g. [https://stackoverflow.com/questions/19179210/kullback-leibler-distance-between-2-samples](https://stackoverflow.com/questions/19179210/kullback-leibler-distance-between-2-samples) )\n\nCan someone please provide some comments?\n\nThanks"}, {"id": "oe7yjt", "title": "[P] Is it possible to create an aiming software with machine learning?", "score": 1, "url": "https://www.reddit.com/r/MachineLearning/comments/oe7yjt/p_is_it_possible_to_create_an_aiming_software/", "author": "l1x-", "subreddit": "r/MachineLearning", "description": "I would like to create a system that sprays water on animals to get them out from our garden (as opposed to get a licenses hunter and kill them). The foxes use our garden as a cemetery for the prey they eat and I have to burn and burry animal corpses every week (we have small children and I am afraid of rabies). I was thinking about installing cameras and create a model that can classify animals. That part is sort of easy. The second part would be a system that sprays water on them. That one I am not sure of. I was trying to find aiming software but not even sure what to google for.\n\nDoes anybody have any experience with this?"}, {"id": "odtx7w", "title": "[D] Top conferences about machine learning on time series data", "score": 7, "url": "https://www.reddit.com/r/MachineLearning/comments/odtx7w/d_top_conferences_about_machine_learning_on_time/", "author": "ihatebeinganonymous", "subreddit": "r/MachineLearning", "description": "Hi,\n\nWe all know where to go see SotA and read about machine learning on text data (ACL), visual data (CVPR etc..), or speech data (ICASSP/INTERSPEECH). But what is the equivalent venue, if any, for time-series analysis? Is it ICASSP? (I don't think so)\n\nMoreover, what are some famous research groups doing machine learning on time series data?\n\n&#x200B;\n\nMany thanks"}, {"id": "odl6as", "title": "[D] Is deep learning having an impact in life sciences yet?", "score": 24, "url": "https://www.reddit.com/r/MachineLearning/comments/odl6as/d_is_deep_learning_having_an_impact_in_life/", "author": "yousingcalmincense", "subreddit": "r/MachineLearning", "description": "In particular, is it having an impact in industry to the point that it is actually improving products or processes?\n\nIf not, why?\n\nIf so, what examples can you point to?\n\nSequencing and medical imaging come to mind as areas that probably have large datasets to work on with DL but are there other areas and problems that look promising for DL algorithms? What are the most exciting modeling approaches in those areas? I know there have been a number of papers on protein language modeling from FAIR, including MSA Transformer, but any other papers you would like to share?\n\nDo you know of any life science companies with a big DL effort? Are life science companies staffing up with DL scientists mostly on hope and hype or are there starting to be concrete improvements from DL that industry can leverage?"}, {"id": "oe267s", "title": "[D] Real-Time Continuous Model Improvement - Is it doable?", "score": 0, "url": "https://www.reddit.com/r/MachineLearning/comments/oe267s/d_realtime_continuous_model_improvement_is_it/", "author": "IamJustAWizard", "subreddit": "r/MachineLearning", "description": "Let's say there is a trained NER model, and a UI where user can see entities for the text they have entered.\n\nThey also have possibility to correct NER model detected entities by either removing labels, adjusting label length, or marking new entities.\n\nIs it possible to take this feedback into consideration in real-time, and improve the NER model on the fly WITHOUT fully re-training (w/ feedback) the model?\n\nAre there any practical examples of this?\n\nHow would you approach this or how others are doing it in the industry?"}, {"id": "odsfyx", "title": "[D] Recent papers on RNN Explainability", "score": 5, "url": "https://www.reddit.com/r/MachineLearning/comments/odsfyx/d_recent_papers_on_rnn_explainability/", "author": "MafiaSkafia", "subreddit": "r/MachineLearning", "description": "Hello, so i've been dabbling around explainability in ml algortithms, more specificially in various types of NNs. I've came around lots of papers about gradient methods, perturbance methods, and post-hoc methods like SHAP. This latter has also been used to try and explain RNNs, but i feel that there are 2 major downsides to it:\n\n- huge computational cost\n- feature independence assumption\n\nAnd so, i've been looking for more recent papers on explainability in RNNs (or any similar networks), that would be model agnostic and wouldn't have these limitations. Do you guys know any recent papers on this topic?\n\nThank you in advance."}, {"id": "odlesi", "title": "[P] Analyzing and summarizing backpack reviews with AutoML, BERT, and GPT-3", "score": 16, "url": "https://v.redd.it/ffod0q8ag7971", "author": "Tom-Logan", "subreddit": "r/MachineLearning", "description": ""}, {"id": "odm1q7", "title": "[D] Moving from Machine Learning Engineer to Software Engineer", "score": 12, "url": "https://www.reddit.com/r/MachineLearning/comments/odm1q7/d_moving_from_machine_learning_engineer_to/", "author": "Lewba", "subreddit": "r/MachineLearning", "description": "My current job, and my first post-masters, is as an MLE. Recently I've been looking for new jobs and have been applying to SWE positions too as I feel I, and possibly many other DS/MLEs, could benefit from having some professional backend experience in the long run. Do you think this is a good option, or will it be hard to get back into an ML position in a few years time?"}, {"id": "od2csk", "title": "[P] DeepLab2: A TensorFlow Library for Deep Labeling Web Demo", "score": 574, "url": "https://i.redd.it/e5uefg6381971.png", "author": "Illustrious_Row_9971", "subreddit": "r/MachineLearning", "description": ""}, {"id": "oe3d4j", "title": "COMPAS software - how does it work?!", "score": 0, "url": "https://www.reddit.com/r/MachineLearning/comments/oe3d4j/compas_software_how_does_it_work/", "author": "cherriO5", "subreddit": "r/MachineLearning", "description": "hey everyone! I'm a complete layman in this field so sorry if my question is badly put. Could anyone explain to me the type of machine learning method that is used for the COMPAS software, which assesses the likelihood of a defendant becoming a recidivist? [https://en.wikipedia.org/wiki/COMPAS\\_(software)](https://en.wikipedia.org/wiki/COMPAS_(software))\n\nMany thanks!!"}, {"id": "odnuwm", "title": "Additional steps for small n, small p regression problem [Discussion]", "score": 4, "url": "https://www.reddit.com/r/MachineLearning/comments/odnuwm/additional_steps_for_small_n_small_p_regression/", "author": "akirp001", "subreddit": "r/MachineLearning", "description": "I have a dataset with about 2k rows and 10 features. The features, distribution wise, are all normal. The target variable is as normally distributed as you will find in the real world. \n\nStandard OLS produces an r2 of about 25%. XGBoost gets up to 30%. And both produce residuals that are also normally distributed around 0 with no obvious showing off bias in any one direction.\n\nI'm going to try DL next, but Im not sure how much extra mileage I can squeeze out of that. And so I have been thinking what other things are worth doing to improve model performance while preserving this as a regression problem.\n\nSome guides suggest using Gan's to generate synthetic data but I feel like that isn't likely to achieve much.\n\nAnyone have a suggestion on something I'm missing?"}, {"id": "odbl3o", "title": "[P] Open-source Neural Search framework to implement semantic search & multimedia search. Just released 2.0, seeking your feedback.", "score": 23, "url": "https://www.reddit.com/r/MachineLearning/comments/odbl3o/p_opensource_neural_search_framework_to_implement/", "author": "opensourcecolumbus", "subreddit": "r/MachineLearning", "description": "I heard your feedback on [1.0 release post](https://www.reddit.com/r/MachineLearning/comments/n0v5jy/project_framework_to_build_ai_powered_search_with/)  on my project [Jina](https://github.com/jina-ai/jina/), many people were keen to use Jina for multimedia search because that's where use of Neural Networks makes significant difference. So I focused on that part and I was able to transform it from 1.0 to 2.0 within 3 months.\n\n[Last post on 1.0 release to give you some idea what this project is about](https://preview.redd.it/wa9hmcg5y3971.png?width=726&format=png&auto=webp&s=47eae65cedd81cec6be1350e04d9c22a2b34541d)\n\nActually, I should say - \"'we' made this\", because there were more than 155 contributors who did it, not just me. The primary changes we made\n\n* We saw MachineLearning beginners struggle in using Jina 1.0, so we separated the codebase where Machine Learning expertise is required([jina-hub](https://github.com/jina-ai/jina-hub)) and the one which MachineLearning beginners can use(the [jina](https://github.com/jina-ai/jina/) core). Now ML beginners don't need to worry about jina-hub and can use jina hub packages directly to implement ML specific tasks without the need to understand advanced ML concepts. While advanced ML users can create their own jina-hub packages.\n* We cut down a lots of abstractions to make it easy to use for beginners\n* Made python APIs more intuitive to use\n* Improved performance(3.6x faster on startup) \n\nHere's [Jina 2.0](https://github.com/jina-ai/jina/) and here's [Jina 1.0](https://github.com/jina-ai/jina/tree/v1.0.0).  I seek feedback from people who are looking at this project for the  first time, as well as people who have tried their hands before but had some challenges in using it. Few questions, I'm seeking answers to\n\n1. Do you feel that we have reduced complexity by a lot of margin?\n2. How easy it is to use for a beginner now?\n3. What questions are still unanswered?"}, {"id": "odqspg", "title": "[P] AI Tournament - Reinforcement Learning Competition Final", "score": 1, "url": "/r/reinforcementlearning/comments/occobf/ai_tournament_reinforcement_learning_competition/", "author": "DIAMBRA_AIArena", "subreddit": "r/MachineLearning", "description": ""}, {"id": "odojdu", "title": "[D] Journal Subscriptions", "score": 0, "url": "https://www.reddit.com/r/MachineLearning/comments/odojdu/d_journal_subscriptions/", "author": "Eve26th", "subreddit": "r/MachineLearning", "description": "Any good machine learning journal subscriptions in paperback(in Europe) at reasonable price?"}, {"id": "od8nfi", "title": "[D] Alternatives to W&B?", "score": 19, "url": "https://www.reddit.com/r/MachineLearning/comments/od8nfi/d_alternatives_to_wb/", "author": "Seankala", "subreddit": "r/MachineLearning", "description": "Hi. I've been using W&B for a while now and it's overall been great but I've been waking up to more \"network errors\" than I'd like that caused programs I'd been running to crash. I don't mind using W&B for more \"lighthearted\" projects but am becoming a bit concerned about using it for time-sensitive projects.\n\nDoes anybody know of any alternatives that may be worth looking into? I don't mind using simple logging or even TensorBoard again but am curious if anyone may know of anything. Thanks."}, {"id": "od0sjc", "title": "[R] Google AI Introduces A Machine Learning Based System For Game Developers To Quickly And Efficiently Train Game-Testing Agents", "score": 61, "url": "https://www.reddit.com/r/MachineLearning/comments/od0sjc/r_google_ai_introduces_a_machine_learning_based/", "author": "techsucker", "subreddit": "r/MachineLearning", "description": "Google AI recently announced a machine learning-based framework that game developers could use to train game-testing agents quickly and efficiently, freeing human testers to focus on more complicated problems. The resulting system requires no machine learning (ML) expertise, works with a wide range of popular game genres, and can train an ML policy, which generates game actions from the game state on a single game instance in less than an hour. Google AI has also provided an\u00a0[open-source library](https://github.com/google-research/falken)\u00a0that shows how these techniques may be used in practice.\n\nFull Story: [https://www.marktechpost.com/2021/07/03/google-ai-introduces-a-machine-learning-based-system-for-game-developers-to-quickly-and-efficiently-train-game-testing-agents/](https://www.marktechpost.com/2021/07/03/google-ai-introduces-a-machine-learning-based-system-for-game-developers-to-quickly-and-efficiently-train-game-testing-agents/) \n\nGoogle blog: [https://ai.googleblog.com/2021/06/quickly-training-game-playing-agents.html](https://ai.googleblog.com/2021/06/quickly-training-game-playing-agents.html)\n\nGithub: https://github.com/google-research/falken"}, {"id": "odfuax", "title": "[R] Stanford AI Lab Introduces AGQA: A New Benchmark For Compositional, Spatio-Temporal Reasoning", "score": 2, "url": "https://www.reddit.com/r/MachineLearning/comments/odfuax/r_stanford_ai_lab_introduces_agqa_a_new_benchmark/", "author": "techsucker", "subreddit": "r/MachineLearning", "description": "Designing machines capable of exhibiting a compositional understanding of visual events has been an important goal of the computer vision community. Stanford AI has recently introduced the benchmark,\u2019 Action Genome Question Answering\u2019 (AGQA). It measures temporal, spatial, and compositional reasoning via nearly two hundred million question answering pairs. The questions are complex, compositional, and annotated to allow definitive tests that find the types of questions that the models can and cannot answer.\n\nThe researchers designed a synthetic generation process using rules-based question templates to generate questions from scene information, representing what occurs in the video using symbols. Synthetic generation enables researchers to control the content, structure, and compositional reasoning steps needed to answer each generated question.\n\nFull Story: [https://www.marktechpost.com/2021/07/04/stanford-ai-lab-introduces-agqa-a-new-benchmark-for-compositional-spatio-temporal-reasoning/](https://www.marktechpost.com/2021/07/04/stanford-ai-lab-introduces-agqa-a-new-benchmark-for-compositional-spatio-temporal-reasoning/) \n\nPaper: https://arxiv.org/pdf/2103.16002.pdf\n\nData: https://cs.stanford.edu/people/ranjaykrishna/agqa/"}, {"id": "oczyne", "title": "[N] NeurIPS 2021 Open Competitions", "score": 59, "url": "https://www.reddit.com/r/MachineLearning/comments/oczyne/n_neurips_2021_open_competitions/", "author": "SubstantialRange", "subreddit": "r/MachineLearning", "description": "[https://neurips.cc/Conferences/2021/CompetitionTrack](https://neurips.cc/Conferences/2021/CompetitionTrack)"}, {"id": "odna6t", "title": "[D] separable convolution vs normal convolution", "score": 0, "url": "https://www.reddit.com/r/MachineLearning/comments/odna6t/d_separable_convolution_vs_normal_convolution/", "author": "kns2000", "subreddit": "r/MachineLearning", "description": "When is it better to use separable convolution instead of normal convolution for image recognition tasks? What are the pros and cons of separable convolutions?"}, {"id": "ocytwv", "title": "[D] Experts Created Blogs And Communities On Machine Learning And Deep Learning", "score": 52, "url": "https://www.theinsaneapp.com/2021/04/top-machine-learning-blogs-to-follow-in-2021.html", "author": "TheInsaneApp", "subreddit": "r/MachineLearning", "description": ""}, {"id": "odiv6l", "title": "[D] What is the meaning of the c hyper parameter in LSGAN+GP?", "score": 0, "url": "https://www.reddit.com/r/MachineLearning/comments/odiv6l/d_what_is_the_meaning_of_the_c_hyper_parameter_in/", "author": "chasep255", "subreddit": "r/MachineLearning", "description": "I have been trying to figure out the paper at this url [https://www.arxiv-vanity.com/papers/1712.06391/](https://www.arxiv-vanity.com/papers/1712.06391/).  What I can't figure out is the meaning of the c hyper parameter.  I am trying to add gradient penalty to my own Least Squares GAN.  I understand the meaning of lambda which is the weight of the gradient penalty, but I am unsure what c is.  I can't find any other reference to it in the paper.  \n\n*LSGANs and set the hyper-parameters c and \u03bb to 30 and 150, respectively. For this experiment, our implementation is based on the official implementation of WGANs-GP.*"}, {"id": "ocrm9x", "title": "[N] Distill.pub is going on hiatus", "score": 226, "url": "https://www.reddit.com/r/MachineLearning/comments/ocrm9x/n_distillpub_is_going_on_hiatus/", "author": "regalalgorithm", "subreddit": "r/MachineLearning", "description": "[Distill.pub](https://Distill.pub) has announced [it's going on hiatus](https://distill.pub/2021/distill-hiatus/), possibly indefinitely. Sad news for sure..."}, {"id": "odd3u2", "title": "[D] ICML 2021", "score": 3, "url": "https://www.reddit.com/r/MachineLearning/comments/odd3u2/d_icml_2021/", "author": "stantheta", "subreddit": "r/MachineLearning", "description": "Is anyone here volunteering in ICML 2021 ? Would like to connect with them before the event starts."}, {"id": "odnjnd", "title": "[R] Google AI Introduces A Dataset for Studying Gender Bias in Machine Translation", "score": 0, "url": "https://www.reddit.com/r/MachineLearning/comments/odnjnd/r_google_ai_introduces_a_dataset_for_studying/", "author": "techsucker", "subreddit": "r/MachineLearning", "description": "[Neural machine translation (NMT)](https://ai.googleblog.com/2020/06/recent-advances-in-google-translate.html) has been growing with leaps and bounds, and it has also enabled natural and fluid translations to a considerable degree. However, societal bias has been one problem that has been reflected time and again because of the stereotypical data already present while training the machine learning models. Gender, in particular, is an extremely sensitive issue wherein picking the correct pronouns is of utmost significance because it directly refers to how people self-identify. Google has claimed that it has been working to reduce the biases present with the help of innovative techniques and by using machine learning principles.\n\nFull Story: [https://www.marktechpost.com/2021/07/04/google-ai-introduces-a-dataset-for-studying-gender-bias-in-machine-translation/](https://www.marktechpost.com/2021/07/04/google-ai-introduces-a-dataset-for-studying-gender-bias-in-machine-translation/) \n\nDataset: [https://storage.googleapis.com/gresearch/translate-gender-challenge-sets/Readme.html](https://storage.googleapis.com/gresearch/translate-gender-challenge-sets/Readme.html)\n\nGoogle Blog: [https://ai.googleblog.com/2021/06/a-dataset-for-studying-gender-bias-in.html](https://ai.googleblog.com/2021/06/a-dataset-for-studying-gender-bias-in.html)"}, {"id": "odjl48", "title": "[D] Determining which algorithm to use for different datasets", "score": 0, "url": "https://www.reddit.com/r/MachineLearning/comments/odjl48/d_determining_which_algorithm_to_use_for/", "author": "Katarika", "subreddit": "r/MachineLearning", "description": "Is their a way to determine which algorithm to use when developing models for different datasets? Or one chooses the one that looks suitable for that dataset."}, {"id": "odg8x6", "title": "[D] What is the equivalent of muscle memory in deep/reinforcement learning?", "score": 0, "url": "https://www.reddit.com/r/MachineLearning/comments/odg8x6/d_what_is_the_equivalent_of_muscle_memory_in/", "author": "EstablishmentOdd785", "subreddit": "r/MachineLearning", "description": "Imagine an agent that has learned to ride a bike, but then switches to learning to swim and then after a while tries to ride a bike again. Cell state in LSTMs feel relevant?  \n\n\np.s. a Brief intro  [Muscle memory is real, but it\u2019s probably not what you think (popsci.com)](https://www.popsci.com/what-is-muscle-memory/)"}, {"id": "odfkmv", "title": "[D] Looking for a data/model management tool/solution", "score": 1, "url": "https://www.reddit.com/r/MachineLearning/comments/odfkmv/d_looking_for_a_datamodel_management_toolsolution/", "author": "rexlow0823", "subreddit": "r/MachineLearning", "description": "Hey devs!\n\nI\u2019ve got a MLOps question here. How do you guys manage data or perhaps what tools/stacks do you use to version control your data and models? \n\nI recently discovered Kubeflow with add-ons like Rok but it\u2019s so painful to deploy in an on-premise cluster (mainly some networking bugs in the library)\n\nWould love to explore some other options. I\u2019m looking for a framework-agnostic solution here. Thanks!"}, {"id": "ocx9tt", "title": "[D][N] \"The Many Paths to Understanding Deep Learning\" closing panel with Brendan Fong, Karolina Dziugaite, Oriol Vinyals and Yasaman Bahri", "score": 28, "url": "https://www.reddit.com/r/MachineLearning/comments/ocx9tt/dn_the_many_paths_to_understanding_deep_learning/", "author": "joaogui1", "subreddit": "r/MachineLearning", "description": "Hi folks! Me and some friends from data-ICMC have been organining a series of seminars that will conclude this Friday with a closing panel with amazing speakers   \n\n\n* Brendan Fong - Chief Executive of the Topos Institute, founding editor of **Compositionality** and author of An Invitation to Applied Category Theory\n* Karolina Dziugaite - Lead Research Scientist at ElementAI and associate member Mila \n* Oriol Vinyals - author of Seq2Seq, Pointer Networks, WaveNet, AlphaStar and many other amazing papers\n* Yasaman Bahri -  Research Scientist at Google Brain with amazing works on using ideas from Physics to better understand the behavior of Neural Networks\n\nThe panel will happen on July 9th 15:00-17:00 UK time/7:00-9:00 PST, please come watch and ask your questions!  \n[Link](https://twitter.com/_joaogui1/status/1409209399114076167) to the twitter thread  \n[Link](https://www.youtube.com/watch?v=NfvYufQwA_o) to the panel on Youtube"}, {"id": "od513e", "title": "[P] MNIST on the browser! (Running PyTorch models client-side)", "score": 3, "url": "https://www.reddit.com/r/MachineLearning/comments/od513e/p_mnist_on_the_browser_running_pytorch_models/", "author": "harjyotbagga", "subreddit": "r/MachineLearning", "description": " I was recently studying about user privacy in Machine / Deep Learning and how a lot of applications are moving towards deploying their model client-side (on the browser itself). While TF-JS is a popular choice amongst people. I came to know about onnx.js, which is a JavaScript library to run models from the browser. The biggest advantage of ONNX is that it allows interoperability across different open source AI frameworks, which itself offers more flexibility for AI frameworks adoption.\n\nSo I decided to implement the very famous MNIST dataset model using PyTorch & help make predictions on the client side. You can check my project here: [https://github.com/harjyotbagga/MNIST-on-the-web](https://github.com/harjyotbagga/MNIST-on-the-web) or it's implementation here: [https://bugz-mnist.herokuapp.com/](https://bugz-mnist.herokuapp.com/)\n\nIf you like the project, please leave a star, it motivates me to work harder :)"}, {"id": "od2l5t", "title": "[Discussion] What does \"attend by relative positions\" in \"Attention Is All You Need\" mean?", "score": 7, "url": "https://www.reddit.com/r/MachineLearning/comments/od2l5t/discussion_what_does_attend_by_relative_positions/", "author": "zeroWorry", "subreddit": "r/MachineLearning", "description": "Hi. There is a paragraph in [the attention paper](https://arxiv.org/abs/1706.03762) under section 3.5 positional encoding that reads:\n\n>We chose this function because we hypothesized it would allow the model  to easily learn to attend by relative positions, since for any fixed  offset k, PEpos+k can be represented as a linear function of PEpos.\n\nProving the latter section i.e. that \"any fixed  offset k, PEpos+k can be represented as a linear function of PEpos\" is rather straightforward by using trigonometric addition formula, but I do not understand their claims:\n\na. what does \"it would allow the model  to easily learn to attend by relative positions\" even mean, and what is its application?\n\nb. how does proving that \"any fixed  offset k, PEpos+k can be represented as a linear function of PEpos\" leads to a proof of their hypotheses?\n\nThe closest I can justify this section to myself is that if each offset is just a  linear transformation of the first positional embedding, then the  series of the self-attention paper's positional embeddings is not an  absolute embedding to begin with, but is an embedding of latent  relative positional transformations. The application of it thus would be that it can be extended to sequences of arbitrary lengths?\n\nMoreover, if their hypothesis is valid, why did we need a [paper that explicitly uses relative positional encoding](https://arxiv.org/pdf/1803.02155.pdf) or one like [reformers](https://arxiv.org/abs/2104.09864) or[transformer-xl](https://arxiv.org/abs/1901.02860)?\n\n&#x200B;\n\nThanks!"}, {"id": "od2evn", "title": "[D]: Does Pose Estimation needs to be trained with data from specific poses?", "score": 7, "url": "https://www.reddit.com/r/MachineLearning/comments/od2evn/d_does_pose_estimation_needs_to_be_trained_with/", "author": "FakeRedditModerator", "subreddit": "r/MachineLearning", "description": "I wanted to start a project with pose estimation but reading the documentation of PoseNet and OpenPose, it seems that the model is already trained, I only need to \u201ccalibrate\u201d it to identify specific poses through key points. \n\nIf you have any tips, don\u2019t hesitate on commenting. Just Do It"}, {"id": "od2x7h", "title": "[D] Exploring node2vec embeddings with graph layouts", "score": 5, "url": "https://github.com/anvaka/explore_node2vec", "author": "anvaka", "subreddit": "r/MachineLearning", "description": ""}, {"id": "odidb5", "title": "[D] Model to tell if your crush is interested in you or not based on your text conversations", "score": 0, "url": "https://www.reddit.com/r/MachineLearning/comments/odidb5/d_model_to_tell_if_your_crush_is_interested_in/", "author": "watermalone_314", "subreddit": "r/MachineLearning", "description": "The good old dilemma whether ask them out or not, is he/she really interested in me? \nI'm experiencing this since about a year now and I'm done overthinking. Let's find a solution for once and all. Today I'm like \"okay she seems interested\" and tomorrow \"Man, this is not gonna work I guess\" \n\nSo this will be basically \"she loves me/she loves me not\" based on actual facts\n\nWouldn't this save tons of overthinking?\ud83d\ude02\nBut seriously, it would be really difficult to achieve a good accuracy though. Despite that, I'll  at least try\n\nWhat do you think about this idea?"}, {"id": "odf5r1", "title": "[D]Is YOLOR Better and Faster than YOLOv4?", "score": 0, "url": "https://www.reddit.com/r/MachineLearning/comments/odf5r1/dis_yolor_better_and_faster_than_yolov4/", "author": "NickFortez06", "subreddit": "r/MachineLearning", "description": "YOLOR just got released, but is it better and faster than YoloV4, Scaled YoloV4, YoloV5 and PP-YOLOv2?\n\nhttps://preview.redd.it/z56doyo495971.png?width=1092&format=png&auto=webp&s=67154ec61951f5871f62ec359e5670e69b388a96\n\nWe\u2019ll In order to answer this question we first have to review this ground breaking academic paper by Chein-Yao Wang and team to find out:  \n\\- What exactly is YoloR,  \n\\- Whats unique in its architecture,  \n\\- How it works,  \n\\- What results they achieved as well as  \n\\- How it compares to other state of the art models.\n\nRead my article here where I explain the details of the Paper: https://augmentedstartups.medium.com/is-yolor-better-and-faster-than-yolov4-54812da66cc1"}, {"id": "ocwzo8", "title": "[R] Measuring the sensitivity of Gaussian processes to kernel choice", "score": 8, "url": "https://www.reddit.com/r/MachineLearning/comments/ocwzo8/r_measuring_the_sensitivity_of_gaussian_processes/", "author": "laneDoc", "subreddit": "r/MachineLearning", "description": "[https://arxiv.org/abs/2106.06510](https://arxiv.org/abs/2106.06510) \n\nhttps://preview.redd.it/9izuake9oz871.png?width=3600&format=png&auto=webp&s=4d28d74f1dc38e47b3b903fecadd3b9fb4d7154a\n\nhttps://preview.redd.it/4km9s6faoz871.png?width=3600&format=png&auto=webp&s=caf9e516a02af4914e4bbd2a570ce375cc929fe1\n\nhttps://preview.redd.it/tu855j7boz871.png?width=1800&format=png&auto=webp&s=dffee4884e357a52658fdaf26c8701166a9ecdff"}, {"id": "ocid8b", "title": "[D] CVPR 2021: paper summaries and highlights (blog post)", "score": 147, "url": "https://www.reddit.com/r/MachineLearning/comments/ocid8b/d_cvpr_2021_paper_summaries_and_highlights_blog/", "author": "youali", "subreddit": "r/MachineLearning", "description": "The 2021 CVPR conference, one of the main computer vision and machine learning conferences, concluded its second 100% virtual version last week with a record of papers presented at the main conference. 1660 papers (vs 1467 papers last year) were accepted with an acceptance rate of 23.7% (vs 22.1% last year). Such a huge (and growing) number of papers can be a bit overwhelming, so if you want to get a quick overview of the conf, I hope this blog post can help with just that.\n\nBlog post: https://yassouali.github.io/ml-blog/cvpr2021/"}, {"id": "od8dgb", "title": "[P] ICYMI - Get code for ML/AI papers anywhere on the internet (Google, Arxiv, Twitter, Scholar, and other sites)!", "score": 0, "url": "/r/LatestInML/comments/od8d6q/icymi_get_code_for_mlai_papers_anywhere_on_the/", "author": "cv2020br", "subreddit": "r/MachineLearning", "description": ""}, {"id": "od70to", "title": "[D] Variable Input Dimension for ML", "score": 1, "url": "https://www.reddit.com/r/MachineLearning/comments/od70to/d_variable_input_dimension_for_ml/", "author": "HFSeven", "subreddit": "r/MachineLearning", "description": "Hi! I am looking into wireless sensors scenario where input to ML model is current network state and based on that classify class of network! It comprises of fixed feature vector of nodes present in the network! Say if we have ten nodes each having two features like [distance to some landmark, energy]. The input sample to ML model consist of taking these features of nodes and based on that classifying the scenario! The feature vector dimension associated to each node is same but the challenge is that the number of nodes vary from one sample to another ( first sample : three nodes, second sample : ten nodes etc)! I strted with assuming fixed number of nodes always in all samples but now want to generalize it to different scenarios!\nWhat do you suggest best approach to tackle this problem! Was thinking of using RNN for coping with variable input dimensions or Graph network! Some simplest approach will be best to start with! Any suggestions/ expert advice?\nOn similar lines: https://datascience.stackexchange.com/questions/37262/dealing-with-feature-vectors-of-variable-length"}, {"id": "ocw2on", "title": "Prophet vs DeepAR Time Series Forecasting [D]", "score": 6, "url": "https://www.reddit.com/r/MachineLearning/comments/ocw2on/prophet_vs_deepar_time_series_forecasting_d/", "author": "lemlo100", "subreddit": "r/MachineLearning", "description": "I\u2018m experimenting with DeepAR on a dataset of sales histories at the moment. I haven\u2019t done time series forecasting before so I\u2019ve been reading a bit. This field seems like deep learning hasn\u2019t really broken through and Prophet seems to be very popular. I have a number of time series in the ten thousands though so DeepAR seemed promising with its global learning approach. I looked at Prophet and was weirded out by the fact that they didn\u2019t  compare their results to any deep learning approaches or at least comment on them."}, {"id": "ocz4z8", "title": "[D] If using sliced data, then should I use the most informative perspectives or other perspectives?", "score": 3, "url": "https://www.reddit.com/r/MachineLearning/comments/ocz4z8/d_if_using_sliced_data_then_should_i_use_the_most/", "author": "mavavilj", "subreddit": "r/MachineLearning", "description": "I'm trying to reassure that my intuition here is correct.\n\nI have sliced data, where between there are intervals of \"missing data\".\n\nIn order to do feature extraction, I've had to think about:\n\n* Should I study the \"whole data\" or the direction(s) with most data?\n\nSo consider e.g. the following picture, which may be XYZ-data viewed top-down or XY-data (but which we may orientate differently by rotating the data).\n\n     ..  .. ...  .. \n     ------gap-----\n    y.  .  . . . ..  <===  view is useful, but bounded in accuracy.\n     ------gap-----\n     . .. ..   . ..\n           x      \\\n          /\\       \\ viewing from here pointless, since data incomplete? \n          ||         It's possible, but does it give distorted results?\n          ||\n    most complete data visible from here?\n\nAs one sees, most information is visible from the bottom. So if one fits models etc. by having the origin or viewpoint there, then one applies it to most information. Another informative view is the one perpendicular to this view. It has gaps, but again it has true data on those slices between \"missing data\".\n\nOTOH, if one views the data from some other point, then one will get a \"view\" where one sees the data, but one cannot be sure whether it resembles \"true features\"? Since the way the data has been captured, does not really seem to capture \"off-axis\" perspective?\n\nSo if one e.g. fits a linear model along the two most informative views vs some other view, then do the missing data segments distort the interpretations for other than \"most informative\" viewpoints?"}, {"id": "od3jdi", "title": "[D] What questions do you have for Amanda Duarte?", "score": 0, "url": "https://www.reddit.com/r/MachineLearning/comments/od3jdi/d_what_questions_do_you_have_for_amanda_duarte/", "author": "beanstalkim", "subreddit": "r/MachineLearning", "description": "Event.-  July 8th, from 9am to 945am PT.\n\n\nAMA with Amanda Duarte, co-creator of the How2Sign Dataset\n\nAMA details at https://wandb.ai/wandb_fc/AMA/reports/AMA-with-Amanda-Duarte-co-creator-of-the-How2Sign-Dataset--Vmlldzo4MjAwNTk?galleryTag=events"}, {"id": "ocsklt", "title": "[D] A hacky work-around for slow linear algebra operations on pyspark.", "score": 6, "url": "https://www.reddit.com/r/MachineLearning/comments/ocsklt/d_a_hacky_workaround_for_slow_linear_algebra/", "author": "finebalance", "subreddit": "r/MachineLearning", "description": "Need some suggestions on scaling a pipeline on spark that performs Collaborative Filtering for about 200k-1m people, but does so in groups, with the largest group being approx. 40-50k customers at best. In addition to Collaborative Filtering, which is reasonably fast with ALS, there's a lot of linear algebra that occurs that I couldn't really figure out how to perform with the spark Dataframe API, and had to drop down to the RDD API to perform, and that leads to a significant loss in performance. I've currently got multiple variations of this script - in scala, pyspark, and python - and by far the fastest, despite not being distributor/parallelized, is python, where I'm using numpy for all linear algebra tasks, and python for the remaining transformations. \n\nSo, to summarize, I've got a pipeline with a lot of complicated linear algebra that spark doesn't seem to have performant native data structures for, and the workarounds I've devised - RDDs level manipulations for most operations, parallelizing and broadcasting the RDDs to perform matmul in chunks, etc - are significantly slower than just performing the operations in-memory on numpy. \n\nI've got a couple of ideas on how to scale this, but they are a bit hacky, so I was hoping that somebody more experienced could pitch in. \n\n* Keep the entire script in python. Used Dask to distribute the processing of various groups of customers in parallel across the cluster. \n\n* Keep the entire script in python, but run that using pyspark, keeping a pandas UDF as an entry/exit point for various python functions. However, since pandas UDF have certain limitations in that I can only input & output a single dataframe, but my analysis requires multiple datasets, I need to do some workarounds. Here's what I've what thought:\n\n    a) Read all datasets into pyspark. All relevant datasets have same number of rows, indexed with customer and other attributes, so I'll concat each row of a dataset into a single column array. So, basically, the 3-4 datasets become 3-4 columns in a consolidated dataset + a customer index. \n\n    b) Transfer this across to python via a pandas UDF. \n\n    c) Extract all relevant datasets from this combined structure in python, perform all the operations (around 1000 loc) and resemble the outputs into a similar structure as the input and transfer back to pyspark. \n\n    d) Since I used a pandas UDF computations across all groups would have occurred in parallel, and this them becomes like running a Dask like distributed compute via pyspark. \n\n    e) Extract all the data from this consolidated array, map types, and save via pyspark. \n\n    f) This is extremely hacky, and has a few downsides, but I think it'll do the job. I realize that I won't really be able to debug the python udf code easily, so that'll be an irritant, and the solution is still fundamentally limited by the size of the largest single executor I can get, but despite that it'll likely perform better than the current solution. \n\nAny suggestions on how to better structure this, or ideas about how to do more rapid linear algebra on pyspark natively would be greatly appreciated."}, {"id": "ocvczx", "title": "[D] DMD is better without Ergodic Theory and Koopman Operators", "score": 2, "url": "/r/math/comments/obrsds/dmd_is_better_without_ergodic_theory_and_koopman/", "author": "AcademicOverAnalysis", "subreddit": "r/MachineLearning", "description": ""}, {"id": "ockvw1", "title": "[R] Dataset generating method for modelling an immune system", "score": 17, "url": "https://www.reddit.com/r/MachineLearning/comments/ockvw1/r_dataset_generating_method_for_modelling_an/", "author": "jostmey", "subreddit": "r/MachineLearning", "description": "We have invented a way to model a person's immune system with machine learning.\n\nDeveloping B and T cells edit their genome to assemble randomized immune receptor genes. The many different immune receptor genes generated by this process ensures there is an immune receptor that can bind to any disease antigen but also creates immune receptors that bind self-antigens on healthy cells and tissues. If left unchecked, these immune receptors can drive autoimmune disease. In healthy people, developing B and T cells undergo a selection process to remove B and T cells with receptors that recognize self-antigens.\n\n**We describe a technique for assembling a dataset to model the selection process for any person with machine learning.** This is important because the selection process is host-specific and varies with each person. The differences in each person's immune system is the biological basis for organ transplant rejection and related diseases. Check it out.\n\n*Journal Link:* [https://www.nature.com/articles/s41435-021-00141-9 ](https://www.nature.com/articles/s41435-021-00141-9)\n\n*Open Access Link:* [https://rdcu.be/cmt4E](https://rdcu.be/cmt4E)"}, {"id": "ocy71x", "title": "[P] EfficientNet-lite in Keras (functional API).", "score": 0, "url": "https://www.reddit.com/r/MachineLearning/comments/ocy71x/p_efficientnetlite_in_keras_functional_api/", "author": "xenotecc", "subreddit": "r/MachineLearning", "description": "I created a repository with EfficientNet-lite model variants adapted to Keras:\n\n[Github Link](https://github.com/sebastian-sz/efficientnet-lite-keras)\n\nThe goal of the project was to mimic `keras.applications` behavior, as much as possible.\n\nThe lite model variants were previously available in Tensorflow 1.x, adopting them to Keras made them more consistent with existing API and documentation.\n\nAccording to [original repository](https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet/lite), the lite variants:\n\n* Use ReLU6 instead of Swish.\n* Do not use SE.\n* Have fixed Stem and Head while scaling the models.\n\nHope that this helps somebody!"}, {"id": "oclhbz", "title": "[D] Stop naming things one letter!", "score": 13, "url": "https://www.reddit.com/r/MachineLearning/comments/oclhbz/d_stop_naming_things_one_letter/", "author": "float16", "subreddit": "r/MachineLearning", "description": "in your code.\n\n\"One letter\" also includes spelling out the name of a Greek letter.\n\nThat is all."}, {"id": "ocvbjo", "title": "[P] I taught an AI to play a modified version of Breakout", "score": 0, "url": "https://www.reddit.com/r/MachineLearning/comments/ocvbjo/p_i_taught_an_ai_to_play_a_modified_version_of/", "author": "VIPTankz123", "subreddit": "r/MachineLearning", "description": "Here's the full video: [https://www.youtube.com/watch?v=A6lUt-uPEmI](https://www.youtube.com/watch?v=A6lUt-uPEmI)\n\nI created the AI using Duelling Double Deep Q learning, and after some experimentation, it performed fairly well. I created the version of breakout, where multiple balls are allowed, and there are 3 different levels for the AI to play. The details of the AI's hyperparameters are shown below:\n\nLearning Rate: 1e-5, Discount Rate: 0.99, Memory Size: 350k, Replace Target Network: 10k\n\nEpsilon Start: 1, Epsilon Decay: 9e-7, Epsilon End: 0.1\n\nInput and Network:\n\nThis game used pixel input from the game, which are originally 1280x720, however were downsized to 64x32. The layers of the network were:\n\nConv Layer1: 32 filters, kernel size 8, stride 4, padding 2\n\nConv Layer2: 64 filters, kernel size 4, stride 2, padding 1\n\nConv Layer3:64 filters, kernel size 3, stride 1, padding 0\n\nFully Connected Layer: 512 Units.\n\nThe final layer used 2 heads as duelling DDQN does, with one having a single output for the value, and the other having 3 nodes for each action.\n\nThank you for reading my Post, I hope you learned something new about RL! If you wanna see more RL agents playing games, check out my YouTube channel, which you can find using the link to the video."}, {"id": "od0iac", "title": "[D] Launching Our First Cash Prize Competition. What do You Think?", "score": 0, "url": "https://www.reddit.com/r/MachineLearning/comments/od0iac/d_launching_our_first_cash_prize_competition_what/", "author": "daniel-data", "subreddit": "r/MachineLearning", "description": "&#x200B;\n\n[Keyword Recency Prediction](https://preview.redd.it/clrwirjbq0971.png?width=1207&format=png&auto=webp&s=dbfeaebb2e13ef9c043538e2df52789dd3d9eda1)\n\n&#x200B;\n\nWe are very excited to announce our first company-sponsored data science competition, in which we will award a total of USD $2,000 in cash prizes.\n\nAfter working hard for more than a year and a half, understanding the needs of data scientists, trying to deliver value to them in different ways, and trying to understand the needs, pain points and problems of companies, we have finally reached an agreement with our first sponsor: Battelle (https://www.battelle.org/).\n\nWe take this opportunity to thank all of you for your trust in us. We are absolutely sure that this is just the beginning of many other sponsored competitions that will help the data science community to develop, and companies to solve their problems with the help of data science.\n\nWhat do you think about it?\n\n[Join Competition Here](https://www.datasource.ai/en/home/data-science-competitions-for-startups/keyword-recency-prediction)"}, {"id": "obw2xc", "title": "[P] trained the model based on dark art sketches. got such bizarre forms of life", "score": 611, "url": "https://www.reddit.com/gallery/obvwnh", "author": "Altruistic-Dot4513", "subreddit": "r/MachineLearning", "description": ""}, {"id": "ocgwqj", "title": "[D] Does LHR affect deep learning", "score": 6, "url": "https://www.reddit.com/r/MachineLearning/comments/ocgwqj/d_does_lhr_affect_deep_learning/", "author": "nice_servo", "subreddit": "r/MachineLearning", "description": "Hi, I want to buy a 3070 TI or 3080TI and all i  can find is an LHR or anti-mining cards. Will this block or affect any  of the AI or deep learning. For example yolov4, cuda, training etc. will any of  these be affected? I am only learning.\n\nThank you."}, {"id": "oclcte", "title": "[P] Need models for time series forecasting", "score": 3, "url": "https://www.reddit.com/r/MachineLearning/comments/oclcte/p_need_models_for_time_series_forecasting/", "author": "Tis-is-the-way", "subreddit": "r/MachineLearning", "description": "Apart from LSTM and GRU, what other deep learning based models/techniques can I use for time series forecasting.\n\nI have a date column and only 1 variable column which I need to predict.\n\nKindly suggest some and I would be grateful if you have code implementation."}, {"id": "ockz0j", "title": "[Discussion] Features and Credibility", "score": 2, "url": "https://www.reddit.com/r/MachineLearning/comments/ockz0j/discussion_features_and_credibility/", "author": "ossifrage_", "subreddit": "r/MachineLearning", "description": "I am working on a project at work (Insurance) to see if we can find signal in various publicly available BigQuery datasets. One of these datasets includes very detailed information on traffic fatalities. This is the dataset: [https://console.cloud.google.com/marketplace/product/dataflix-public-datasets/data-360-traffic-and-safety](https://console.cloud.google.com/marketplace/product/dataflix-public-datasets/data-360-traffic-and-safety)\n\nFor each training record I have a physical location, but not lat/long (I could look it up if needed). \n\nWhat I am struggling with is how to assign features from this dataset to use in my model. If I aggregate to zip or even county the data starts to get very thin, with some regions having only a handful of accidents ever reported over many decades. Taking an average over such a small number of events feels problematic. \n\nIf I take the approach of aggregating the data to county are there accepted methods to account for the \"strength\" of the average? Maybe using confidence intervals?\n\nAnother option that I have considered is creating a cluster of the nearest 1000 accidents for every record in the training set. I could then use some characteristics of the cluster (average age, area covered, etc) in the model. This is pretty heavy computationally but seems like it could work.\n\nHas anyone used a similar clustering strategy?"}, {"id": "ocla3e", "title": "[D] MSE in Regression Models", "score": 0, "url": "https://www.reddit.com/r/MachineLearning/comments/ocla3e/d_mse_in_regression_models/", "author": "blueest", "subreddit": "r/MachineLearning", "description": "Suppose you have two regression models (e.g a linear regression and a regression decision tree) - is the only way to compare them by using their MSE? If one of the model has a MSE of 900, and the other has a MSE of 8065 : you conclude that the model with the lower MSE is \"more accurate\".\n\nBut is it possible to understand how \"bad\" is the model with the lower MSE? E.g. even though 900 is less than 8065 - is it still a good idea to use a model with a MSE of 900?\n\nIn classification, you can compare the accuracy of 2 models: one model can have an accuracy of 51% and the other model can have an accuracy of 56%. Although 56 is bigger than 51, an accuracy of 56 still isn't great.\n\nThe same way, is it possible to understand a MSE of 900? Or is it only understood relative to the MSE of other models?\n\nThanks"}, {"id": "occjx0", "title": "[D] Is there a way to get a score for a label that is not based purely on probability?", "score": 5, "url": "https://www.reddit.com/r/MachineLearning/comments/occjx0/d_is_there_a_way_to_get_a_score_for_a_label_that/", "author": "Ancient-Shelter7512", "subreddit": "r/MachineLearning", "description": "First, I am quite new to ML and NLP, but I am trying to figure out the faisibility of a project.\n\nIf for example, I train or fine-tune a model for emotion classification (fear, anger, happy, etc.). Is there a way to quantify the emotion? My understanding is that the score will be based on the probability of the model to be right on that emotion/label.\n\nBut what if what we want to know is if the person is a little happy, or happy or very happy?  Is it mandatory to create multiple labels for each grade of happyness, or is there a solution to get a numerical score on happyness without adding extra labels in the data?\n\nThe main idea here is to track emotion swings during conversations, like emotion build-ups. Getting more angry or happy during a conversation."}, {"id": "ociyha", "title": "[D] Is there a such thing as \"allocation models\"? (Assignment Models)?", "score": 0, "url": "https://www.reddit.com/r/MachineLearning/comments/ociyha/d_is_there_a_such_thing_as_allocation_models/", "author": "jj4646", "subreddit": "r/MachineLearning", "description": "Suppose you have a legal office with 5 lawyers. Each of these 5 lawyers have different skillsets (e.g. one is a new lawyer, one is very experienced, one knows a lot about criminal law but little about financial law, one knows a lot about property law but little about human rights law, etc.). \n\nThese lawyers see people on a walk-in basis. People arrive and form lines to visit these lawyers. However, (in this unrealistic and fictional example) the people do not choose which lawyer they see. This decision is made by a secretary based on some heuristic conditions (e.g. specialty, idle/occupied layer, queue length) : it is entirely possible that a person could have a question related to property law, but ends up seeing the lawyer who specializes in criminal law.\n\nThe office is is interested in improving their services, this means: better matching people with the right lawyer, but they are also interested in increasing the number of visits and reducing the time of each visit - even if it means sending a person to a potentially less knowledgeable lawyer for a given subject. \n\nAre there are any statistical/machine learning models that can be used to \"better allocate\" people to lawyers in this example? E.g. suppose a person has a covariate vector (x1, x2... x5) - can some model be used to optimally assign these people?\n\nThanks!\n\nPossible ideas:\n\nThe office is also interested in shuffling the order of people - e.g. if 3 people arrived in order a, b,c. If somehow they knew that the total service time for serving these people would be faster if they served them in order b, a, c ... they would be interested in doing this. \n\nAlso, the office would be ok in shifting people from one line to another line after the initial assessment if it were to make a difference"}, {"id": "ocip29", "title": "[D] Is there really any need to undersampling/oversampling in deep learning models? Are imbalanced datasets really an issue for deep neural networks?", "score": 0, "url": "https://www.reddit.com/r/MachineLearning/comments/ocip29/d_is_there_really_any_need_to/", "author": "insanelylogical", "subreddit": "r/MachineLearning", "description": "Overparameterized deep neural networks are high variance by nature so bias in the bias/variance trade-off does not seem like an issue that needs addressing for the training of neural networks.\n\nAs long as there is some minimum amount of data for a given group, the model should be able to learn features that distinguishes that group in theory even if there is large disparity.\n\nIf not, would someone care to explain a case where undersampling/oversampling is needed?"}, {"id": "oci4e4", "title": "[D]Social Media Analytics to Churn and Propensity Modelling", "score": 0, "url": "https://www.reddit.com/r/MachineLearning/comments/oci4e4/dsocial_media_analytics_to_churn_and_propensity/", "author": "m2rik", "subreddit": "r/MachineLearning", "description": "Hello,I am researching a strategy of connecting social media analytics data that I already have from my social media such as Facebook, Twitter, Instagram etc. Mainly textual data and sentiments to potentially predicting churn or propensity by linking them together.\n\nData Points that I have collected include:#comments, #likes, #shares on Video, posts etc.Sentiment of the comments across different media etc., Engagement results.\n\nIf anybody here knows about any work on the same lines or has done something similar, if you could link me to some of the research or products that already do this!"}, {"id": "oc8snm", "title": "[N] Raquel Urtasun \"Waabi: The next generation of self-driving technology\"", "score": 5, "url": "https://www.reddit.com/r/MachineLearning/comments/oc8snm/n_raquel_urtasun_waabi_the_next_generation_of/", "author": "meldiwin", "subreddit": "r/MachineLearning", "description": "Hello Guys,\n\nWe wanted to share our conversation with  Raquel Urtasun on Waabi: The next generation of self-driving technology, in case of anyone would be interested."}, {"id": "oc1mvc", "title": "[R] Selection of Source Images Heavily Influences the Effectiveness of Adversarial Attacks", "score": 12, "url": "https://arxiv.org/abs/2106.07141", "author": "uozbulak", "subreddit": "r/MachineLearning", "description": ""}, {"id": "ocdq73", "title": "[R] Proposed \u2018New Hope\u2019 Blockchain Platforms Enable Large-Scale DNN Training on Smart Contracts", "score": 0, "url": "https://www.reddit.com/r/MachineLearning/comments/ocdq73/r_proposed_new_hope_blockchain_platforms_enable/", "author": "Yuqing7", "subreddit": "r/MachineLearning", "description": "A recent paper proposes A New Hope (ANH), a set of novel blockchain platforms designed to enable the integration of large-scale deep neural networks (DNNs) into smart contracts. \n\nHere is a quick read: [Proposed \u2018New Hope\u2019 Blockchain Platforms Enable Large-Scale DNN Training on Smart Contracts.](https://syncedreview.com/2021/07/02/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-54/)\n\nThe paper *Training Massive Deep Neural Networks in a Smart Contract: A New Hope* is on [arXiv](https://arxiv.org/abs/2106.14763)."}, {"id": "obne9p", "title": "[D] Is arxiv-sanity down? What people use these days?", "score": 48, "url": "https://www.reddit.com/r/MachineLearning/comments/obne9p/d_is_arxivsanity_down_what_people_use_these_days/", "author": "IborkedyourGPU", "subreddit": "r/MachineLearning", "description": "Years ago I used to play with http://www.arxiv-sanity.com/ quite a bit. After a long hiatus, I decided to dust off my account, but apparently the website is down:\n\n> Host Not Found\n> DNS error (the host name of the page you are looking for does not exist) or Server did not accept the connection.\n> \n> Please check that the host name has been spelled correctly.\n\nIs this a coincidence, or is the site permanently down? \n\nAlso, what do y'all use today in order to stay sane with the flurry of papers on arXiv? Are there other websites/videos/social media which play the same role arxiv-sanity used to play, back in the days?"}, {"id": "obolgm", "title": "[D] New SOTA StyleGAN2 inversion paper explained in 5 minutes: Pivotal Tuning for Latent-based Editing of Real Images (PTI) by Daniel Roich et al.", "score": 39, "url": "https://www.reddit.com/r/MachineLearning/comments/obolgm/d_new_sota_stylegan2_inversion_paper_explained_in/", "author": "KirillTheMunchKing", "subreddit": "r/MachineLearning", "description": "Recently multiple new StyleGAN2 inversion techniques were proposed, however, they all suffer from the inherent editability/reconstruction tradeoff meaning that reconstructions with perfect identity preservations fall outside of the generator's well-defined latent space which hinders editing. On the other hand, reconstructions that are well suited for edits tend to have a significant identity gap with the person on the target photo. Daniel Roich and his colleagues from Tel Aviv University propose a simple yet effective two-step solution: first, fit a vector that reconstructs the image well, and then use it as a pivot to fine-tune the generator so that it reconstructs the input image almost perfectly while retaining all of the editing capabilities of the original latent space.\n\nRead the [full paper digest](https://t.me/casual_gan/60) (reading time \\~5 minutes) to learn about how to obtain the pivot latent code, how to correctly fine-tune the generator to have a near-perfect reconstruction of the input image, and most importantly, how to regularize the fine-tuning process in a way that keeps the editing properties of the generator's latent space intact.\n\nMeanwhile, check out the paper digest poster by [Casual GAN Papers](https://t.me/casual_gan)!\n\n[Pivotal Tuning Inversion](https://preview.redd.it/il2jiwl4hm871.png?width=701&format=png&auto=webp&s=b0244a0719cebefff65d01b82cc62a4139fd376c)\n\n\\[[Full Explanation Post](https://t.me/casual_gan/58)\\] \\[[Arxiv](https://arxiv.org/pdf/2101.04061.pdf)\\] \\[[Code](https://nvlabs.github.io/alias-free-gan/)\\]\n\nMore recent popular computer vision paper breakdowns:\n\n>\\[[Alias-free GAN](https://t.me/casual_gan/58)\\]  \n>  \n>\\[[GFPGAN](https://t.me/casual_gan/54)\\]  \n>  \n>\\[[GANs N' Roses](https://t.me/casual_gan/53)\\]"}, {"id": "oc401m", "title": "[R] Revisiting Point Cloud Shape Classification with a Simple and Effective Baseline", "score": 4, "url": "https://www.reddit.com/r/MachineLearning/comments/oc401m/r_revisiting_point_cloud_shape_classification/", "author": "computer_visioner", "subreddit": "r/MachineLearning", "description": "New work from Princeton that revisits the progress made in point cloud classification. They show that:  \n   \n\\- auxiliary factors, independent of the model architecture, make a large difference in performance.  When these factors are controlled for, PointNet++, a relatively older network, performs as well as or better than recent methods.   \n\n\n\\- a very simple projection baseline outperforms the SOTA methods.\n\n&#x200B;\n\nCode: [https://github.com/princeton-vl/SimpleView](https://github.com/princeton-vl/SimpleView)\n\nPaper: [https://arxiv.org/abs/2106.05304](https://arxiv.org/abs/2106.05304)"}, {"id": "obw4io", "title": "[D] Should I tell my Master's thesis advisor that I lost interest in a topic? Or should I push through the grind?", "score": 6, "url": "https://www.reddit.com/r/MachineLearning/comments/obw4io/d_should_i_tell_my_masters_thesis_advisor_that_i/", "author": "iamanoriginalname", "subreddit": "r/MachineLearning", "description": "I am currently a Master of Computer Science student doing my thesis in AI-related topics. My thesis advisor and I initially agreed that I would look into the a particular kind of AI model and see what I can do with it. He pointed out a problem that he thinks is foundational to this type of model and hope that I can find some sort of solution to it. After traversing through the literature and the code for *about a month*, I find them very hard to go through. These papers almost always have some kind of topics that were not taught during any of my courses. In addition, it is very hard to find the implementation of these models, since there are relatively few publications on the topic and many of them are very theoretical. I often feel stuck when I am doing either reading the papers or trying to implement these models.\n\nAt the same time, I started to pick up interest in Reinforcement Learning (RL). I find most of the papers in top conferences to be fairly accessible to me. Therefore, I sent a brief message to my advisor saying that I find my current topic difficult and that I would like to change to RL. He said that he does not recommend changing the topic and that I should spend more time on my current topic before moving to RL. However, given that my program only has 3 semesters of pure research, I think I should act fast if I think a topic is not working out instead of spending more unproductive time on it.\n\nShould I keep my current topic or change to a new one? If you think I should change the topic, how do I communicate with my advisor to not upset him?"}, {"id": "obr2rt", "title": "[D] How do people handle hyperparameter optimization?", "score": 10, "url": "https://www.reddit.com/r/MachineLearning/comments/obr2rt/d_how_do_people_handle_hyperparameter_optimization/", "author": "CS_Student95", "subreddit": "r/MachineLearning", "description": "Broadly speaking, does it make sense, or is it common for folks doing research in other areas of ML (for example computer vision), to be well read on modern hyperparameter optimization research? Or do they typically just use external libraries? \n\n&#x200B;\n\nI'm currently working on my first research project, and am curious what the best path to go down is"}, {"id": "obcj9a", "title": "[D] Every Model Learned by Gradient Descent Is Approximately a Kernel Machine", "score": 145, "url": "https://www.reddit.com/r/MachineLearning/comments/obcj9a/d_every_model_learned_by_gradient_descent_is/", "author": "jj4646", "subreddit": "r/MachineLearning", "description": "https://arxiv.org/abs/2012.00152\n\nSomeone today was talking about how this paper might end up becoming one of the most important papers in machine learning - does anyone know why exactly?\n\nThanks"}, {"id": "oc6k6z", "title": "[D] Prediction of labour hours of blacksmiths in big organisation", "score": 0, "url": "https://www.reddit.com/r/MachineLearning/comments/oc6k6z/d_prediction_of_labour_hours_of_blacksmiths_in/", "author": "Foedt_til_at_ball", "subreddit": "r/MachineLearning", "description": "Im currently working in an organisation, which produces machines for the food industry as a software engineer.\n\nWhen a new machine is sold, the parts are split up into smaller \"items\" which has a route within a workorder.\n\nSo for example, a blacksmith needs to weld and produce an item for the concerned machine.\n\nThis will take X amount of labour hours, and the cost of the labour hours for this speficic item needs to be calculated into the final price, that the customer needs to pay.\n\nThe problem right now within the organisation is, that these estimated labour hours that items will take to produce, is estimated by different persons, based on their experience and memory of earlier items that have been produced, and since almost every item is unique we dont have data for new items, therefore the estimated hours is just guesses...Besides this, it must also be taking into account, that estimated hours can vary greatly depending on the person that makes the guess, due to the different persons remembers different projects and so on, therefore their guesses will not be the same.. This is a problem and i want to automate this process of estimating labour hours.\n\nThe problem is, that theese estimated hours are 5/10 times wrong, when made by humans..\n\nBut i was thinking if it was possible through machine learning/ deep learning / data-prediction or neural networks to create, a piece of software that could automate this process and estimate better than humans.\n\nTHE DATA WE ALREADY HAVE:\n\nwhich type of item is it? what raw material does it use? What is the quantity needed to produce this item? Which blacksmith has produced similiar items prior, and how much time did he use? / Or how much time did this type of item take to create in general? (This is a bit iffy because every item is unique, but some items are of the same \"type\" so to speak, for example a drier could have different dimensions/size/material and so on, but a drier is still a drier.\n\nI was thinking if machine learning could be used, to go through data and look for patterns between items of the same category/type and make estimates based on these parameters.\n\nI am currently still on the drawing board, and thinking of different ideas on how to implement this.\n\nBut i was hoping for somebody that could comment on my idea, and maybe have suggestions to what data i need / dont need, and which technologies would be great for this.\n\nIm a software engineer so i know how to code, but im not that experienced with machine learning, besides creating data prediction software for my bachelors, where we used pandas and FBprophet lib for python.\n\nI hope that somebody is willing to help me out with ideas! Any help is greatly appreciated! I have a dream of creating this and making it work, so we can automate estimated labour hours for new items that are going to be produced :)"}, {"id": "oc4y9v", "title": "[R] Review of \"MLP-Mixer: An all-MLP Architecture for Vision\"", "score": 0, "url": "https://www.reddit.com/r/MachineLearning/comments/oc4y9v/r_review_of_mlpmixer_an_allmlp_architecture_for/", "author": "Flying_Scholars", "subreddit": "r/MachineLearning", "description": "**Paper:** \"MLP-Mixer: An all-MLP Architecture for Vision\"  \n*Ilya Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer,  Xiaohua Zhai, Thomas Unterthiner, Jessica Yung, Andreas Steiner, Daniel  Keysers, Jakob Uszkoreit, Mario Lucic, Alexey Dosovitskiy*  \n[https://arxiv.org/abs/2105.01601v4](https://arxiv.org/abs/2105.01601v4)\n\n**TL;DR of the Review:**  MLP-Mixer is a simple deep learning architecture for vision, with results on par with the best ConvNets and Vision Transformers  when trained on sufficient data. However, some claims in the paper would  benefit from clarifications and refinements, namely that (1) the  proposed spatial mixing layer has a linear computational complexity in the number of patches and that (2) the MLP-Mixer architecture does not use convolutions. In addition, a demonstration of the necessity of the global spatial mixing layer proposed, as opposed to  a more traditional convolutional kernel, would be useful. Finally I propose that models trained on such large datasets might  essentially act as a \"big look-up table\", and propose an experiment to  test this hypothesis. **Link to full review in the comments**"}, {"id": "obnlwi", "title": "[N] A digest of AI News from the first half of 2021", "score": 9, "url": "https://www.reddit.com/r/MachineLearning/comments/obnlwi/n_a_digest_of_ai_news_from_the_first_half_of_2021/", "author": "regalalgorithm", "subreddit": "r/MachineLearning", "description": "Hi! We at Skynet Today just released [our digest of AI News from the First Half of 2021](https://www.skynettoday.com/overviews/ai-news-2021-first-half), based on articles we curated weekly on our newsletter [Last Week in AI](https://lastweekin.ai/). Lots of it you are probably aware of, but a lot may be novel to you.  \n\n\nIf nothing else, we have some neat images, like this word cloud of the words from titles of news stories:\n\nhttps://preview.redd.it/06gs3mu78m871.png?width=2460&format=png&auto=webp&s=3e27db5a897adcef59c26cf41d599b8f74a5aa40\n\nAnd this analysis of what keywords get the most coverage:  \n\n\nhttps://preview.redd.it/2xhe78tb8m871.png?width=2525&format=png&auto=webp&s=691ab1737b2b88203736bea5214a259621f7b122\n\nWould love it if you check it out and offer feedback!"}, {"id": "obpeut", "title": "[R] Improving low-resource ASR performance with untranscribed out-of-domain data", "score": 5, "url": "https://arxiv.org/pdf/2106.01227.pdf", "author": "jb1999_rd", "subreddit": "r/MachineLearning", "description": ""}, {"id": "obhtut", "title": "[D] Community specializing in ML regulation", "score": 9, "url": "https://www.reddit.com/r/MachineLearning/comments/obhtut/d_community_specializing_in_ml_regulation/", "author": "mac_cumhaill", "subreddit": "r/MachineLearning", "description": "I work at the intersection of ML and healthcare, over the last three years a huge amount of [ML regulation](https://digital-strategy.ec.europa.eu/en/policies/european-approach-artificial-intelligence) has come out of the EU and [FDA](https://www.fda.gov/news-events/press-announcements/fda-releases-artificial-intelligencemachine-learning-action-plan). I'm wondering is anyone aware of a community (Reddit, stack exchange community etc) that specializes in keeping up to date with ML regulation around the world and discussing related questions?"}, {"id": "obf89y", "title": "[D] Is this 'YOLOv5' implementation legitimate?", "score": 13, "url": "https://www.reddit.com/r/MachineLearning/comments/obf89y/d_is_this_yolov5_implementation_legitimate/", "author": "nicknochnack", "subreddit": "r/MachineLearning", "description": "I found this implementation of what's dubbed 'YOLOv5' for PyTorch on GitHub (https://github.com/ultralytics/yolov5) and it seems to work pretty well given my use case.\n\nHowever it doesn't look as though there is a formal research paper behind the model. The authors have documented the model and provided an architecture model of the network via Tensorboard (https://github.com/ultralytics/yolov5/issues/1333).\n\nMy question, are there any issues with the model being named YOLOv5? How is this treated in the academic community given there isn't yet a formal research paper behind the model?\n\nTL;DR; Found a great OD implementation named YOLOv5, works brilliantly. Wonder if there are any issues with this naming convention given the model hasn't been given formal recognition via a paper by the academic community!"}, {"id": "oc2vv6", "title": "A company that researches a sentient A.I possibility [D]", "score": 0, "url": "https://www.reddit.com/r/MachineLearning/comments/oc2vv6/a_company_that_researches_a_sentient_ai/", "author": "MashedPotato71", "subreddit": "r/MachineLearning", "description": "Now I know that the whole idea of a \"Sentient A.I\" is still far from our understanding since we don't even understand what \"consciousness\" is and how to create self-awareness.\n\nBut I mean surely there would exist a company that researches this topic, consciousness, sentient A.I., self-awareness.. etc. Right? If so I would love to know them."}, {"id": "ob3wp8", "title": "[N] G2Net Competition: Predict Gravitational Wave Signals From Binary Black Hole Collisions", "score": 68, "url": "https://www.reddit.com/r/MachineLearning/comments/ob3wp8/n_g2net_competition_predict_gravitational_wave/", "author": "SubstantialRange", "subreddit": "r/MachineLearning", "description": "[https://www.kaggle.com/c/g2net-gravitational-wave-detection](https://www.kaggle.com/c/g2net-gravitational-wave-detection)\n\n&#x200B;\n\n>It's been said that teamwork makes the dream work. This couldn't be truer for the breakthrough discovery of gravitational waves (GW), signals from colliding binary black holes in 2015. It required the collaboration of experts in physics, mathematics, information science, and computing. GW signals have led researchers to observe a new population of massive, stellar-origin black holes, to unlock the mysteries of neutron star mergers, and to measure the expansion of the Universe.   \n>  \n>These signals are unimaginably tiny ripples in the fabric of space-time and even though the global network of GW detectors are some of the most sensitive instruments on the planet, the signals are buried in detector noise. Analysis of GW data and the detection of these signals is a crucial mission for the growing global network of increasingly sensitive GW detectors.   \n>  \n>These challenges in data analysis and noise characterization could be solved with the help of data science.  As with the multi-disciplined approach to the discovery of GWs, additional expertise will be needed to further GW research. In particular, social and natural sciences have taken an interest in machine learning, deep learning, classification problems, data mining, and visualization to develop new techniques and algorithms to efficiently handle complex and massive data sets.   \n>  \n>The increase in computing power and the development of innovative techniques for the rapid analysis of data will be vital to the exciting new field of GW Astronomy. Potential outcomes may include increased sensitivity to GW signals, application to control and feedback systems for next-generation detectors, noise removal, data conditioning tools, and signal characterization.    \n>  \n>G2Net is a network of Gravitational Wave, Geophysics and Machine Learning. Via an Action from COST (European Cooperation in Science and Technology), a funding agency for research and innovation networks, G2Net aims to create a broad network of scientists. From four different areas of expertise, namely GW physics, Geophysics, Computing Science and Robotics, these scientists have agreed on a common goal of tackling challenges in data analysis and noise characterization for GW detectors.   \n>  \n> In this competition, you\u2019ll aim to detect GW signals from the mergers of binary black holes. Specifically, you'll build a model to analyze simulated GW time-series data from a network of Earth-based detectors.   \n>  \n>If successful, you'll play a part in solving a crucial mission in the exciting new field of GW science. With the development of new algorithms, scientists will have a better handle on the potential power of the data science community and their innovative approaches to data analysis.   \n>  \n>Moreover, it will enable closer interaction between computer science and physics, which could benefit both disciplines. Your participation can further this collaboration and the help advance this breakthrough discovery."}, {"id": "obhcv5", "title": "[D] Are there Object Detection papers specializing on Textures, Uniform Background, etc. for Pavement Fault Detection?", "score": 5, "url": "https://www.reddit.com/r/MachineLearning/comments/obhcv5/d_are_there_object_detection_papers_specializing/", "author": "sarmientoj24", "subreddit": "r/MachineLearning", "description": "I am trying to do experiments involving Object Detection for Road Damage Detection (potholes, cracks, manholes, etc.) similar to this Global Road Damage Detection Challenge 2020 [https://rdd2020.sekilab.global/](https://rdd2020.sekilab.global/) (RDDC2020)\n\nAlso, this seems to be very similar to Surface Inspection Object Detection. I am referring it as ***\"uniform\"*** since compared to typical Object Detection where the backgrounds are different, the objects are ***basically embedded to the background as road defects.***\n\nCurrently, my experiments are lined up as pretty basic:\n\nObject Detection\n\n* Region-based Proposal: Faster-RCNN \n* Regression-based Anchor-based method: YOLOv3\n* Transformer-based Object Detection: DETR\n* EfficientDet (highest scoring in PASCALVOC)\n* Anchor-Free methods: FCOS or GFocal\n\nbut it seems as though I could increase the robustness, accuracy of the results if there are \n\n1. ***Pre-processing methods that specialize on bringing out textures.*** I plan to convert the RGB to 3 channels of (1) CLAHE grayscale, (2) Gabor Fractal #1, (3) Gabor Fractal #2 (or Wavelet Transforms?). ***Are there better pre-processing methods for this?***\n2. ***Auto-Augmentation for Object Detection.*** I saw Scale-Aware Automatic Augmentation etc for Object Detection. Since roads are usually **uniform in texture,** it makes more sense that to augment a car in a background (forexample). ***But is this a good way to go as well?***\n3. ***Object Detection Methods that work well with uniform \"background\", textures, and \"defect-like\" objects in an image.***\n\nAs part of my work, we are collecting standardized images **via mounting the camera in the vehicle face-down to the road.** Hence, it shall be **more standardized, and have less \"noise/distractions\" in the image than the RDDC2020 dataset.**\n\n***tldr; I am looking for papers, techniques, etc. that would be more suited to this kind of problem.***"}, {"id": "ob2rll", "title": "[D] How often is Lightgbm used for forecasting in the industry?", "score": 46, "url": "https://www.reddit.com/r/MachineLearning/comments/ob2rll/d_how_often_is_lightgbm_used_for_forecasting_in/", "author": "Professional-Pain208", "subreddit": "r/MachineLearning", "description": "Doordash recently published an article mentioning that they use exclusively Lightgbm for forecasting purposes. I was wondering if that is the case anywhere else? I've mainly been using Prophet/Arima, and they present some good arguments for using GBM models. Is there something I'm missing? Why are standard statistical forecasting approaches still so much more popular than ML based approaches?\n\n[https://doordash.engineering/2021/06/29/managing-supply-and-demand-balance-through-machine-learning/](https://doordash.engineering/2021/06/29/managing-supply-and-demand-balance-through-machine-learning/)"}, {"id": "oawexp", "title": "[P] Atom\u2014free one-click segmentation tool", "score": 88, "url": "https://www.reddit.com/r/MachineLearning/comments/oawexp/p_atomfree_oneclick_segmentation_tool/", "author": "toby__bryant", "subreddit": "r/MachineLearning", "description": "Hi everyone, \n\nwe built a free one-click segmentation tool for image annotation: [https://medium.com/hasty-ai/the-first-completely-free-one-click-segmentation-tool-hasty-ai-a3c98f28603a](https://medium.com/hasty-ai/the-first-completely-free-one-click-segmentation-tool-hasty-ai-a3c98f28603a) \n\nThere are already many approaches to using clicks to annotate images, however, most methods integrate the clicks early in additional input channels (see the widely used [DEXTR](https://arxiv.org/pdf/1711.09081.pdf) for example). Often the user has to add several clicks before getting a prediction. Even with interactive segmentation techniques, additional clicks tend to be treated the same as earlier clicks. In order to improve upon initial predictions, our positive or negative clicks are taken together with the previously predicted mask. In addition to the clicks + image features, we also generate clicks + prior-prediction features independently. These separate sets of features prevent a bad initial prediction from harming the features generated from the image. By combining both sets of features, our approach allows the user to continue adding positive and negative clicks in order to reach a high IoU. \n\nYou can use it for free without any limits by creating an account here: [https://hasty.ai/annotation/](https://hasty.ai/annotation/)"}, {"id": "obp8x8", "title": "[D] Has deep-learning-based image compression been solved?", "score": 0, "url": "https://www.reddit.com/r/MachineLearning/comments/obp8x8/d_has_deeplearningbased_image_compression_been/", "author": "TheCockatoo", "subreddit": "r/MachineLearning", "description": "If I'm not mistaken, autoencoders became the de facto approach a few years ago. Have there been any fundamental developments since then?"}, {"id": "obdhyc", "title": "[R] A Survey on Neural Speech Synthesis", "score": 3, "url": "https://arxiv.org/pdf/2106.15561.pdf", "author": "tobyoup", "subreddit": "r/MachineLearning", "description": ""}, {"id": "ob80km", "title": "[D] Deliberately oversampling training data near low-scoring examples", "score": 7, "url": "https://www.reddit.com/r/MachineLearning/comments/ob80km/d_deliberately_oversampling_training_data_near/", "author": "PM_ME_YOUR_GESTALT", "subreddit": "r/MachineLearning", "description": "For a project I am working on, I am finding that training goes much faster if I do the following:\n\n1. Sample N examples from a very large collection of training data of size N0 >> N\n2. Train the model for 10 epochs on these samples\n3. Compute the error across the N training examples\n4. Find the k worst-scoring training examples\n5. Lookup training examples (from the the large set N0) that are near the worst-performing examples\n6. Create a new size N training set that is enriched for these \"hard\" examples\n7. Train the model for 10 epochs on this new \"hard\" training set\n8. Repeat Steps 2 -- 7\n\nI feel like there is probably a name for what I am doing here, but my Googling skills are failing me here. Conceptually, if I abstract my big training set (the full N0 collection) as something like a continuous manifold, then I am deliberately sampling rougher parts of the manifold. If this was a classification problem, the \"hard\" examples would probably correspond to classes with low support on N0. I tried looking up terms like \"adversarial sampling\" and \"importance sampling,\" but neither one seems to be the right name for this approach.\n\nWhat is the correct terminology for this approach? Are there any key references that I can read? Thanks very much for any advice."}, {"id": "oarcal", "title": "[N] NVIDIA Partners with Google Cloud To Establish AI-on-5G Innovation Lab, Speeding AI Deployment", "score": 48, "url": "https://www.reddit.com/r/MachineLearning/comments/oarcal/n_nvidia_partners_with_google_cloud_to_establish/", "author": "techsucker", "subreddit": "r/MachineLearning", "description": "In partnership with [Google Cloud, NVIDIA is establishing the first AI-on-5G Innovation Lab](https://nvidianews.nvidia.com/news/nvidia-and-google-cloud-to-create-industrys-first-ai-on-5g-lab-to-speed-development-of-ai-everywhere). This will support network infrastructure players and AI software partners to develop, test, and adopt solutions to accelerate advanced AI and 5G applications.\n\nThe global telecoms industry undergoes a significant transformation as the speed and low latency of 5G provides an unparalleled opportunity for AI-on-5G to be delivered at corporate borders. This collaboration between NVIDIA and Google Cloud will assist network operators and infrastructure players design and launch new profit centers based on AI and machine learning.\u00a0\n\nFull Story: [https://www.marktechpost.com/2021/06/29/nvidia-partners-with-google-cloud-to-establish-ai-on-5g-innovation-lab-speeding-ai-deployment/](https://www.marktechpost.com/2021/06/29/nvidia-partners-with-google-cloud-to-establish-ai-on-5g-innovation-lab-speeding-ai-deployment/)"}, {"id": "oaambv", "title": "[N] GitHub and OpenAI release Copilot: an AI pair programmer", "score": 604, "url": "https://www.reddit.com/r/MachineLearning/comments/oaambv/n_github_and_openai_release_copilot_an_ai_pair/", "author": "BrownPandaBoi", "subreddit": "r/MachineLearning", "description": "Link to copilot: https://copilot.github.com/   \n\nIt is currently being made available as a VSCode extension. Relevant description from the website: \n\n> **What is GitHub Copilot?**\n> GitHub Copilot is an AI pair programmer that helps you write code faster and with less work. GitHub Copilot draws context from comments and code, and suggests individual lines and whole functions instantly. GitHub Copilot is powered by OpenAI Codex, a new AI system created by OpenAI. The GitHub Copilot technical preview is available as a Visual Studio Code extension.\n\n> **How good is GitHub Copilot?**\n> We recently benchmarked against a set of Python functions that have good test coverage in open source repos. We blanked out the function bodies and asked GitHub Copilot to fill them in. The model got this right 43% of the time on the first try, and 57% of the time when allowed 10 attempts. And it\u2019s getting smarter all the time.\n\nThe service is based on OpenAI's Codex model, which has not been released yet but [Greg Brockman (OpenAI CTO) tweeted that it will be made available through their API later this summer](https://twitter.com/gdb/status/1409890354132750336?s=20)"}, {"id": "oay3tf", "title": "[R] New Study Proposes CW Networks: Greater Expressive Power Than GNNs", "score": 9, "url": "https://www.reddit.com/r/MachineLearning/comments/oay3tf/r_new_study_proposes_cw_networks_greater/", "author": "Yuqing7", "subreddit": "r/MachineLearning", "description": "A research team from University of Cambridge, Imperial College London & Twitter, UCLA, MPI-MIS, and SJTU & UNSW proposes CW Networks (CWNs), a message-passing scheme that operates on regular cell complexes and achieves stronger expressive power than graph neural networks (GNNs). \n\nHere is a quick read: [New Study Proposes CW Networks: Greater Expressive Power Than GNNs.](https://syncedreview.com/2021/06/30/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-52/)\n\nThe paper *Weisfeiler and Lehman Go Cellular: CW Networks* is on [arXiv](https://arxiv.org/abs/2106.12575)."}, {"id": "oasxxx", "title": "[N] New opportunity: Postdoctoral Fellow in ML-based perception for maritime activities", "score": 24, "url": "https://www.reddit.com/r/MachineLearning/comments/oasxxx/n_new_opportunity_postdoctoral_fellow_in_mlbased/", "author": "KatjaKim", "subreddit": "r/MachineLearning", "description": "The Norwegian University of Science and Technology (NTNU) is looking for a Postdoctoral Fellow to work on autonomous perception in extreme environments using machine learning (deep learning) and other data science tools.\n\nThe work will be conducted in a high-level international research environment at the premises of NTNU in Trondheim at the Department of Marine Technology -- a world leader in education, research, and innovation for engineering systems in the marine environment. More information here: [https://www.jobbnorge.no/en/available-jobs/job/208634/postdoctoral-fellow](https://www.jobbnorge.no/en/available-jobs/job/208634/postdoctoral-fellow)"}, {"id": "ob0fah", "title": "[N] AI Habitat 2.0 and New 3D Scene Dataset releases from FAIR", "score": 5, "url": "https://www.reddit.com/r/MachineLearning/comments/ob0fah/n_ai_habitat_20_and_new_3d_scene_dataset_releases/", "author": "Skylion007", "subreddit": "r/MachineLearning", "description": "Facebook AI is releasing a new version of AI Habitat and a new Matterport 3D dataset:\n\n# Habitat 2.0 Paper: [https://arxiv.org/abs/2106.14405](https://arxiv.org/abs/2106.14405)\n\n# HM3D Scenes Dataset\n\n[https://ai.facebook.com/blog/introducing-the-habitat-matterport-3d-research-data-set-for-training-embodied-ai](https://ai.facebook.com/blog/introducing-the-habitat-matterport-3d-research-data-set-for-training-embodied-ai)\n\n>Today, Facebook AI, in collaboration with Matterport, are making  available a Matterport open source licensed data set, the largest ever  data set of indoor 3D scans, to benefit the research community.  Habitat-Matterport 3D Research Dataset (HM3D) is a collection of 1,000  Habitat-compatible 3D scans made up of accurately scaled residential  spaces such as apartments, multifamily housing, and single-family homes,  as well as commercial spaces like office buildings and retail stores.\n\nThe scenes can be previewed at the following link: [https://aihabitat.org/datasets/hm3d/](https://aihabitat.org/datasets/hm3d/)\n\nTwitter Thread: [https://twitter.com/ai\\_habitat/status/1410248635707625481](https://twitter.com/ai_habitat/status/1410248635707625481)\n\n# AI Habitat 2.0 (URDF Support) and ReplicaCAD\n\n>Habitat 2.0\u2019s new data set, ReplicaCAD, is a mirror of [Replica](https://l.facebook.com/l.php?u=https%3A%2F%2Fgithub.com%2Ffacebookresearch%2FReplica-Dataset%3Ffbclid%3DIwAR2qjpRHqQlqz1kpwKkBE_mL3RaEB1FOAaR9-Gw709zBBw2Svdeb-wkqt8o&h=AT2uG0cx8DJrwi-Igj_fmuXodus1kGkTke2pl-JLbEElDlqNEhMiOKMOdhYK-u9tagQu1ifiFJpxZ5sSOUX-yHdndlRtAbx2imjYI6K8Aun3PTDH-Vto8T6CiJ3xOaZKu8Q1U2ofgq-yoMqs5SI), [Facebook Reality Lab\u2019s data set](https://l.facebook.com/l.php?u=https%3A%2F%2Ftech.fb.com%2Ffacebook-reality-labs-replica-simulations-help-advance-ai-and-ar%2F&h=AT3OfFtCruWjSmFNqqoJH0y_jEwy36DlhwcHHbXO5Jp2q02eq9xvGs6MqQfEFklWNPq8IIrMyBjruc9eAczW05oXiH0Inw2wEqYrkoLseG_YYEx3_lFhaAcvUm1HCNB8QONtrA)  previously released for 3D environments, now rebuilt to support the  movement and manipulation of objects. In ReplicaCAD, previously static  3D scans have been converted to individual 3D models with physical  parameters, collision proxy shapes, and semantic annotations that can  enable training for movement and manipulation for the first time. To  create the new data set, we relied on a team of 3D artists to reproduce  identical renderings of spaces within Replica, but with full attention  to specifications relating to their material composition, geometry, and  texture. The interactive recreations also incorporated information about  size and friction, whether an object (such as a refrigerator or door)  has compartments that could open or close, and how those mechanisms  worked, among other considerations.\n\nBlog post: [https://ai.facebook.com/blog/habitat-20-training-home-assistant-robots-with-faster-simulation-and-new-benchmarks/](https://ai.facebook.com/blog/habitat-20-training-home-assistant-robots-with-faster-simulation-and-new-benchmarks/)\n\nPaper: [https://arxiv.org/abs/2106.14405](https://arxiv.org/abs/2106.14405)\n\nTLDR: New version of Habitat with URDF/Articulated Object Support. Also new scene dataset called ReplicaCAD to help benchmark robotics tasks.\n\nTwitter Thread:  [https://twitter.com/ai\\_habitat/status/1410260951811596291](https://twitter.com/ai_habitat/status/1410260951811596291)"}, {"id": "oav5dt", "title": "[D] Are there papers or techniques for preprocessing of images for image generation using GANs or VAEs?", "score": 10, "url": "https://www.reddit.com/r/MachineLearning/comments/oav5dt/d_are_there_papers_or_techniques_for/", "author": "steam681", "subreddit": "r/MachineLearning", "description": "I would like to generate images of cars from OpenImages or LSUN. But there are still problems in some of the generated images. Either the car is too small, too closely zoomed, there are blobs of people, etc.\n\nI would like to know if there are papers or techniques tackling data preprocessing so that the training images are filtered, or modified to be better so that the generated images are better as well?"}, {"id": "oarqka", "title": "[D] Research on predictive uncertainty / confidence?", "score": 19, "url": "https://www.reddit.com/r/MachineLearning/comments/oarqka/d_research_on_predictive_uncertainty_confidence/", "author": "NeedMoarInfoAgain", "subreddit": "r/MachineLearning", "description": " I'm interested in learning about advances on modeling uncertainty in deep learning, particularly for classification. I know there are some papers from Google \\[1,2\\] and some work on Bayesian NNs but have there been any significant changes or improvements?\n\nAny code / implementations are also appreciated.\n\n\\[1\\] [https://arxiv.org/abs/1612.01474](https://arxiv.org/pdf/1612.01474.pdf)\n\n\\[2\\] [https://arxiv.org/abs/1906.02530](https://arxiv.org/abs/1906.02530)"}, {"id": "oas183", "title": "[R] Brax: A Differentiable Physics Engine for Large Scale Rigid Body Simulation, with a focus on performance and parallelism on accelerators, written in JAX.", "score": 15, "url": "https://www.reddit.com/r/MachineLearning/comments/oas183/r_brax_a_differentiable_physics_engine_for_large/", "author": "hardmaru", "subreddit": "r/MachineLearning", "description": "**Abstract**\n\nWe present Brax, an open source library for rigid body simulation with a focus on performance and parallelism on accelerators, written in JAX. We present results on a suite of tasks inspired by the existing reinforcement learning literature, but remade in our engine. Additionally, we provide reimplementations of PPO, SAC, ES, and direct policy optimization in JAX that compile alongside our environments, allowing the learning algorithm and the environment processing to occur on the same device, and to scale seamlessly on accelerators. Finally, we include notebooks that facilitate training of performant policies on common OpenAI Gym MuJoCo-like tasks in minutes.\n\nhttps://arxiv.org/abs/2106.13281\n\n**GitHub repo description**\n\nBrax is a differentiable physics engine that simulates environments made up of rigid bodies, joints, and actuators. It's also a suite of learning algorithms to train agents to operate in these environments (PPO, SAC, evolutionary strategy, and direct trajectory optimization are implemented).\n\nBrax is written in [JAX](https://github.com/google/jax) and is designed for use on acceleration hardware. It is both efficient for single-core training, and scalable to massively parallel simulation, without the need for pesky datacenters.\n\nhttps://github.com/google/brax"}, {"id": "oazdq4", "title": "[D] Contemplating life through machine learning", "score": 2, "url": "https://www.reddit.com/r/MachineLearning/comments/oazdq4/d_contemplating_life_through_machine_learning/", "author": "kmanchel", "subreddit": "r/MachineLearning", "description": "Having initially ventured into machine learning to make a living out of it, I realized my most valued lessons from machine learning are concepts that allow me to make better sense of the world (or at least try to). \n\nHere is an example:\n\nLike many people, I've never paid attention the concept of privilege (and lack there of) in life. Especially in terms of socioeconomic status, I had always thought that hard work is THE WAY to climb the socioeconomic ladder. Though I knew privilege had some role to play, I've always underplayed it until I came across the concept of model initialization. Whether this is in image classification, language models, or reinforcement learning, I've come across a whole slew of studies exploring and proving the importance initialization in the algorithm's ability to guide a model to convergence. \n\nWhile I acknowledge that human beings are much more complex (of a \"higher dimensionality\") than any given ML model, drawing parallels from studies of model initialization has made me appreciate the privilege that I've been given in life a lot more than I ever did. As I watch myself ascend this socioeconomic ladder (converging through life), I cannot help but be thankful of my privilege (aka my initial state) and aspire to provide others with the same. \n\nIf anyone else has come across such thoughts from concepts you learned through ML, please share!"}, {"id": "ob30lc", "title": "[D] Are predictions on Multi-Step Forecast independent?", "score": 1, "url": "https://www.reddit.com/r/MachineLearning/comments/ob30lc/d_are_predictions_on_multistep_forecast/", "author": "saikjuan", "subreddit": "r/MachineLearning", "description": "Hey!\nI'm working on a time series forecasting, and I'm being inspired by a paper from Uber in which they do trips forecasting (in which they measure uncertainty). \n\nIn their paper, they do a forecast for the next day, but I need to forecast the next seven days at once. \n\nI'm worried that I ignore possible corrrlations and I find a bias estimation for the variance of the forecast for each step. Is there some source you know or something that I can read ao that I understand if the outputs of a simple NN in a Multi-Step forecsst are either independent or correlated?"}, {"id": "oaq7bn", "title": "[R] Modularity in Reinforcement Learning via Algorithmic Independence in Credit Assignment", "score": 13, "url": "https://www.reddit.com/r/MachineLearning/comments/oaq7bn/r_modularity_in_reinforcement_learning_via/", "author": "aphbbd", "subreddit": "r/MachineLearning", "description": "New work from Berkeley and Princeton that draws upon causality and algorithmic information theory to analyze the causal structure of credit assignment.  \n\n\nWebsite: [https://sites.google.com/view/modularcreditassignment](https://sites.google.com/view/modularcreditassignment?fbclid=IwAR0mDW1sM29pYtFEdRXTIAgrft2mqt7lcUVo4dw0QRxyHI2EnpWIaeVRBMs)\n\nPaper: [https://sites.google.com/view/modularcreditassignment](https://sites.google.com/view/modularcreditassignment?fbclid=IwAR1trTxIT-pW6E9K9sMOABnKdRHArLu_hE68ZjRc35GHSsMNzosvln0k0-U)\n\nTalk: [https://www.youtube.com/watch?v=\\_KXdjWxJ2Ow](https://www.youtube.com/watch?v=_KXdjWxJ2Ow&fbclid=IwAR2vFWxLm4SQngTO05eF9FFUYPPaq27jo0QpuRMDOodsZmwyOlucKRZILk4)\n\nTwitter: [https://twitter.com/mmmbchang/status/1410066323720204291](https://twitter.com/mmmbchang/status/1410066323720204291?fbclid=IwAR0_ahBCMP0fTEDehrRTurgUyLW4TFdxsXNZBvsF-dR1AZ7ya5Vf6lkf7co)"}, {"id": "oatjad", "title": "[Discussion] Should I use downstream data for pre-training in self-supervised learning?", "score": 2, "url": "https://www.reddit.com/r/MachineLearning/comments/oatjad/discussion_should_i_use_downstream_data_for/", "author": "awesome_ml", "subreddit": "r/MachineLearning", "description": "Intuitively, if the downstream can be accessed in the pre-training phase, we can train a better but task-specific pre-trained feature extractor. However, I tried some image classification experiments, the performance of using downstream data does not always beat the pre-trained model training without downstream data. I use the same setting with Moco except using half of the downstream data in the pre-training. It is surprising to me. Do I miss something?"}, {"id": "oaw61v", "title": "[Discussion] how to understand the dilemma raised by this BLOG for BN-Conv fusion?", "score": 0, "url": "https://www.reddit.com/r/MachineLearning/comments/oaw61v/discussion_how_to_understand_the_dilemma_raised/", "author": "Real-Smoke9124", "subreddit": "r/MachineLearning", "description": "Consider the BN-Conv sequence. We can try to fold BN to Conv just like this BLOG.\n\n[https://scortex.io/batch-norm-folding-an-easy-way-to-improve-your-network-speed/](https://scortex.io/batch-norm-folding-an-easy-way-to-improve-your-network-speed/)\n\n&#x200B;\n\nhttps://preview.redd.it/02dwv5hzae871.png?width=700&format=png&auto=webp&s=29962080066c7dc241f88948d7321cc26a6f30c8\n\nAnd this author argues that:\n\n>So now, it should be easy to reapply the exact same ideas as previously and come up with a keras implementation right? Except for one thing that was not taken into account here: padding.\u00a0  \n>  \n>Indeed if we use zero padding in convolutions, we will have 0 values entering the convolution, after the BatchNorm. When folded, it will still be the case: to fold properly, we would need to modify the default 0 values by -(b\u2019 \u2013 b), that is apply the BatchNorm transformation to the padding so that we can apply its inverse later.\n\n&#x200B;\n\nI don't really get the \\`-(b' - b)\\`. Is there anyone who knows how to derive this formula?\n\nThanks again."}, {"id": "oa3t3c", "title": "[N] Random Forests and Gradient Boosted Trees now native for TensorFlow / Keras", "score": 166, "url": "https://www.reddit.com/r/MachineLearning/comments/oa3t3c/n_random_forests_and_gradient_boosted_trees_now/", "author": "domvwt", "subreddit": "r/MachineLearning", "description": "[Announcement here](https://blog.tensorflow.org/2021/05/introducing-tensorflow-decision-forests.html)\n\nGoogle have released implementations of Random Forest, GBT, and CART algorithms for Keras. Potentially useful for anyone working with TFX or Tensorflow Serving for production models. \n\nHas anyone had a chance to compare with CatBoost / XGBoost / LGBM? What are people's experiences so far?"}, {"id": "oadstf", "title": "[R] CLIPDraw: Exploring Text-to-Drawing Synthesis", "score": 20, "url": "https://www.reddit.com/r/MachineLearning/comments/oadstf/r_clipdraw_exploring_texttodrawing_synthesis/", "author": "kvfrans", "subreddit": "r/MachineLearning", "description": "[Visuals](https://i.imgur.com/m7dlOnc.mp4)\n\n[Arxiv Paper](https://arxiv.org/abs/2106.14843)\n\n[Blog Post](https://kvfrans.com/clipdraw-exploring-text-to-drawing-synthesis/)\n\n[Colab Notebook](https://colab.research.google.com/github/kvfrans/clipdraw/blob/main/clipdraw.ipynb)\n\nCLIPDraw is an algorithm that synthesizes stroke-based drawings based on natural language input. CLIPDraw does not require any training; rather a pre-trained CLIP language-image encoder is used as a metric for maximizing similarity between the given description and a generated drawing. Crucially, CLIPDraw operates over vector strokes rather than pixel images, a constraint that biases drawings towards simpler human-recognizable shapes. Results compare between CLIPDraw and other synthesis-through-optimization methods, as well as highlight various interesting behaviors of CLIPDraw, such as satisfying ambiguous text in multiple ways, reliably producing drawings in diverse artistic styles, and scaling from simple to complex visual representations as stroke count is increased.\n\nGlad to get this out and share with you all. The Colab notebook can generally synthesize drawings within a minute or two so please enjoy playing around with it! The paper / blog focus on showcasing interesting text-to-image behaviors of CLIPDraw."}, {"id": "oau2ya", "title": "[D] Do I need to apply spectral norm to my embedding matrix when training a conditional W-GAN?", "score": 1, "url": "https://www.reddit.com/r/MachineLearning/comments/oau2ya/d_do_i_need_to_apply_spectral_norm_to_my/", "author": "chasep255", "subreddit": "r/MachineLearning", "description": "I am now trying to use a WGAN to enhance the output of my 256x compressed VQVAE auto encoder for music.  I am conditioning the critic on an embedding for each song (I have 5000 something), and the relative time within each song from which the sample was drawn.  \n\n        input_audio = keras.Input((input_size,))\n        input_class = keras.Input((), dtype = 'int32')\n        input_time = keras.Input(())\n        \n        x = layers.Reshape((-1, 1))(input_audio)\n        for f in [32, 64, 128, 256]:\n            x = tfa.layers.SpectralNormalization(layers.Conv1D(f, 16, strides = 8, activation = 'elu', padding = 'same'))(x)\n            x = tfa.layers.SpectralNormalization(layers.Conv1D(f, 5, activation = 'elu', padding = 'same'))(x)\n            x = tfa.layers.SpectralNormalization(layers.Conv1D(f, 5, activation = 'elu', padding = 'same'))(x)\n        \n        x = layers.Flatten()(x)\n        e = layers.Embedding(num_classes, 256, name = 'class_embedding')(input_class)\n        t = layers.Reshape((1,))(input_time)\n        x = layers.Concatenate(axis = -1)((x, e, t))\n        x = tfa.layers.SpectralNormalization(layers.Dense(1024, activation = 'elu'))(x)\n        x = tfa.layers.SpectralNormalization(layers.Dense(1024, activation = 'elu'))(x)\n        x = tfa.layers.SpectralNormalization(layers.Dense(1, use_bias = False))(x)\n\nSo here I am using spectral norm to enforce the lipschitz constraint.  I apply it to all layers through which the input from my generator (input\\_audio) passes.  What I am unsure about is whether I should also apply spectral norm to my embedding layer used to condition on the song.  I found a conditional WGAN-GP example and they seem to only be concerned about the gradients between the generator output and critic output.  Not the gradients between the embedding and critic output.  Anyone know/have an opinion on whether I should apply spectral norm to my embedding?  Currently I am running without spectral norm so I guess I will see if it works.  Then again with deep learning I feel like a lot of incorrect stuff tends to \"work\" even though it was a mistake."}, {"id": "oat8zu", "title": "[D] Is the term 'validation set' a misnomer?", "score": 0, "url": "https://www.reddit.com/r/MachineLearning/comments/oat8zu/d_is_the_term_validation_set_a_misnomer/", "author": "RexBox", "subreddit": "r/MachineLearning", "description": "In ML and statistics, the validation set is used in optimizing models through parameter tweaking, early stopping, or other model adaptations. It differs from the train set in that it is not trained upon, and it differs from the test set in that it is used in optimization.\n\nAccording to Oxford Languages, the term 'validation' means 'the action of checking or proving the validity or accuracy of something' (alternative definitions can be found, but they transmit the same idea). This definition fits the test set much better, in my opinion. I have heard of the term 'development set' being used in place of 'validation set', which seems a lot more sensical to me.\n\nI'd like to hear the opinions of others on this. If I were to call my validation set a 'development set' in a paper, would this be confusing?"}, {"id": "oaa1c4", "title": "[D] Charformer Paper Explained and Visualized: Fast Character Transformers via Gradient-based Subword Tokenization", "score": 20, "url": "https://www.reddit.com/r/MachineLearning/comments/oaa1c4/d_charformer_paper_explained_and_visualized_fast/", "author": "AICoffeeBreak", "subreddit": "r/MachineLearning", "description": "Hi, if you wanted to know how the Charformer from Google Research and DeepMind works [https://arxiv.org/abs/2106.12672](https://arxiv.org/abs/2106.12672), or you also think it is very cool to abolish tokenization as pre-processing and learn sub-words from characters on the fly, check out this video: (full video at [https://youtu.be/debgj24BAZE](https://youtu.be/debgj24BAZE), trailer in this post.)\n\n\\[Discussion\\] What do you think? When (or will) this become mainstream or is it just too much of a hassle? Apropos hassle: that will not be the case anymore when huggingface will implement it, lol!\n\nMs. Coffee Bean explains the importance of flexible tokenization and then moves onto explaining the \u201cCharformer: Fast Character Transformers via Gradient-based Subword Tokenization\u201d paper.\n\nPaper \ud83d\udcc4: Tay, Yi, Vinh Tran, Sebastian Ruder, Jai Gupta, Hyung Won Chung, Dara Bahri, Zhen Qin, Simon Baumgartner, Cong Yu and Donald Metzler. [\u201cCharformer: Fast Character Transformers via Gradient-based Subword Tokenization.\u201d (2021).](https://arxiv.org/abs/2106.12672)  \n\nOutline:\n\n00:00 What are tokenizers good for?  \n02:49 Where does rigid tokenization fail?  \n03:51 Charformer: end-to-end tokenization  \n08:33 Again, but in summary.  \n09:57 Reducing the sequence length  \n10:37 Meta-comments on token mixing   \n\n\n[Short part of the full video on the Charformer by AICoffeeBreak.](https://reddit.com/link/oaa1c4/video/67d33gbsw7871/player)"}, {"id": "oa9aay", "title": "What\u2019s the best library for model parallelism? [D]", "score": 15, "url": "https://www.reddit.com/r/MachineLearning/comments/oa9aay/whats_the_best_library_for_model_parallelism_d/", "author": "willspag", "subreddit": "r/MachineLearning", "description": "I\u2019m working on building some pretty large SOTA models, with parameters in the high millions/ low billions, so I\u2018ve been  looking into using model parallelism and batch parallelism since I\u2019m running out of RAM. I started implementing a BERT transformer in mesh tensorflow to get the hang of parallelism, but it\u2019s a little tricky and not very well documented. Is there a simpler library or maybe even a way to do it in standard Tensorflow/Keras? \n\nI\u2019m open to other deep learning frameworks too, I\u2019d just rather have something with more documentation given how long the BERT implementation took me to figure out all their functions.\n\nThanks!"}, {"id": "oah96t", "title": "[R] Dynamic Mode Decomposition for Control Affine Dynamical Systems, Traveling to (Japan?), and The Global Effort to Controlling SIR Models", "score": 3, "url": "https://www.reddit.com/r/MachineLearning/comments/oah96t/r_dynamic_mode_decomposition_for_control_affine/", "author": "AcademicOverAnalysis", "subreddit": "r/MachineLearning", "description": "This is the latest lecture for my class, Data Driven Methods in Dynamical Systems, which has continued through the summer after the official end of my course in the Spring. [https://youtu.be/A9sGC575CVA](https://youtu.be/A9sGC575CVA)\n\nThis topic demonstrates how you can use observed controller and trajectory data from a control affine dynamical system to make predictions about a new feedback controller that you want to analyze!\n\nThis theory might hold the record for how quickly it was developed in my group. We went from a vague desire to do DMD for control affine systems, and in two weeks, we developed two new operators (control Liouville Operators and a new Multiplication operator), a new kind of occupation kernel (control occupation kernels), and then we devised the theory that gave us a DMD algorithm that is \u201coff policy.\u201d And that ultimate manuscript that we wrote up in two weeks was accepted to MNTS 2020, but isn\u2019t going to appear until this August (2021\u2026 COVID\u2026)\n\narXiv link here: [https://arxiv.org/abs/2101.02620](https://arxiv.org/abs/2101.02620)\n\nI had a lot of fun working on this project, publishing the results, and making this video. I took a prompt from The Tim Traveler\u2019s YouTube channel (International Staycation Challenge) to start the video off, and I still manage to tie it back into the problem at hand.\n\n(One typo in the video I found. The feedback controller should be mapping to R\\^m, not R\\^n.)\n\nI have learned a lot from this community, and I look forward to everyone\u2019s input."}, {"id": "oa7d2v", "title": "[D] Tutorial on Neural Temporal Point Processes", "score": 16, "url": "https://www.reddit.com/r/MachineLearning/comments/oa7d2v/d_tutorial_on_neural_temporal_point_processes/", "author": "shchur", "subreddit": "r/MachineLearning", "description": "Temporal point processes (TPPs) are generative models for continuous-time event sequences, like transactions in financial systems, timestamps of earthquakes, or entries in electronic health records. Old-school TPP models (Poisson processes, Hawkes processes) can only capture simple patterns, such as global trends or burstiness. Neural TPPs combine the fundamental ideas from point process literature with deep learning approaches, and thus allow us to define more flexible and scalable TPP models.\n\nThis blog post goes over the main components of a neural TPP model and shows how to implement one in PyTorch.\n\nhttps://shchur.github.io/blog/2021/tpp2-neural-tpps/\n\nThis is the second part of my [series of posts](https://shchur.github.io/blog/) on TPPs. The main idea of this series is to summarize all the things I wish someone had told me when I started working on TPPs myself \u2013 most of the knowledge in this field is spread across different papers and textbooks. I hope this will be interesting for people who want to learn about TPPs or are already working on this topic."}, {"id": "oalo7w", "title": "[D] Modeling around biases within data", "score": 1, "url": "https://www.reddit.com/r/MachineLearning/comments/oalo7w/d_modeling_around_biases_within_data/", "author": "jj4646", "subreddit": "r/MachineLearning", "description": "Suppose you are trying to build a statistical model that helps you triage medical patients at a day clinic.\n\n Let's suppose there is a general assumption: patients that are prioritized and seen first (i.e. they wait less), end up spending on average shorter times in the clinic (e.g. compared to similar patients who weren't prioritized).\n\n However, at the same time, you decide to use \"total time spent\" as response variable for the statistical model : the decision to triage a patient is largely decided by how long they are expected to stay at the clinic. Thus, data that comes into existence after the implementation of the triage model will reflects the impact of prioritization/deprioritization of past cases (let's assume that prior to this model, patients were exclusively seen on a \" first come first served \" basis).\n\nGiven these considerations, do there exist any methods in statistics that are capable of \"untangling these biases\" or figure out ways to \"model around these biases\"? Or once these biases exist in the data, they then become impossible to isolate?\n\nThanks"}, {"id": "oa8hm9", "title": "[D] Paper digest: Alias-Free GAN\" by Tero Karras et al. explained in 10 minutes!", "score": 6, "url": "https://www.reddit.com/r/MachineLearning/comments/oa8hm9/d_paper_digest_aliasfree_gan_by_tero_karras_et_al/", "author": "KirillTheMunchKing", "subreddit": "r/MachineLearning", "description": " \n\n[ Pay attention to the beard moving separately from the face on the left image ](https://reddit.com/link/oa8hm9/video/uc3kixmdk7871/player)\n\nStyleGAN2 is king, except apparently it isn't. Tero Karras and his pals at NVIDIA developed a modification of StyleGAN2 that is just as good in terms of image quality, yet drastically improves the translational and rotational equivariance of images. In other words, the synthesis process no longer depends on absolute pixel coordinates, textures are not sticking to coordinates, instead moving together with the corresponding objects. This is a big deal since slight changes to the architecture solve fundamental problems with the generator's design making GANs better suited for video and animation.\n\nRead the [full paper digest](https://t.me/casual_gan/58) (reading time \\~10 minutes) to learn about the revamped design of the generator inspired by ideas from digital signal processing. For example, how images are treating as discrete sample grids that represent bandlimited functions on a continuous domain, and how continuous translational and rotational equivariance are enforced with specially designed alias-suppressing upsampling filters and nonlinearities.\n\nMeanwhile, check out the paper digest poster by [Casual GAN Papers](https://t.me/casual_gan)!\n\n[Modified StyleGAN2 architecture for alias free synthesis](https://preview.redd.it/ujjv2pbck7871.png?width=801&format=png&auto=webp&s=9c6e33580aff06f5031bcfb298367a178fa7f6c4)\n\n\\[[Full Explanation Post](https://t.me/casual_gan/58)\\] \\[[Arxiv](https://arxiv.org/pdf/2101.04061.pdf)\\] \\[[Code](https://nvlabs.github.io/alias-free-gan/)\\]\n\nMore recent popular computer vision paper breakdowns:\n\n>\\[[CIPS](https://t.me/casual_gan/51)\\]  \n>  \n>\\[[GFPGAN](https://t.me/casual_gan/54)\\]  \n>  \n>\\[[GANs N' Roses](https://t.me/casual_gan/53)\\]"}, {"id": "oaj18g", "title": "[D] Efficient implementations of spatially separable convolutions with channels", "score": 0, "url": "https://www.reddit.com/r/MachineLearning/comments/oaj18g/d_efficient_implementations_of_spatially/", "author": "henning_ml", "subreddit": "r/MachineLearning", "description": "I seem to have a very simple problem that I still cannot seem to solve.\n\nI have a problem where I need to convolve an input with a spatially (not depthwise) separable kernel `K`. Let `K` have shape `(C,C,N,N)` with `C` denoting channels and `N` the spatial dimensions. I can factor `K` into `Kx` and `Ky` such that `Kx.shape = (C,C,N,1)` and `Ky.shape = (C,C,1,N)` and `K = Kx*Ky` (assuming python broadcasting rules).\n\nFor spatially separable kernels with `C=1`, I can obviously do `O = conv(conv(I, Kx), Ky)` and I am done. However, that does not work when I have channels.\n\nWhat chaining the convolutions does is:\n\n    for channel_in in range(C):\n        for channel_out in range(C):\n            for i,j in (H,W):\n                for x in N:\n                    O_[channel_out, i, j] += I[channel_in, i+x, j]*Kx[channel_in, channel_out, x]\n    for channel_in in range(C):\n        for channel_out in range(C):\n            for i,j in (H,W):\n                for y in N:\n                    O[channel_out, i, j] += O[channel_in, i, j+y]*Ky[channel_in, channel_out, x]\n\nI am mixing channels already! What I need to do is:\n\n    for channel_in in range(C):\n        for channel_out in range(C):\n            for i,j in (H,W):\n                for x in N:\n                    O_[channel_out, i, j] += I[channel_in, i+x, j]*Kx[channel_in, channel_out, x]\n                for y in N:\n                    O[channel_out, i, j] += O_[channel_in, i, j+y]*Kx[channel_in, channel_out, y]\n    \n\nIs there an easy way to express this with regular convolutions in an efficient way? Am I missing something?"}, {"id": "oa8wx9", "title": "[D] Real NVP Source Code", "score": 4, "url": "https://www.reddit.com/r/MachineLearning/comments/oa8wx9/d_real_nvp_source_code/", "author": "iamanoriginalname", "subreddit": "r/MachineLearning", "description": "I am trying my best to find the official implementation for the original Real NVP paper [https://arxiv.org/abs/1605.08803](https://arxiv.org/abs/1605.08803). People say that it has been incorporated into the Tensorflow library. However, after looking at the source code for the RealNVP module of TensorFlow [https://github.com/tensorflow/probability/blob/v0.13.0/tensorflow\\_probability/python/bijectors/real\\_nvp.py#L38-L332](https://github.com/tensorflow/probability/blob/v0.13.0/tensorflow_probability/python/bijectors/real_nvp.py#L38-L332), I don't think it is the code that the author used to obtain the results in the paper, since it does not use Residual net (which is what the authors used to obtain their results). I am wondering if anyone has the original code that the author provided?\n\nI found the RealNVP page in Modelzoo.co [https://modelzoo.co/model/realnvp](https://modelzoo.co/model/realnvp) that claims to be the original code by the authors. However, the link to Github is no longer valid."}, {"id": "o9s7e0", "title": "[P] Sketched Webpages Generator - A customizable open-source software to generate randomized sketched web-pages, for the deep learning problem of converting sketches to code", "score": 91, "url": "https://www.reddit.com/r/MachineLearning/comments/o9s7e0/p_sketched_webpages_generator_a_customizable/", "author": "devtarek", "subreddit": "r/MachineLearning", "description": "Hi everyone,\n\nWe coded this synthesized dataset generator to contribute in the field of converting wireframes / hand sketched images of webpages to code (HTML) using deep learning approaches.\n\nCheck [project repository on github](https://github.com/Dev-Tarek/sketched-webpages-generator)\n\nThis was actually a part our graduation project that I published a year ago, but I couldn't follow up with the research because I had to do military service.\n\nI thought of sharing it here to see what you guys think if this currently helps anyone and to know if this problem (sketch to code using deep learning) is still active, so I hope to get your feedback!\n\nThanks."}, {"id": "oa79dv", "title": "[P] Still working on AI painting from source image. Source image is photo. Canvas texture added after painting finished in GIMP.", "score": 4, "url": "https://www.reddit.com/r/MachineLearning/comments/oa79dv/p_still_working_on_ai_painting_from_source_image/", "author": "gbbb1982", "subreddit": "r/MachineLearning", "description": "&#x200B;\n\nhttps://preview.redd.it/xp8dlnh877871.jpg?width=1166&format=pjpg&auto=webp&s=c04d7bc13b80e8c88d3523774e3e8ead26c16210"}, {"id": "oaaoja", "title": "[R] DeepMind & Amii Extend Emphatic Algorithms for Deep RL, Improving Performance on Atari Games", "score": 2, "url": "https://www.reddit.com/r/MachineLearning/comments/oaaoja/r_deepmind_amii_extend_emphatic_algorithms_for/", "author": "Yuqing7", "subreddit": "r/MachineLearning", "description": "A research team from DeepMind and Amii extends the emphatic method to multi-step deep reinforcement learning (RL) targets, and demonstrates that combining emphatic trace with deep neural networks can improve performance on classic Atari video games. \n\nHere is a quick read: [DeepMind & Amii Extend Emphatic Algorithms for Deep RL, Improving Performance on Atari Games.](https://syncedreview.com/2021/06/29/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-51/)\n\nThe paper *Emphatic Algorithms for Deep Reinforcement Learning* is on [arXiv](https://arxiv.org/abs/2106.11779)."}, {"id": "oa9g2r", "title": "[P] Made a reddit Comment Ripper a while ago, decided to share it with everyone.", "score": 2, "url": "https://www.reddit.com/r/MachineLearning/comments/oa9g2r/p_made_a_reddit_comment_ripper_a_while_ago/", "author": "assassinshadow11", "subreddit": "r/MachineLearning", "description": "For a while I was looking for a tool that could extract comments from relevant reddit posts for ML purposes, could not find one that would suit my needs hence, built this little tool that does the job. It's pretty easy to add tags we are interested in and tags we deem irrelevant along with some more minor controls. \n\nGithub Link: [https://github.com/assassinshadow0/RedditCommentRipper](https://github.com/assassinshadow0/RedditCommentRipper) (C#, unity. I'm sorry about the crappy UI)\n\nThe data set is used in a project that generates compliments based on selfies. It is not hyper accurate but I did get some chuckles here and there.\n\nSelfie Compliment: [https://selfieai.live/](https://selfieai.live/)\n\nHope someone finds the tool useful."}, {"id": "o9v6az", "title": "[D] What is the smallest dataset/architecture where you can observe deep double descent?", "score": 31, "url": "https://www.reddit.com/r/MachineLearning/comments/o9v6az/d_what_is_the_smallest_datasetarchitecture_where/", "author": "Eodmg", "subreddit": "r/MachineLearning", "description": "I read the OpenAI paper a while back and tried to see it in action on a dataset of mine, but I never managed to see the test error go back down even after a bunch of tuning. What datasets/architectures work for seeing it in action?\n\nAlso, my dataset is quite noisy (the same input corresponds to different outputs in many data points). The authors notice that the second descent happens once the interpolation threshold is reached. What happens if there is no interpolation threshold in the case of noisy data? Does deep double descent not occur or does it occur once it has reached the minimum achievable error?\n\nAlso, is there a reasonably decent definition of when the interpolation threshold has been reached? If the network interpolates perfectly, then there are no gradient-based updates that can be made, meaning that the second descent is over. How can we know when we are \"close enough\" to interpolation to expect that the test error will begin to go down?"}, {"id": "o9myxp", "title": "[R] Leibniz University Hannover Proposes World-GAN: A 3D GAN for Minecraft Level Generation", "score": 78, "url": "https://www.reddit.com/r/MachineLearning/comments/o9myxp/r_leibniz_university_hannover_proposes_worldgan_a/", "author": "Yuqing7", "subreddit": "r/MachineLearning", "description": "A research team from Leibniz University Hannover introduces World-GAN, a 3D generative adversarial network that aims to learn and generate structures directly in the 3D voxel space in Minecraft. \n\nHere is a quick read: [Leibniz University Hannover Proposes World-GAN: A 3D GAN for Minecraft Level Generation.](https://syncedreview.com/2021/06/28/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-50/)\n\nThe source code is available on the project [GitHub](https://github.com/Mawiszus/World-GAN). The paper *World-GAN: a Generative Model for Minecraft Worlds* is on [arXiv](https://arxiv.org/abs/2106.10155)."}, {"id": "oa5p9i", "title": "[P] pytorch-widedeep v1.0: deep learning for tabular data that you can combine with images and text", "score": 2, "url": "https://www.reddit.com/r/MachineLearning/comments/oa5p9i/p_pytorchwidedeep_v10_deep_learning_for_tabular/", "author": "jrzaurin", "subreddit": "r/MachineLearning", "description": "pytorch-widedeep is a package to use deep learning with tabular data. In particular, is intended to facilitate the combination of text and images with corresponding tabular data using wide and deep models. There are four models available for tabular data, including ResNets, TabNet and the TabTransformer. More models and some additional functionalities coming soon! If you want to contribute you are more than welcome. \n\n* [Main repo](https://github.com/jrzaurin/pytorch-widedeep)\n* [Documentation](https://pytorch-widedeep.readthedocs.io/en/latest/index.html)\n* [Posts and Tutorials](https://jrzaurin.github.io/infinitoml/)\n* [Code for over 1000 experiments and a comparison with LightGBM](https://towardsdatascience.com/pytorch-widedeep-deep-learning-for-tabular-data-iv-deep-learning-vs-lightgbm-cadcbf571eaf?source=your_stories_page-------------------------------------)"}, {"id": "oa8dh6", "title": "PyKale Preprint: Knowledge-Aware Machine Learning from Multiple Sources in Python [P][R]", "score": 0, "url": "https://www.reddit.com/r/MachineLearning/comments/oa8dh6/pykale_preprint_knowledgeaware_machine_learning/", "author": "haipinglu", "subreddit": "r/MachineLearning", "description": "A [10-page preprint](https://arxiv.org/abs/2106.09756) is on arXiv to describe the **green machine learning** design principles behind our **pipeline-based API** below as well as features and examples in our [PyKale library](https://github.com/pykale/pykale) for multimodal learning and transfer learning with deep learning and dimensionality reduction on graphs, images, texts, and videos to enable and accelerate **interdisciplinary** research: [\\[2106.09756\\] PyKale: Knowledge-Aware Machine Learning from Multiple Sources in Python (arxiv.org)](https://arxiv.org/abs/2106.09756) \n\n[Pipeline-Based API in PyKale](https://preview.redd.it/8ktlzokwi7871.png?width=1671&format=png&auto=webp&s=17450d0ba6901e2553798c86d8dec23c3985fea4)\n\n[Green Machine Learning Concepts in PyKale](https://preview.redd.it/9typko3xg7871.png?width=1457&format=png&auto=webp&s=ee8e28254a1fa7e354e0de48cb2950c6fa164b94)"}, {"id": "o9euga", "title": "[D] CVPR officially bans social media discussion during the review period", "score": 249, "url": "https://www.reddit.com/r/MachineLearning/comments/o9euga/d_cvpr_officially_bans_social_media_discussion/", "author": "MassivePellfish", "subreddit": "r/MachineLearning", "description": "> The current PAMITC policies prohibit authors from issuing press releases or talking with the press about papers that are under review at IEEE/CVF conferences. This facilitates double blind review and helps preserve the public trust in the scientific peer review process.\n\n\nDetails:\n\nhttps://docs.google.com/document/d/17TdORiPEwsEBnZdFci7OeOgv0jiI37udK9z1vlE4GOQ/edit\nhttps://amytabb.com/ts/2021-06-16-cvpr-motion-position/"}, {"id": "oa7k7k", "title": "[D] how do I feed audio to a neural network?", "score": 0, "url": "https://www.reddit.com/r/MachineLearning/comments/oa7k7k/d_how_do_i_feed_audio_to_a_neural_network/", "author": "xXDarkCubeXx", "subreddit": "r/MachineLearning", "description": "I'm trying to build an audio to midi converter with the help of DL. so what I've been doing is just feeding raw audio to a dense network (3 hidden with BN) and it tells me which note was played. And although the network seems to be learning something (it outputs predictions with very high confidence even 0s and 0.999s, \\[not sure if that's good or if it should be calibrated\\]). However, the recall, the precision after only a few stabilize at around 0.1, 0.3 respectively which is a bit disappointing and for the loss it just spikes once in a while but I think that's just variation in the data so we can assume it's stuck at a local minima and the network isn't learning jack sh\\*t.\n\nI know feeding raw audio is the wrong way to do it but I was just experimenting. I also know feeding a spectogram to CNN network would likely produce better results.\n\nMy question is what are other methods that I can use that I don't know about? and what can I learn from my little experiment? and are the results I'm getting normal?\n\nAlso, feel free to link some good resources where I can learn more about DL audio."}, {"id": "oa73jw", "title": "[D] Is Colab Pro recommended for training a large dataset in machine learning?", "score": 1, "url": "https://www.reddit.com/r/MachineLearning/comments/oa73jw/d_is_colab_pro_recommended_for_training_a_large/", "author": "saucyhambon", "subreddit": "r/MachineLearning", "description": "Hello all,\n\nHope you are all doing good. \n\nFor my MSc project, I am using Google Colab for masked language modelling, and my dataset contains over 70,000 instances.\n\nNow, I have started to train my model using my large dataset, but it is taking forever using the GPU runtime on Colab.\n\nFor example, at the moment, I have initiated my trainer; and it has trained 217 instances out of a possible 48474 - which has taken 40 minutes so far!\n\nSo, I am wondering,if I upgrade to Colab Pro, would it speed up my training? And is it worth it for what I am doing?\n\nThanks."}, {"id": "oa3303", "title": "[R] Time-Series Representation Learning via Temporal and Contextual Contrasting", "score": 2, "url": "https://arxiv.org/abs/2106.14112", "author": "emad_eldeen", "subreddit": "r/MachineLearning", "description": ""}, {"id": "o9zopz", "title": "[N][R] A Brief Tutorial for Developing AutoML Tools with Hypernets", "score": 4, "url": "https://www.reddit.com/r/MachineLearning/comments/o9zopz/nr_a_brief_tutorial_for_developing_automl_tools/", "author": "DCL-2021", "subreddit": "r/MachineLearning", "description": "# A Brief Tutorial for Developing AutoML Tools with Hypernets\n\n*Please see* [*here*](https://github.com/DataCanvasIO/Hypernets) *for the* `Hypernets` *library.*\n\nParameter tuning is an inevitable step for successfully building a machine learning model. Even for a simple model as K-nearest neighbors(KNN) for the classification task, we need to at least determine the number of the neighbors and the distance metric to be used to predict the label of a given example. Let alone models which have much more tunable parameters and have to be trained multiple times before we can pick suitable values for their parameters. Furthermore, tuning parameters in a brute force approach is inefficient while using an advanced search method takes intensive efforts. Can we focus more on parts of machine learning like designing novel models while only performing procedures like parameter tuning in a simple and happy way?\n\nThe answer is positive.\n\n`Hypernets`, a unified Automated Machine learning(AutoML) framework, offers us a very simple way to solve such problems. Taking the parameter tuning problem of the KNN model as an example, using a `search_param` function from the `Hypernets`, the only required work for us is to define a function serving as the measure of the quality of a set of given parameters.\n\n    from sklearn import neighbors\n    \n    def score_function(X_train, y_train, X_evl, y_evl, \n                       n_neighbors=Choice([3, 5, 6, 10, 20]),\n                       weights=Choice(['uniform', 'distance']),\n                       algorithm=Choice(['auto', 'ball_tree', 'kd_tree', 'brute']),\n                       leaf_size=30,\n                       p=Choice([1, 2])):\n        # The avaliable values for each tunable parameter are those provided by the list\n        # elements of the argument of Choice(). For example, the parameter \"n_neighbors\",\n        # the number of the nearest neighbors used to predict the label of a given example, \n        # can be chosen as 3, 5, 6, 10, and 20. \n        model = neighbors.KNeighborsClassifier(n_neighbors, weights, algorithm, leaf_size, p)\n        model.fit(X_train, y_train)\n        scores = model.score(X_evl, y_evl) #This score is taken as the mean accuracy of the model on (X_evl, y_evl)\n        return scores\n\nCurrently, there is no need to know how the function `search_param` is able to perform parameter tuning by utilizing the above score function--we only manually provide possible values we like to `Choice()` for each tunable parameter. Now let's use the `search_param` function with the gird search algorithm, or other search algorithms such as random search or Monte-Carlo Tree search, to find the suitable parameter values for our KNN model by simply passing the `score_function` defined above as an argument to it:\n\n    import hypernets.utils.param_tuning as pt\n    history = pt.search_params(score_function, 'grid', max_trials=10, optimize_direction='max')\n\nThe best model parameters can be obtained by calling the following method of `history`\n\n    best_param = history.get_best().sample\n\nThis is not the whole story.\n\nParameter tuning is only a fraction of the full-pipeline AutoML process and `Hypernets` is capable of doing far more things than just tuning parameters. In the following sections, we will briefly introduce `Hypernets` as an AutoML framework and wish to clarify:\n\n* the basic building blocks of `Hypernets`;\n* basic procedures to develop an AutoML tool for parameter tuning problem and the more general full-pipeline machine learning modeling;\n* some advanced features of `Hypernets`.\n\n## Parameter tuning for KNN with an AutoML tool built with Hypernets\n\n`Hypernets` is an AutoML framework that allows the users to easily develop various kinds of AutoML and Automated Deep Learning(AutoDL) tools without reinventing some necessary components which are often common to such tools. Before `Hypernets`, there already existed many AutoML tools. However, these tools are usually designed for some specific purposes thus not convenient to be generalized to other ones. As a result, the AutoML community may have to take a lot of efforts to repeatedly develop some common parts before deploying their AutoML models due to the lack of an underlying AutoML framework. \n\nhttps://preview.redd.it/ry1cz7d0f4871.png?width=1029&format=png&auto=webp&s=3142f3f5fffba28e615ebbc6d352038aa31bcc64\n\n`Hypernets` can save such efforts to a large extent while offering more possibilities.\n\n* First, it decouples the basic components of a general [AutoML procedure](#fig_automl) as [four distinct parts](#fig_hypernets): the `HyperSpace`, the `Searcher`, the `HyperModel`, and the `Estimation Strategy`. This idea is motivated by allowing users to manipulate different components of an AutoML tool built with `Hypernets` accordingly for different purposes.\n\n&#x200B;\n\nhttps://preview.redd.it/zs83na02f4871.png?width=1809&format=png&auto=webp&s=0a7c4dbbc443fb7397a3dad6f022f3cfbd7833bd\n\n* Second, the `HyperSpace` is designed to be a powerful search space. The `HyperSpace` consists of three different kinds of space: the **module space**, the **parameter space** and the **connection space**, where the module space is designed to contain various machine learning models, data preprocessing or feature engineerings, the parameter space provides the parameters to be searched for machine learning models and the connection space determines the way how different module spaces connect. These connected module spaces and parameter spaces finally give us a highly comprehensive search space which is able to describe the full-pipeline machine learning modeling ranging from data preprocessing to model ensemble.\n* Third, `Hypernets` provides many search algorithms including simple methods, such as Random Search and Grid Search, and advanced ones such as Monte-Carlo Tree Search. Users can not only simply choose one from these efficient search methods but also similarly design new search algorithms.\n* Finally, `Hypernets` also supports many advanced techniques to further improve performances of the trained machine learning models. For example, users can apply early stopping to accelerate the training process and prevent overfitting; data cleaning can be applied to improve data quality; data drift detection can be enabled to improve the generalization ability of the model, etc.\n\nBased on the above brief introduction, using the `Hypernets` to develop an AutoML tool can now be decomposed as three parts: designing the **Search Space**, an instance of the `Hyperspace`, constructing the **Hypermodel** which will be sampled from the search space using a searcher provided by `Hypernets` during the search process, and building the **Estimator** which receives a sampled Hypermodel, evaluates it and then returns the corresponding rewards such that the searcher can update the Hypermodel to be sampled based on the rewards.\n\nWe will provide a toy example, designing an AutoML tool for KNN, to help the readers walk through all steps of implementing the `Hypernets` to an AutoML task.\n\nTo reveal the core features and ideas of `Hypernets`, we first continue to solve the problem defined in the very beginning--how to perform parameter tuning of KNN automatically using `Hyernets`\\--but in a different manner: we view the parameter tuning problem as a complete AutoML task and develop a complete AutoML tool for this task from scratch using `Hypernets`. For simplicity, we only consider the classification task, and the regression case can be easily generalized. As introduced above, this developing procedure contains 3 steps and we will simply follow these steps. See [here](https://github.com/BochenLv/knn_toy_model/blob/main/hypertoy/param_tuning_v2.py) for details.\n\n* ***Designing the search space.*** In the case of parameter tuning, our search space of the AutoML task, a HyperSpace, is very simple in the sense that there is only one module space which contains only one machine learning model--our KNN model--along with its parameter space. To incorporate these spaces, we first define the ParameterSpace for tunable parameters with different values and then build the whole HyperSpace to include this ParameterSpace so that the search algorithm can search suitable parameters among available ones.\n* ***Constructing the Hypermodel.*** The HyperModel does not require many modifications for our specific task since many core functionalities of the HyperModel have already been well defined in `Hypernets` and are common across different machine learning models and tasks. We only pay attention to two functions, the `_get_estimator`, which returns the corresponding KNN model of the sampled search space, and the `load_estimator`, which loads the configurations of the saved model. The most important method for a HyperModel is the \"search\" method. By calling the `search` method, the search algorithm searches in the search space and returns a sample of the search space to be utilized for the HyperModel. This HyperModel is then evaluated based on the chosen reward metric and updated towards the optimizing direction.\n* ***Building the Estimator.*** Building the Estimator often takes the most effort for developing a new AutoML tool using `Hypernets`. The `Estimators` required by `Hypernets` is in fact a more general notion than the frequently used one in `sklearn`\\--the machine learning model. Fortunately, for our case of parameter tuning of KNN, the `Estimator` is easy to be implemented since the sampled search space only contains one machine learning model which is the only thing that needs to be evaluated by the `Estimator`. Moreover, we emphasize that the actual abilities of the `Estimator` are not restricted to that defined in this section and we refer the readers to the [next section](#sec_eg) for further details.\n\nWith the above AutoML tool, we are now ready to perform a complete automatic parameter tuning for KNN. In general, we only need four lines of codes to complete such implementation after we finish designing the required AutoML tools--not for the specific example presented here but a more general routine. This routine is summarized as follows:\n\n1. Define the search space.\n2. Choose a searcher from those search algorithms provided by `Hypernets`. One required  argument for the searcher is the search space in which the searcher will perform searching.\n3. Construct the HyperModel which receives the searcher as its required argument. In our example, the HyperModel is the `KnnModel`.\n4. The `search` method of our HyperModel is called to automatically perform the search process on the dataset (X\\_train, y\\_train) and record the current best model parameters.\n5. One can get the best model in the following way:\n\nNow we can celebrate the fine-tuned KNN model!\n\nThe convenience of following this procedure lies in that one needs not to develop anything else to perform parameter tuning of the KNN model for other classification task datasets without categorical features. Instead, simply passing these datasets to the `search` method of the `KnnModel` will return us the model with suitable parameters.\n\nHowever, readers will also immediately notice that, before sending the dataset to the model, one has to manually handle the categorical features of some datasets if there exist such things because the KNN model can not treat with categorical features properly. Some users may also want our AutoML tool to be able to perform more things like data cleaning. It is therefore a great idea to extend our AutoML tool for the KNN model to automate the full pipeline of machine learning tasks once for all. These are exactly the topics of the [next section](#sec_eg).\n\n## Building your full-pipeline AutoML tool for KNN<span id=sec_eg>\n\nTypically, the procedures of a full-pipeline machine learning modeling range from data preprocessing to model ensemble. For the purpose of enabling our AutoML tool to automate such full-pipeline modeling, we need to design a more comprehensive search space, which should at least include transformations of the data, feature engineerings, and the machine learning models along with their tunable parameters. Such an AutoML tool will largely relieve us from the headaches of dealing with data and feature issues of datasets.\n\nTherefore, the most important part and the primary work we will do is to extend our search space based on the introduction of the basic building blocks of `Hypernets` in the last section. For clarity, we still follow the 3 steps of developing our AutoML tools for full-pipeline KNN model with `Hypernets` as indicated before.\n\n* ***Designing a search space.*** To enable our AutoML tool to perform procedures like data    preprocessing, we need to encapsulate these procedures into module spaces for our search space, a `HyperSpace` object, and then connect them using the `ConnectionSpace` as introduced above. For this reason, these module spaces are now divided into two kinds: one containing the **preprocessor** and the other for **machine learning model**, i.e. KNN model here. We now devote to wrapping these two kinds of module spaces into our search space respectively for full-pipeline AutoML process.\n* ***Constructing the Hypermodel.*** Similar to the last section of the parameter tuning problem, to construct the HyperModel(named as `KnnModel`) one only needs to define two functions properly: `_get_estimator` and `load_estimator`. Other necessary parts of it have already been well defined in `Hypernets`.\n* ***Building the Estimator.*** One may immediately notice that we nearly did nothing in last step. Is our Hypermodel defined there a unique one? The answer is positive. The uniqueness of `HyperModel` built for a specific machine learning model, e.g. the Hypermodel for KNN or support vector machine, is provided by its associated Estimator through receiving the corresponding search space. As discussed before, the Estimator used in `Hypernets` is a more general notion than the usual one--the machine learning model--which is a fraction of the Estimator but also the origin of the uniqueness of each Estimator because the steps before introducing machine learning models to the full-pipeline modeling are usually common for different cases. As a result, although an Estimator usually includes many arguments and functions to support advanced features of `Hypernets`, fortunately, there is nearly nothing that needs to be rewritten from scratch when we want to extend our procedures to other machine learning models.\n\nThere are extra things need to be noted: our KNN model should be utilized in the form of a `ModuleSpace` in the search space and should automatically adjust itself for the classification or regression task. For these purposes, a `ComplexKnn` is provided to wrap the KNN to the HyperSpace for our full-pipeline machine learning modeling when we designed our search space:\n\n        class ComplexKnn(HyperEstimator):\n            def __init__(self, fit_kwargs, \n                            n_neighbors=2, weights='uniform', algorithm='brute', \n                            leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None,\n                            space=None, name=None, **kwargs\n                        ):\n                ...\n                HyperEstimator.__init__(self, fit_kwargs, space, name, **kwargs)\n    \n            def _build_estimator(self, task, kwargs):\n                if task == 'regression':\n                    knn = KnnRegressorWrapper(**kwargs)\n                else:\n                    knn = KnnClassifierWrapper(**kwargs)\n                return knn     \n\nwhere the `HyperEstimator` inherits from the `ModuleSpace` to transfer our KNN model to a module space in the search space. Please refer [`estimator.py`](https://github.com/BochenLv/knn_toy_model/blob/main/hypertoy/estimator.py) for further details.\n\nWe now have the complete AutoML tool for full-pipeline machine learning modeling with KNN! Let's try to use our extended AutoML tool for an example following the routine discussed in the end of the last section:\n\n    #Load the data and suppose that the task is multi-classification\n    from sklearn.model_selection import train_test_split\n    X, y = load_your_data()\n    X_train, y_train, X_test, y_test = train_test_split(X, y, test_size=0.1)\n    \n    #Design a search space\n    search_space = get_your_search_space\n    \n    #Choose a searcher from the Hypernets searchers\n    searcher = GridSearcher(search_space)\n    \n    #Pass the searcher as an argument to your model, a Hypermodel object\n    model = KnnModel(searcher, task='multiclass', reward='accuracy')\n    \n    #Call the 'search' method\n    model.search(X_train, y_train, X_eval=X_test, y_eval=y_test)\n\nFor convenience, we also provide an [example](https://github.com/BochenLv/knn_toy_model/blob/main/test_full_pipeline_simple.ipynb). With this kind of AutoML tool, we can simply pass the datasets to our AutoML tools without considering issues regarding the datasets for that our AutoML tool is designed to automate the full-pipeline of machine learning modeling. More importantly, our procedures presented here can be easily generalized to other machine learning models. There are also many techniques such as cross validation which can be further added to our toy tool to improve its performance. We will leave these contents for future discussions."}, {"id": "o9hdle", "title": "[D] Did anyone ever go through a 3-hours long panel interview for a machine learning position?", "score": 68, "url": "https://www.reddit.com/r/MachineLearning/comments/o9hdle/d_did_anyone_ever_go_through_a_3hours_long_panel/", "author": "vaibhavsxn", "subreddit": "r/MachineLearning", "description": "Can you share your insights about it? What can a person talk about for 3 hours?"}, {"id": "oa43bf", "title": "[D] Reduce a data set of a cone-like object surrounded by outliers to the cone-like object?", "score": 0, "url": "https://www.reddit.com/r/MachineLearning/comments/oa43bf/d_reduce_a_data_set_of_a_conelike_object/", "author": "mavavilj", "subreddit": "r/MachineLearning", "description": "Reduce a data set of a cone-like object surrounded by outliers to the cone-like object?\n\nParticularly,  what would be a robust method, since I cannot assume what kind of  outlier there may be. The outliers may have points higher than the cone  object's highest point. Or outlier may be very low in height.\n\nI  was first thinking of looking at second derivative of the contour curve  of the entire data in order to figure out where there's a change from  \"outlier data\" to \"cone-like object\". But I'm not sure if this is  robust."}, {"id": "o9xijo", "title": "[D] Machine Learning Python Tooling and their place in a pipeline?", "score": 2, "url": "https://www.reddit.com/r/MachineLearning/comments/o9xijo/d_machine_learning_python_tooling_and_their_place/", "author": "iamquah", "subreddit": "r/MachineLearning", "description": "Hey all! \n\nI was wondering if people could give insights into various tooling around ML? I've come into various issues that I'm sure have been addressed before. I'd love some insights into what issues you faced and what tools / solutions you came up with? In my case I've been battling 2 issueS:\n\nExperiment tracking: It's really easy to get lost in the sea of configs for the different experiments (was the learning rate here 0.1 or 0.11?) and I'm wondering how people address this issue early on. I've been looking at mlflow and it looks like it will be appropriate for what I'm considering so far? My hacky solution is to create various data classes that can be json-ified which I can upload with the weights. This way I can \"search\" through my models and find what experiments have been tried.\n\nFeature stores: My data can and should probably be cached at intermediate steps but how do you clearly represent \"where\" the data came from? I.e The current flow is A -> B -> C but you've now got an experiment running A->B->D or A->B->C' where C' is just some variant on the processing e.g average of 10 days as opposed to 5. I've not thought up a solution yet but I'd love to hear what other people think\n\ntl;dr what tools do you wish you had used and what issues do you wish you had the foresight to anticipate in your startup / end-to-end ML application?"}, {"id": "oa122p", "title": "[D] What are the current state-of-the-art models for regression on images using CNN?", "score": 0, "url": "https://www.reddit.com/r/MachineLearning/comments/oa122p/d_what_are_the_current_stateoftheart_models_for/", "author": "DolantheMFWizard", "subreddit": "r/MachineLearning", "description": "I'm working in converting finance data into images then using a CNN to predict rate of return from a particular company in 4 months. So this is a regression problem and I'd like to know the current SOTA methods in CNN regression."}, {"id": "o9zzev", "title": "[D] can regression models be used for ranking?", "score": 0, "url": "https://www.reddit.com/r/MachineLearning/comments/o9zzev/d_can_regression_models_be_used_for_ranking/", "author": "jj4646", "subreddit": "r/MachineLearning", "description": "Suppose you create a regression model that predicts  the hourly salary based on age and height. Suppose you have some new age and height measurements for 5 new people. The regression model predicts the salary of these 5 new people, e.g. $20/hr, $15/hr, $0/hr, $40/hr, $100/hr.\n\nDoes the statistical framework of regression models allow you to \"statistically rank\" these people? Suppose the model was a good fit for the training data, can you say with statistical confidence that \"it's likely the 5th person earns the most and the 3rd person earns the least\"?\n\nOr are there sepperate \"ranking models\" for these kinds of problems?\n\nThanks"}, {"id": "o9lqb8", "title": "Probabilistic modelling [project] w. dependent types: ML engineering as research (early stages)", "score": 9, "url": "https://www.reddit.com/r/MachineLearning/comments/o9lqb8/probabilistic_modelling_project_w_dependent_types/", "author": "tmp-1379", "subreddit": "r/MachineLearning", "description": "I've been working on [a probabilistic modelling framework](https://github.com/joelberkeley/spidr) with an API in Idris, a purely functional programming language with a very advanced type system incl. dependent types, quantitative types and theorem proving. For starters, you can do things like\n\n    concat : Tensor (n :: tail) dtype ->\n             Tensor (m :: tail) dtype -> Tensor ((n + m) :: tail) dtype\n\nThe project isn't meant to be like your standard ML framework. In particular, I'm not focusing on features. I'm more interested in **exploring** how to express things clearly and effectively, through cutting edge engineering technologies and techniques, and really good docs, and to help others come to their own decisions about how they can be clear and effective in their own work.\n\nIt's still very early stages. The tensor API's incomplete, and I'm just starting on adding a backend, so nothing runs yet (though you may be surprised how much meaningful content you can add with types). I'm intending to target Graphcore's Poplibs for the backend, but I'd like to think the design allows for others, such as TensorFlow if someone so wished.\n\np.s. apologies for reposts - had difficulty with the flair. This is also research. Is it possible to have more than one flair?\n\nEDIT mistake in \\`concat\\`"}, {"id": "o9gkxk", "title": "[D] Behavioral Testing of ML Models (Unit tests for machine learning) [Video]", "score": 14, "url": "https://www.reddit.com/r/MachineLearning/comments/o9gkxk/d_behavioral_testing_of_ml_models_unit_tests_for/", "author": "jayalammar", "subreddit": "r/MachineLearning", "description": "This video discusses the paper [Beyond Accuracy: Behavioral Testing of NLP Models with CheckList](https://www.aclweb.org/anthology/2020.acl-main.442/), winner of Best Paper at ACL 2020.\n\nEvaluating ML models using a single metric (like accuracy or F1-score) produce a low-resolution picture of model performance. Behavioral tests can give us a much higher resolution evaluation of a model's capabilities. By creating tests (which are small targeted test sets), we can better compare models or observe how model performance changes after re-training a model (or fine-tuning it).\n\n[https://youtu.be/Cse-3MM7mso](https://youtu.be/Cse-3MM7mso)\n\nHope you enjoy it"}, {"id": "o9weql", "title": "[D] Machine Learning Models for Triage", "score": 0, "url": "https://www.reddit.com/r/MachineLearning/comments/o9weql/d_machine_learning_models_for_triage/", "author": "jj4646", "subreddit": "r/MachineLearning", "description": " \n\nIn theory, could hospital triage be solved using \"regression models\" (e.g. logistic regression, random forest regression)? Suppose you have a historical dataset where you data (covariates) on patients such as \"age, gender, height, weight, blood pressure, etc.\" , and the response variable is \"the number of days the patient spent in the hospital\".\n\n1) Just a shot in the dark - it seems that you should be able to make a regression model that predicts \"how long the patient is expected to stay in the hospital\"?\n\n2) Suppose the hospital is interested in improving how they triage their patients. In this imaginary example, let's say that the hospital wants to identify patients who will be discharged from the hospital shortly after they come to the hospital (because the logic being, patients who leave the hospital shortly after coming to the hospital, usually have non threatening conditions and do not need to be prioritized). Suppose the hospital makes such a regression model : now for 5 new patients, they predict the discharge times for these patients as (2 days, 5 days, 30 days, 40 days, 10 days).\n\nIf the hospital has reasons to believe that the regression model is effective (based on performance)- (along with the prediction intervals for these estimates) would it be reasonable for the hospital not to prioritize the patients who are expected to stay only 2 days and 5 days in the hospital?\n\n3) does anyone have any recommendations for statistical/machine learning models that can be used to help humans with these kinds of triage decisions?\n\nThanks"}, {"id": "o8x5uo", "title": "[R] Building robust biodiversity-focused models for passive monitoring sensors - Link to free zoom lecture by the authors in comments", "score": 467, "url": "https://i.redd.it/i749evku8t771.gif", "author": "pinter69", "subreddit": "r/MachineLearning", "description": ""}, {"id": "o9f4t9", "title": "[N] Facebook AI Releases An Image Similarity Data Set And Announces The Launch Of An Associated Competition With A $200,000 Prize Pool (Paper and Competition Details Included)", "score": 19, "url": "https://www.reddit.com/r/MachineLearning/comments/o9f4t9/n_facebook_ai_releases_an_image_similarity_data/", "author": "techsucker", "subreddit": "r/MachineLearning", "description": "Recently, Facebook released the [Image Similarity data set](https://arxiv.org/pdf/2106.09672.pdf) and announced an associated\u00a0[competition\u00a0](https://www.drivendata.org/competitions/79/competition-image-similarity-1-dev/)hosted by DrivenData with a whopping $200,000 prize pool. The competition began on June 19, 2021, and will conclude on October 28, 2021. The challenge is being supported by Pinterest, BBC, Getty Images, iStock, and Shutterstock.\n\nThe data set contains nearly 1 million reference images and 50,000 query images, some of which are manipulated versions of a reference image. Through this dataset and challenge, Facebook hopes to enable new implementations of ML-based systems that can be utilized to help predict the similarity of two pieces of visual content and aid the industry in the at-scale detection of manipulated images\n\nFull Story: [https://www.marktechpost.com/2021/06/28/facebook-ai-releases-an-image-similarity-data-set-and-announces-the-launch-of-an-associated-competition-with-a-200000-prize-pool/](https://www.marktechpost.com/2021/06/28/facebook-ai-releases-an-image-similarity-data-set-and-announces-the-launch-of-an-associated-competition-with-a-200000-prize-pool/) \n\nPaper: https://arxiv.org/abs/2106.09672\n\nCompetition: https://www.drivendata.org/competitions/79/competition-image-similarity-1-dev/"}, {"id": "o9recq", "title": "[D] Any good resources on Causal RL?", "score": 0, "url": "https://www.reddit.com/r/MachineLearning/comments/o9recq/d_any_good_resources_on_causal_rl/", "author": "Ok-Philosophy562", "subreddit": "r/MachineLearning", "description": "I came across the topic of causal RL on an ICML tutorial.  Any resources (preferably videos or courses) on this topic?\n\nThe tutorial: https://icml.cc/virtual/2020/tutorial/5752\n\nEdit:  My background is in CV, RL, I am trying to find some \u201cfriendly\u201d resources (preferably videos or courses) so I don\u2019t have to learn all the causal inference which includes non-ML related stuff."}, {"id": "o97ack", "title": "[D]Reward is Unnecessary", "score": 38, "url": "https://www.reddit.com/r/MachineLearning/comments/o97ack/dreward_is_unnecessary/", "author": "Thunderbird120", "subreddit": "r/MachineLearning", "description": "[Link to article](https://tilde.town/~fessus/reward_is_unnecessary.pdf)\n\nSUMMARY:\n\nThis article argues that reward is not necessary for high level intelligent problem solving. \n\nLarge self-supervised sequence models approximate another type of theoretical model which is capable of almost anything, except being trained. This type of model attempts to directly model the underlying joint probability distribution of data in a self-supervised way to facilitate the prediction of arbitrary unknown data given arbitrary known data. In an idealized context, a model capable of modeling its environment sufficiently well is capable of arbitrarily high level problem solving without the need for any reward to pursue. This kind of model is also conceptually equivalent to a perfect data compression algorithm. While this kind of model can't be trained, useful approximations of it can and represent the best path toward human level A.I."}, {"id": "o9b2jn", "title": "[R] A Systematic Study of High-Frequency Bias in CNNs", "score": 17, "url": "https://www.reddit.com/r/MachineLearning/comments/o9b2jn/r_a_systematic_study_of_highfrequency_bias_in_cnns/", "author": "abello966", "subreddit": "r/MachineLearning", "description": "Hello, /r/MachineLearning! \n\nI come here with our recently accepted [workshop paper](https://openaccess.thecvf.com/content/CVPR2021W/UG2/papers/Abello_Dissecting_the_High-Frequency_Bias_in_Convolutional_Neural_Networks_CVPRW_2021_paper.pdf) for [CVPR](https://openaccess.thecvf.com/CVPR2021_workshops/UG2). We test out the common hypothesis that CNNs are overly reliant on invisible high-frequency patterns, and that this would make them brittle, but help generalization. We set out to reproduce Jo & Bengio (2017), which seems to be the first to put out this theory but upon expanding the experiments we found out that their pattern didn't completely hold. Still, we discovered some interesting things on the way. Code is included in our paper, but you can find [it here](https://github.com/Abello966/FrequencyBiasExperiments)\n\nI'm humbly asking for comments, thoughts, critiques or anything else you'd have :)"}, {"id": "o9o1ov", "title": "The benefits of a fixed amount of nodes vs letting the agent add and remove nodes? [D]", "score": 0, "url": "https://www.reddit.com/r/MachineLearning/comments/o9o1ov/the_benefits_of_a_fixed_amount_of_nodes_vs/", "author": "Lobob2", "subreddit": "r/MachineLearning", "description": "The use of a fixed sets and amounts of nodes in agents always seemed strange to me not being able to save unnecessary computing that could be used training other agents. Then as the neural network grows you can remove agents to maximize the processing power you have. I assume there\u2019s a reason pick a set amount of nodes instead but id like to know why."}, {"id": "o9njja", "title": "[P] A website that guess your skin undertone (cool, neutral, warm or olive) using ML", "score": 0, "url": "https://www.reddit.com/r/MachineLearning/comments/o9njja/p_a_website_that_guess_your_skin_undertone_cool/", "author": "Useful_Conclusion_43", "subreddit": "r/MachineLearning", "description": "Hello,\n\nI made a website that guess your skin undertone (cool, neutral, warm or olive) using ML.\n\n[https://whatismyskinundertone.com](http://whatismyskinundertone.com)\n\nThe biggest challenge was to get quality data. I needed photos of people labeled with their skin undertone. I payed multiple make up artist to label data and when all the make up artist said the same   undertone, I added the data. I trained on \\~1000 pics.\n\nHave a good day :)"}, {"id": "o9nbgc", "title": "[D] Jordan Harrod on being an AI researcher and educator", "score": 0, "url": "https://www.reddit.com/r/MachineLearning/comments/o9nbgc/d_jordan_harrod_on_being_an_ai_researcher_and/", "author": "SkynetToday", "subreddit": "r/MachineLearning", "description": "Hello! Not sure if people are aware of Jordan Harrod on here, she is a PhD Candidate in the Harvard-MIT Health Sciences and Technology  program and a YouTuber who creates educational videos about AI. She has a pretty significant following on Youtube and makes weekly videos on a bunch of topics related to AI:\n\nhttps://preview.redd.it/m1qqsw3x41871.png?width=986&format=png&auto=webp&s=4e44ed3e8f265720f042d69c85b498e2b8064462\n\nWe just released an [interview](https://www.letstalkai.show/e/jordan-harrod/) in which we discuss both her research and youtube work in some depth. Check it out if interested! And welcome any feedback wrt interview style/production."}, {"id": "o9ix8x", "title": "[D] Does LSGAN need weight decay or gradient penalty?", "score": 2, "url": "https://www.reddit.com/r/MachineLearning/comments/o9ix8x/d_does_lsgan_need_weight_decay_or_gradient_penalty/", "author": "chasep255", "subreddit": "r/MachineLearning", "description": "I have been recently trying to use LSGAN to enhance the output of an autoencoder.  Based on the original LSGAN paper I read, I see no mention of it needing to restrict the gradients like is the case with WGAN.  However, I have come across some other references where they refer to LSGAN with weight decay or gradient penalty but offer no explanation as to why.  I can't really find an explanation elsewhere.  So does LSGAN need weight decay or gradient penalty?"}, {"id": "o9fhxb", "title": "[R] Call for Participation to NL-Augmenter \ud83e\udd8e \u2192 \ud83d\udc0d", "score": 3, "url": "https://www.reddit.com/r/MachineLearning/comments/o9fhxb/r_call_for_participation_to_nlaugmenter/", "author": "Ganglion_Varicose", "subreddit": "r/MachineLearning", "description": "Hi  r/MachineLearning Members!\n\nWe, a team of researchers spanning Google AI Language, UW, CMU and 7 other institutions organizing NL-Augmenter\u00a0\ud83e\udd8e \u2192 \ud83d\udcf7.\u00a0, are now inviting  transformation submissions to the same!\n\nAll submitters of accepted transformations (and filters) will be included as co-authors on a paper announcing this framework. NL-Augmenter\u00a0\ud83e\udd8e \u2192 \ud83d\udcf7 is a part of the wider GEM benchmark,  [GEM (Generation, Evaluation, Metrics)](https://gem-benchmark.com/nl_augmenter) workshop at ACL, 2021 and their future iterations.\n\nThe NL-Augmenter is a collaborative effort intended to add transformations of datasets dealing with natural language. Transformations augment text datasets in diverse ways, including: introducing spelling errors, translating to a different language, randomizing names and numbers, paraphrasing ... and whatever creative augmentation you contribute to the benchmark.\u00a0We invite submissions of transformations to this framework by way of GitHub pull request, through **September 1, 2021**.\u00a0**All submitters of accepted transformations (and filters) will be included as co-authors on a paper announcing this framework.**\n\nProject:\u00a0[https://github.com/GEM-benchmark/NL-Augmenter](https://github.com/GEM-benchmark/NL-Augmenter)\n\nWe\u00a0strongly believe that the benefits of open science should reach\u00a0everyone and hence we are making this effort to reach you. We also encourage you to share this with other researchers in\u00a0your department who\u00a0would benefit from\u00a0this open collaboration. To know more about the\u00a0framework, check our [motivation and review criteria](https://github.com/GEM-benchmark/NL-Augmenter/blob/main/docs/doc.md)\u00a0and some of\u00a0[our recent work](https://arxiv.org/pdf/2106.09069.pdf).\n\nOrganizers:\n\nKaustubh Dhole (Amelia R&D) , Sebastian Gehrmann (Google AI Language), Varun Gangal (LTI, Carnegie Mellon University), Jascha Sohl-Dickstein (Google Brain), Tonghuang Wu (University of Washington), Simon Mille (Universitat Pompeu Fabra)\u00a0, Zhenhao Li (Imperial College, London), Saad Mahmood (Trivago R&D), Aadesh Gupta (Amelia R&D), Samson Tan (Salesforce Research), Jinho Choi (Emory University)"}, {"id": "o9kcj2", "title": "[D] How to convince management of machine learning?", "score": 0, "url": "https://www.reddit.com/r/MachineLearning/comments/o9kcj2/d_how_to_convince_management_of_machine_learning/", "author": "HXSC", "subreddit": "r/MachineLearning", "description": "This is a question/discussion not so much of machine learning itself, but how to convince others of the merits of ML models.\n\nFor a lot of people, it is often difficult to understand the results of machine learning, or why it makes decisions the way it does. Of course there are explainability/interpretability methods, but I often find management objecting to ML because \"it makes no sense\" and because \"I don't understand it\", especially if they have been making heuristics-based decisions for decades in the field.\n\nML can be a sledgehammer that's not always fit for all occasions, but sometimes it is. But it sometimes seems difficult to convince people that it is sometimes the right tool."}, {"id": "o99zf5", "title": "[P] Swin Transformer TensorFlow Implementation", "score": 10, "url": "https://www.reddit.com/r/MachineLearning/comments/o99zf5/p_swin_transformer_tensorflow_implementation/", "author": "PreparationOld2671", "subreddit": "r/MachineLearning", "description": "A few others and I recently implemented a TensorFlow version of Microsoft's Swin Transformer ([https://arxiv.org/abs/2103.14030](https://arxiv.org/abs/2103.14030)). It's an (almost) direct translation of the official PyTorch code ([https://github.com/microsoft/Swin-Transformer](https://github.com/microsoft/Swin-Transformer)) so that people can easily switch reading between the two. The TensorFlow version repo also includes code that converts PyTorch .pth weights into TensorFlow checkpoints. Using this file, you guys can either use the pretrained weights provided by Microsoft or weights of a custom model trained using PyTorch. I hope you guys find it useful!\n\nHere's the GitHub link: [https://github.com/VcampSoldiers/Swin-Transformer-Tensorflow](https://github.com/VcampSoldiers/Swin-Transformer-Tensorflow)"}, {"id": "o9bgf3", "title": "[D] Some questions about membership inference attack/privacy leakage", "score": 4, "url": "https://www.reddit.com/r/MachineLearning/comments/o9bgf3/d_some_questions_about_membership_inference/", "author": "stevenzhang9577", "subreddit": "r/MachineLearning", "description": "Hi, I'm focusing on the membership inference attack on federated learning. But when I run the code such as DLG algorithm or ML\\_leaks algorithm, all of their code based on CNN rather than federated learning model.\n\nHere's my problem, what's the difference of the MIA algorithms between machine learning and federated learning? I believe there should be some difference between MIA on ML and FL, but I can't find it."}, {"id": "o90tpi", "title": "[D] Nobody here likes \"Artificial Intelligence: A Modern Approach\" by Russel and Norvig", "score": 18, "url": "https://www.reddit.com/r/MachineLearning/comments/o90tpi/d_nobody_here_likes_artificial_intelligence_a/", "author": "arsenale", "subreddit": "r/MachineLearning", "description": "I wondered why it wasn't widely recommended, and then I discovered that it included a section about machine learning only in the 2021 edition, and a chapter about deep learning by Goodfellow (author of Deep Learning\nBook by Aaron Courville, Ian Goodfellow, and Yoshua Bengio).\nProbably it's considered by many an \"encyclopedia\" book, but I find some parts really concise and interesting."}, {"id": "o9fq9k", "title": "[P] Re-implementation of \"Winning the L2RPN Challenge: Power Grid Management via Semi-Markov Afterstate Actor-Critic\"", "score": 1, "url": "https://www.reddit.com/r/MachineLearning/comments/o9fq9k/p_reimplementation_of_winning_the_l2rpn_challenge/", "author": "enliteAI", "subreddit": "r/MachineLearning", "description": "Hi everyone!\n\nThe \u201c[Learning to run a power network](https://competitions.codalab.org/competitions/32935?secret_key=91e789ba-ec54-4db1-a105-57bd8527fd03)\u201d (L2RPN) challenge is a series of competitions proposed by [Kelly et al. (2020)](https://arxiv.org/pdf/2003.07339.pdf) with the aim to test the potential of reinforcement learning to control electrical power transmission.\n\nWe re-implemented the approach winning at 2020's IEEE World Congress on Computational Intelligence - [Semi-MDP with an after-state representation](https://openreview.net/pdf?id=LmUJqB1Cz8) \\- with our applied reinforcement learning framework [Maze](https://github.com/enlite-ai/maze). We discuss this in [this blog post](https://enliteai.medium.com/reinforcing-power-grids-a-baseline-implementation-for-l2rpn-830401fd2e62) and make our implementation available [here](https://github.com/enlite-ai/maze_smaac).\n\nWe\u2019d  be happy if this is useful to to anybody's power grid (and any other) RL experiments\u200a - \u200amaybe even for winning the next challenge!"}, {"id": "o980gz", "title": "[D] Both US and Chinese governments report using ML to study UAP - why?", "score": 5, "url": "https://www.reddit.com/r/MachineLearning/comments/o980gz/d_both_us_and_chinese_governments_report_using_ml/", "author": "Asp184", "subreddit": "r/MachineLearning", "description": "I've been following the recent news on Unidentified Aerial Phenomenon (UAP). The topic has heated up lately, with the US government appearing to earnestly investigate the issue. The following report was released Friday:\nhttps://www.dni.gov/files/ODNI/documents/assessments/Prelimary-Assessment-UAP-20210625.pdf\n\nThe report includes: \"The initial focus will be to employ artificial intelligence/machine learning algorithms to cluster and recognize similarities and patterns in features of the data points.\"\n\nThere's a recent report about the Chinese government also using ML to investigate UAP:\nhttps://www.scmp.com/news/china/science/article/3136078/china-military-uses-ai-track-rapidly-increasing-ufos\n\nWith my (limited) background in ML, I just don't see how ML is in any way connected to identifying what UAP are. What am I missing here? Are these govt reports just throwing around buzzwords to please the public or is this a significant development?"}, {"id": "o8vyt1", "title": "[D] A critical take on: The Dimpled Manifold Model of Adversarial Examples in Machine Learning (Full Video Analysis)", "score": 27, "url": "https://www.reddit.com/r/MachineLearning/comments/o8vyt1/d_a_critical_take_on_the_dimpled_manifold_model/", "author": "ykilcher", "subreddit": "r/MachineLearning", "description": "[https://youtu.be/k\\_hUdZJNzkU](https://youtu.be/k_hUdZJNzkU)\n\nAdversarial Examples have long been a fascinating topic for many Machine Learning researchers. How can a tiny perturbation cause the neural network to change its output by so much? While many explanations have been proposed over the years, they all appear to fall short. This paper attempts to comprehensively explain the existence of adversarial examples by proposing a view of the classification landscape, which they call the Dimpled Manifold Model, which says that any classifier will adjust its decision boundary to align with the low-dimensional data manifold, and only slightly bend around the data. This potentially explains many phenomena around adversarial examples. Warning: In this video, I disagree. Remember that I'm not an authority, but simply give my own opinions.\n\n&#x200B;\n\nOUTLINE:\n\n0:00 - Intro & Overview\n\n7:30 - The old mental image of Adversarial Examples\n\n11:25 - The new Dimpled Manifold Hypothesis\n\n22:55 - The Stretchy Feature Model\n\n29:05 - Why do DNNs create Dimpled Manifolds?\n\n38:30 - What can be explained with the new model?\n\n1:00:40 - Experimental evidence for the Dimpled Manifold Model\n\n1:10:25 - Is Goodfellow's claim debunked?\n\n1:13:00 - Conclusion & Comments\n\n&#x200B;\n\nPaper: [https://arxiv.org/abs/2106.10151](https://arxiv.org/abs/2106.10151)\n\nMy replication code: [https://gist.github.com/yk/de8d987c4eb6a39b6d9c08f0744b1f64](https://gist.github.com/yk/de8d987c4eb6a39b6d9c08f0744b1f64)\n\nGoodfellow's Talk: [https://youtu.be/CIfsB\\_EYsVI?t=4280](https://youtu.be/CIfsB_EYsVI?t=4280)"}, {"id": "o8t8ho", "title": "[D] distance between high dimensional vectors", "score": 45, "url": "https://www.reddit.com/r/MachineLearning/comments/o8t8ho/d_distance_between_high_dimensional_vectors/", "author": "tarunn2799", "subreddit": "r/MachineLearning", "description": "hey, I'm working on a project that involves finding the distances between high dimensional embedding points for a given dataset. I take the output vectors of a model which have 2048 dimensions, and I am trying to find the distance between this point and another similar point. I used euclidean distance to do this but came across the concept of the curse of dimensionality that makes euclidean distance useless at higher dimensions.   \n\n\nI'm trying to figure out what other methods I can use to calculate the distance between these points the right way. TIA"}, {"id": "o9bjd7", "title": "Could an adversarial environment AI be feasibly trained to eliminate a percentage of the agents training on it to maximize agent improvement? [D]", "score": 0, "url": "https://www.reddit.com/r/MachineLearning/comments/o9bjd7/could_an_adversarial_environment_ai_be_feasibly/", "author": "Lobob2", "subreddit": "r/MachineLearning", "description": "I\u2019m working with a coding friend to design car driving agents, I proposed an adversarial track building ai that would be trained to eliminate 80% of the agents, so as the agents improve, the track becomes harder and harder, hopefully leading to the agents going around pixel perfect curves and other interesting behavior. Is this feasible or has it already been done?"}, {"id": "o8uoog", "title": "[D] Eigenvalues from non-linear auto-encoder - is there any paper about this ?", "score": 19, "url": "https://www.reddit.com/r/MachineLearning/comments/o8uoog/d_eigenvalues_from_nonlinear_autoencoder_is_there/", "author": "DanielWicz", "subreddit": "r/MachineLearning", "description": "In general, in auto-encoder, you can get phi\\_k \\* x\\_i, where x\\_i is your input and phi\\_k is your eigenvector of covariance matrix. My question is, if there any paper, that describes how to approximate eigenvalues of the eigenvectors for nonlinear auto-encoders so far? Because I can't find any so far"}, {"id": "o8x801", "title": "[D] SimSwap - Deepfake Without Training", "score": 8, "url": "https://youtu.be/is347MG71yY", "author": "farfromhome2020", "subreddit": "r/MachineLearning", "description": ""}, {"id": "o90jnd", "title": "[D] Projects similar to DreamCoder?", "score": 5, "url": "https://www.reddit.com/r/MachineLearning/comments/o90jnd/d_projects_similar_to_dreamcoder/", "author": "rickeinstein1", "subreddit": "r/MachineLearning", "description": "does anyone know of any projects like [dreamcoder](https://arxiv.org/abs/2006.08381)? i.e. algorithms that can take in a question and output a program(in a specific programming language) which can solve the problem"}, {"id": "o843t5", "title": "[D] Types of Machine Learning Papers", "score": 2248, "url": "https://i.redd.it/y24wbhmjjj771.jpg", "author": "TheInsaneApp", "subreddit": "r/MachineLearning", "description": ""}, {"id": "o932ji", "title": "[D] How can dilated convolutions handle variable length input?", "score": 2, "url": "https://www.reddit.com/r/MachineLearning/comments/o932ji/d_how_can_dilated_convolutions_handle_variable/", "author": "CS_Student95", "subreddit": "r/MachineLearning", "description": "In [this](https://arxiv.org/pdf/1901.10738.pdf) paper the authors discuss using dilated convolutions for processing time series data. They chose this model for their encoder in part because their encoder needed \"to allow variable-length inputs\".\n\nI was wondering how exactly this would work though? Would you build your architecture so it handles the longest sequence you know you'll feed it, then when passing in shorter sequences, just pad the beginning with 0's?"}, {"id": "o8rnk8", "title": "[D] dimensionality reduction: autoencoders vs umap", "score": 18, "url": "https://www.reddit.com/r/MachineLearning/comments/o8rnk8/d_dimensionality_reduction_autoencoders_vs_umap/", "author": "jj4646", "subreddit": "r/MachineLearning", "description": "It seems that dimensionality reduction techniques like pca and tsne are being considered as \"older methods\", whereas autoencoders and umap are being considered as \"newer\".\n\nWhen you compare umap vs autoencoders: do any of either of these methods have any \"structural advantages\" compared to the other?\n\nFor example, when comparing tsne and pca: tsne is said to be very effective at data visualization, but not as effective in terms of dimensionality reduction. You can not use the \"embeddings\" produced from the tsne algorithm as \"inputs\" for classification model (e.g. random forest). Secondly, tsne is said to only \"learn\" the current data - supposedly, new data can not be \"explained\" by the tsne algorithm trained on old data. On the other hand, pca is said to be able to do both.\n\nWhen comparing umap vs autoencoders, are they both capable of doing the same thing? Can both of these algorithms produce outputs that can be used as inputs for classification algorithms? Can both of these algorithms \"learn\" new data? In terms of modelling complexity - do both of these algorithms have the capacity to model similar complexity data?\n\nJust using logic, here is where i think the differences are:\n\n1) one of these algorithms might have a computational advantage over the other (i.e. faster)\n\n2) autoencoders have many possible configurations (e.g. number of neurons in each layer, number of layers, activation functions), whereas the main customization in umap is in the number of neighbors to consider.\n\nThis is all that I can think of.\n\nCan someone please provide comments?\n\nThanks"}, {"id": "o92p3k", "title": "[D] What are the parameters ranges for fine tuning a BERT model for multilabel text classification problem?", "score": 2, "url": "https://www.reddit.com/r/MachineLearning/comments/o92p3k/d_what_are_the_parameters_ranges_for_fine_tuning/", "author": "Towhidul_Tonmoy", "subreddit": "r/MachineLearning", "description": "I need to finetune BERT model on a multi label sentiment classification task. However, my dataset is really small, 5k.\n \nDo you have any suggestions for learning rate, batch size,train test split,dropout, epoch number warmup steps etc.?"}, {"id": "o964yg", "title": "[D] Has anyone ever used machine learning algorithms in the context of survival analysis before?", "score": 2, "url": "https://www.reddit.com/r/MachineLearning/comments/o964yg/d_has_anyone_ever_used_machine_learning/", "author": "jj4646", "subreddit": "r/MachineLearning", "description": "https://sites.google.com/view/chil-survival\n\nhttps://arxiv.org/abs/0811.1645 (survival random forest)\n\nhttps://scikit-survival.readthedocs.io/en/stable/user_guide/random-survival-forest.html\n\nHas anyone ever tried using machine learning algorithms (e.g. such as random forest and neural networks) in the context of survival analysis before?\n\nI have started reading about this topic (machine learning based survival analysis), and it would appear that machine learning based survival models have the ability to estimate the survival curves for individual observations (e.g. scroll  down, https://rviews.rstudio.com/2017/09/25/survival-analysis-with-r/ ).\n\nI just want to confirm: does this mean that mathematically, machine learning based survival models have the ability to predict estimated survival curves (in the form of a Kaplan-Mier \"staircase\") on the individual level? Is this correct?\n\nI read the following statement that sounds concerning:\n\n\"For other survival models that do not rely on the proportional hazards assumption, it is often impossible to estimate survival or cumulative hazard function. Their predictions are risk scores of arbitrary scale. If samples are ordered according to their predicted risk score (in ascending order), one obtains the sequence of events, as predicted by the model.\u00a0\" (https://scikit-survival.readthedocs.io/en/stable/user_guide/understanding_predictions.html)\n\nCan someone please confirm if I have understood this correctly? In the end, I want to use the individual estimated survival curves for comparison purposes (e.g. ranking the median survival times of individuals).\n\nThanks"}, {"id": "o95rp7", "title": "[D] Problem About Real Time Object Detection On Raspberry Pi", "score": 1, "url": "https://www.reddit.com/r/MachineLearning/comments/o95rp7/d_problem_about_real_time_object_detection_on/", "author": "curiousbutadhd", "subreddit": "r/MachineLearning", "description": "Hey people,\nWe are building autonomous underwater vehicle and using yolov5 on raspberry pi for realtime object detection but model detects object from photo about 3 seconds and its so slow for us.\nSo do you have any advice, should i use just opencv without ML model.\nI just need to detect rectangle and circle with specific measurements.\nI am programmer but dont have lots of ecperience with these subjects.\nPlease help me \ud83d\ude4f \nThanks\u2026"}, {"id": "o91avm", "title": "[P] Example ResNet50 params for NN-512 (stand-alone C code for AVX-512 neural nets)", "score": 2, "url": "https://www.reddit.com/r/MachineLearning/comments/o91avm/p_example_resnet50_params_for_nn512_standalone_c/", "author": "NN-512", "subreddit": "r/MachineLearning", "description": "This is Jonathan Aylard's illustration of how Caffe or Tensorflow networks can be ported to NN-512, through simple conversion scripts:\n\nhttps://github.com/jonatron/test_nn512\n\nNN-512 generates stand-alone C code for AVX-512 neural nets. The goal is to reduce the cost of inference by using CPU cloud instances, and to provide neural net implementations with no dependencies: \n\nhttps://NN-512.com/ \n\nWith NN-512, you can easily do 15 ResNet50 inferences per second on a single-core CPU cloud instance that costs $10 per month."}, {"id": "o8z61k", "title": "[D] How do I represent sample efficiency of RL rewards in mathematical notation?", "score": 1, "url": "https://www.reddit.com/r/MachineLearning/comments/o8z61k/d_how_do_i_represent_sample_efficiency_of_rl/", "author": "sarmientoj24", "subreddit": "r/MachineLearning", "description": "So, I define sample efficiency as **the area under the curve/graph** where x axis is the number of episodes while y-axis is the cumulative reward for that episode. ***I would like to formally define it with a mathematical function,***\n\nIf the notation for cumulative reward for xth episode is:\n\nhttps://preview.redd.it/imwvh4jzst771.png?width=690&format=png&auto=webp&s=edda1cb0907ed945fbf055259aae8940d605a9cf\n\nSo is the equation for area under the graph/curve the one below?\n\nhttps://preview.redd.it/zrgbwqjwst771.png?width=266&format=png&auto=webp&s=f42e17344380341dd66d02ab892a0adfef26bc05\n\nI will be just using a Python library to get the area under the graph which uses Simpson's rule for integrating."}, {"id": "o8vc8e", "title": "[D] Comparing M2M to mT5 in low resource translation (10k dataset Yoruba - English)", "score": 2, "url": "https://www.reddit.com/r/MachineLearning/comments/o8vc8e/d_comparing_m2m_to_mt5_in_low_resource/", "author": "maroxtn", "subreddit": "r/MachineLearning", "description": "I wrote an article lately that compares two big language models, M2M and mT5 on the translation task. Both are new, their usage is not very clear, and and fall under the same type.\n\nThe dataset I used for fine tuning has 10k sentence pairs.\n\nI found no clear comparison nor a clear guide on how to fine tune both of the models on the translation task, so I decided to write it myself. (code: [https://github.com/maroxtn/mt5-M2M-comparison](https://github.com/maroxtn/mt5-M2M-comparison))\n\nI trained the two models to translate text from Yoruba (Nigeria) to English, you can see example of the produced translations here:\n\n**Yoruba**: \u00ecgb\u00e0 wo l\u1eb9 di \u1eb9l\u1eb9\u0301r\u00ec\u00ed j\u00e8h\u00f3f\u00e0?  \n**Target**: when did you become one of jehovah\u2019s witnesses?\n\n**M2M**: when were you one of jehovah\u2019s witnesses?  \n**mT5**: what is the meaning of jehovah\u2019s word?\n\n\u2014 \u2014 \u2014 -\n\n**Yoruba**: r\u00e1r\u00e1 \u1eb9 \u00f2 l\u00e8 b\u00e1 a s\u1ecd\u0300r\u1ecd\u0300  \n**Target**: no you can\u2019t\n\n**M2M**: sorry you can\u2019t tell me  \n**mT5**: no you can\u2019t talk to us now\n\n\u2014 \u2014 \u2014 -\n\n**Yoruba**: mo m\u1ecd\u0300 \u00e8y\u00ed k\u00ed n t\u00f3 n\u00ed k\u00ed \u00f3 f\u1eb9\u0301 mi  \n**Target**: i knew this before i told you to marry me\n\n**M2M**: i know that before i had to marry me  \n**mT5**: i know that this is why i want to see you\n\n\u2014 \u2014 \u2014 -\n\n**Yoruba**: \u00e8y\u00ed ni \u00e0w\u1ecdn \u00ecf\u1eb9s\u1eb9\u0300w\u1ecdns\u1eb9\u0300 t\u00ed y\u00f3\u00f2 m\u00e1a w\u00e1y\u00e9 n\u00edn\u00fa \u00ecd\u00edje or\u00edl\u1eb9\u0300-\u00e8d\u00e8 n\u00e0\u00ecj\u00ed\u00edr\u00ed\u00e0 npfl l\u00f3n\u00ec\u00ed:  \n**Target**: this are the matches that will be coming up in the nigeria npfl today:\n\n**M2M**: these are the matches that will be played at the nigeria npfl today:  \n**mT5**: the following are the results of the nigerian football league (npfl) competition:\n\nM2M outperformed mT5 by a big margin (rouge 23 vs 45)\n\nRead more about it here: [https://abdessalemboukil.medium.com/comparing-facebooks-m2m-to-mt5-in-low-resources-translation-english-yoruba-ef56624d2b75](https://abdessalemboukil.medium.com/comparing-facebooks-m2m-to-mt5-in-low-resources-translation-english-yoruba-ef56624d2b75)"}, {"id": "o94mri", "title": "[D] Are there any papers that train models to use (e.g.) vim's language of keyboard shortcuts?", "score": 0, "url": "https://www.reddit.com/r/MachineLearning/comments/o94mri/d_are_there_any_papers_that_train_models_to_use/", "author": "General_Example", "subreddit": "r/MachineLearning", "description": "For example, an input of \"move down 3 lines\" into the model would output \"3j\".\n\nI'm also interested in papers related to other domain specific languages too."}, {"id": "o8wwzx", "title": "[D] When to add intercept in GLM?", "score": 0, "url": "https://www.reddit.com/r/MachineLearning/comments/o8wwzx/d_when_to_add_intercept_in_glm/", "author": "HyogoKita19C", "subreddit": "r/MachineLearning", "description": "I am following cs229 autumn 2018, and I am working on PS#1.\n\nIn question 3, it asks you to train a GLM model with a Poisson response variable. However, the skeleton code loads the input without adding an intercept.\n\nOn the other hand, in question 1 using pure Logistic Regression, the skeleton code loads the input with an added intercept.\n\nShould I add the intercept myself? Is there an error in the code?\n\n&#x200B;\n\n\\--------------------\n\n&#x200B;\n\n(Side note, I am not even sure if the skeleton code and solution code are official. There are some very obvious mistakes in question 2 ...)\n\nFor those that are wondering, the solution I obtained looks like this:\n\ndef main(*lr*, *train\\_path*, *eval\\_path*, *pred\\_path*):  \n *\"\"\"Problem 3(d): Poisson regression with gradient ascent.*  \n*Args:*  \n*lr: Learning rate for gradient ascent.*  \n*train\\_path: Path to CSV file containing dataset for training.*  \n*eval\\_path: Path to CSV file containing dataset for evaluation.*  \n*pred\\_path: Path to save predictions.*  \n*\"\"\"*  \n \\# Load training set  \n *x\\_train*, *y\\_train* = util.load\\_dataset(*train\\_path*, add\\_intercept=False)  \n\n\n\\# \\*\\*\\* START CODE HERE \\*\\*\\*  \n   \n *model* = PoissonRegression(step\\_size=*lr*, eps=1e-5)  \n *model*.fit(*x\\_train*, *y\\_train*)  \n\n\n*x\\_eval*, *y\\_eval* = util.load\\_dataset(*eval\\_path*, add\\_intercept=False)  \n *y\\_pred* = *model*.predict(*x\\_eval*)  \nnp.savetxt(*pred\\_path*, *y\\_pred*)  \n\n\n   plt.figure()  \nplt.plot(*y\\_eval*, *y\\_pred*, 'bx')  \nplt.xlabel('true counts')  \nplt.ylabel('predict counts')  \nplt.savefig('output/p03d.png')  \n\n\n\\# \\*\\*\\* END CODE HERE \\*\\*\\*"}, {"id": "o8dgk2", "title": "[D] Access to computation at DeepMind, FAIR, OpenAI, Brain, Microsoft, etc.?", "score": 17, "url": "https://www.reddit.com/r/MachineLearning/comments/o8dgk2/d_access_to_computation_at_deepmind_fair_openai/", "author": "SubstantialRange", "subreddit": "r/MachineLearning", "description": "Does every research scientist have access to N (e.g. 4) GPUs at any point in time?\n\nDo you need to request access and wait until they're open? If so, what's the typical wait time?\n\nCould a research scientist use 64 GPUs for the next 6 months 24/7?"}, {"id": "o8gut3", "title": "[D] How to make the PyTorch training loop cleaner?", "score": 8, "url": "https://www.reddit.com/r/MachineLearning/comments/o8gut3/d_how_to_make_the_pytorch_training_loop_cleaner/", "author": "latticeprep", "subreddit": "r/MachineLearning", "description": "I know PyTorch Lightning is one way to make the training look a lot nicer.\n\nBut I cannot use it at my job as it's not part of our build yet.\n\nI'd like to clean up the training loop and find a better way to log and compute metrics.\n\nWhat are some examples of a clean PyTorch scaffolding, particularly around the training loop, that makes it easy to compute and log metrics?"}, {"id": "o8co7o", "title": "[R] Is there any research focused on making deep learning more efficient on CPUs?", "score": 16, "url": "https://www.reddit.com/r/MachineLearning/comments/o8co7o/r_is_there_any_research_focused_on_making_deep/", "author": "ats678", "subreddit": "r/MachineLearning", "description": "I know that running DL models on GPUs is nowadays standards, but is there anyone trying to make these models more computationally efficient on CPUs? Would love to read any paper\u2019s suggestions if you guys have any!"}, {"id": "o8blqm", "title": "Charformer: Fast Character Transformers via Gradient-based Subword Tokenization", "score": 18, "url": "https://arxiv.org/abs/2106.12672", "author": "argosopentech", "subreddit": "r/MachineLearning", "description": ""}, {"id": "o8ayk9", "title": "[P][SP] DINO - PyTorch implementation", "score": 17, "url": "https://www.reddit.com/r/MachineLearning/comments/o8ayk9/psp_dino_pytorch_implementation/", "author": "mildlyoverfitted", "subreddit": "r/MachineLearning", "description": "Hi all!  \n\n\nI created a video where I implemented DINO ([Emerging Properties in Self-Supervised Vision Transformers](https://arxiv.org/abs/2104.14294)) \"from scratch\". I took the official code the authors open-sourced, made multiple modifications and finally used it to train a model. The video is relatively long, so feel free to use the timestamps I provide in the description. Lastly, the video also contains two short tutorials on weight normalization and buffers in PyTorch.  \n\n\n[https://youtu.be/psmMEWKk4Uk](https://youtu.be/psmMEWKk4Uk)  \n\n\nHope some of you find it helpful!"}, {"id": "o8g1ns", "title": "[D] how to ensemble labels (not probability) of deep learning model", "score": 2, "url": "https://www.reddit.com/r/MachineLearning/comments/o8g1ns/d_how_to_ensemble_labels_not_probability_of_deep/", "author": "mrtac96", "subreddit": "r/MachineLearning", "description": "I have 7 deep learning models giving labels, I know we can get probabilities, but it's not possible for some reason. I want to know which approach I should use to ensemble these labels to increase ranking on leaderboard. Any other techniques that can help.? In my case test set is double than train.\nOne way is to take mode, but I am looking to get a boost on test set. The test set is totally blind, no initial ranking."}, {"id": "o8f74w", "title": "[R] Sigmoid output on U-Net architecture causing blank output masks?", "score": 3, "url": "https://www.reddit.com/r/MachineLearning/comments/o8f74w/r_sigmoid_output_on_unet_architecture_causing/", "author": "Falnor", "subreddit": "r/MachineLearning", "description": "Hey all,\n\nI'm working on a semantic segmentation network using a [U-Net architecture](https://arxiv.org/abs/1505.04597), but I'm having some serious issues getting good results. I noticed recently that the masks produced by my network were all negative, so I added a sigmoid activation in my output convolution layer hoping that would solve my problem and force the network to learn realistic pixel values, but it seems to have just caused all outputs to be zero. My training loss function is a Dice Loss, though I have also tried a summed Dice-BCE loss which appears to learn but produces the same output even after loss plateaus. Any help is appreciated.\n\nI'm inputting 512x512 images and using same-padding throughout the network, which is outputting three segmentation channels. The API I am using is PyTorch. The overall network architecture is identical to that shown in the paper above."}, {"id": "o8eyzr", "title": "[Research]Evaluating a CNN -multi class model with two separate thresholds", "score": 2, "url": "https://www.reddit.com/r/MachineLearning/comments/o8eyzr/researchevaluating_a_cnn_multi_class_model_with/", "author": "redpolarbearen", "subreddit": "r/MachineLearning", "description": " \n\nI have a model that outputs three classes. But here instead of one threshold, it depends on a combination of two (user input threshold). One threshold varies from 0.1 to 1.0 and the other varies from from 1 to 800 ( this is user input based on domain knowledge).\n\nHow can I evaluate this model for both a) Balanced dataset b) Imbalanced dataset\n\nA normal ROC samples 1D threshold space. How can I adjust this for 2 different thresholds?\n\nIs there any other metrics that I can use that will accommodate the two thresholds?\n\nIf I use sensitivity and recall..( and many others) I am not sure how to put the two threshold picture, do I have then look at the individual thresholds separately for ( example : For 25, we look at threshold ; 0/.9, 0.8, 0.7..) then for (50, we look at ...) and may be show one more case to explain how the threshold affect the positive predictions?"}, {"id": "o8atna", "title": "Is there any voice to voice synthesis available? [D]", "score": 5, "url": "https://www.reddit.com/r/MachineLearning/comments/o8atna/is_there_any_voice_to_voice_synthesis_available_d/", "author": "n1c39uy", "subreddit": "r/MachineLearning", "description": "I have read about text to speech synthesis but is there also something available that does speech to speech synthesis?\n\nSomething comparable to what we do with video deepfakes for example. I have looked around but found nothing interesting."}, {"id": "o8hqc0", "title": "[R] Towards Understanding and Mitigating Social Biases in Language Models", "score": 1, "url": "https://arxiv.org/abs/2106.13219", "author": "bert4QA", "subreddit": "r/MachineLearning", "description": ""}, {"id": "o85zh8", "title": "[R] [P] CTRLsum: Towards Generic Controllable Text Summarization Web Demo", "score": 9, "url": "https://v.redd.it/sxez4il49k771", "author": "Illustrious_Row_9971", "subreddit": "r/MachineLearning", "description": ""}, {"id": "o8cz7m", "title": "[D] How to combine multiple conditioning embedding?", "score": 2, "url": "https://www.reddit.com/r/MachineLearning/comments/o8cz7m/d_how_to_combine_multiple_conditioning_embedding/", "author": "chasep255", "subreddit": "r/MachineLearning", "description": "I have been working on a VQVAE model to encode music.  Now that I mostly have it working (still need to improve the sound a bit) I would like to use some sort of RNN network to generate music using the codebook much like a character rnn can generate text.  I would like to globally condition the RNN using embeddings based on the songs artist(s).  The issue with this is that most songs seem to have multiple artists.  So I figure there are 4 approaches I could take. \n\n1) Add the vectors from each of the artists.\n\n2) Average the vector across all artists.\n\n3) Take the max value from each artist's vector.\n\n4) Concatenate the vectors.\n\nI am not really sure what is the best approach.  I can think of issues with all 4 of them.  I really don't like #4 since that would be order dependent.  \n\nI think Jukebox AI had to solve this problem.  I could not really tell how from their paper, and I was unable to find where they do the artist embedding in their code.  Anyone know how it is done or have any thoughts?"}, {"id": "o7qfbp", "title": "[N] European AI Regulation", "score": 147, "url": "https://www.reddit.com/r/MachineLearning/comments/o7qfbp/n_european_ai_regulation/", "author": "ValidateML", "subreddit": "r/MachineLearning", "description": "Hi, machine learners of Reddit! The European Commission has recently proposed a new regulation for AI-based products that will affect already regulated as well as newly-regulated markets.\n\nThe EU AI Regulation will prohibit a small number of unacceptable-risk AIs and define a set of requirements for high-risk AIs. Many of the groundbreaking innovations in machine learning will be considered high-risk and thus be affected by this new regulation.\n\nIf you or your company is developing AI-centered software and you are interested to learn about the implications of the upcoming European AI Regulation, check out our upcoming (free) webinar:\n\n[https://www.linkedin.com/events/europeanairegulation-whatdoesit6810580422334939136/](https://www.linkedin.com/events/europeanairegulation-whatdoesit6810580422334939136/)\n\nLooking forward to seeing you there!"}, {"id": "o8ftui", "title": "[D] Training/Inference performance info 1080ti/2080ti/3090", "score": 1, "url": "https://www.reddit.com/r/MachineLearning/comments/o8ftui/d_traininginference_performance_info/", "author": "Ok-Influence368", "subreddit": "r/MachineLearning", "description": "People using any of the above mentioned GPU's for deep learning could you please post your training /inference times on standard networks like resnet50 using the full ILSVRC dataset . Time taken to train a single epoch would be most helpful or Images processed per second, also if you could specify your batch size along with it. \n\nThanks in Advance"}, {"id": "o853fw", "title": "[D]Five very informative machine learning newsletters", "score": 6, "url": "https://www.reddit.com/r/MachineLearning/comments/o853fw/dfive_very_informative_machine_learning/", "author": "akira_AI", "subreddit": "r/MachineLearning", "description": "1\nThe Batch ; https://www.deeplearning.ai/the-batch/\nThis is a newsletter published by DeepLearning.AI, led by Prof. Andrew Ng. It contains about 5 topics such as applications and research, and for each topic there is a commentary by an expert on what's new, why it's important, and their thoughts.\n\n\n2\nDeep Learning Weekly ; https://www.deeplearningweekly.com/p/deep-learning-weekly-issue-203\nAbout 3~6 topics are provided for each theme such as industrial application, edge device relation, research, code, etc. Very large amount of contents.\n\n\n3\nAI Weekly ; https://aiweekly.co/\nAbout 3 topics are provided for each theme such as application examples, ethics, robotics, and research. It is valuable because there are few summaries on ethics.\n\n\n4\nPapers With Code ; https://paperswithcode.com/newsletter\nPapers With Code is a newsletter from Papers With Code that introduces papers (and code), datasets, and trends that are being discussed on GitHub.\n\n\n5\nAkira's Machine Learning News  ; https://www.getrevue.co/profile/akiratosei\nA collection of articles on machine learning, including practical examples, technical articles, and papers. The basic commentary is in a few words, but the weekly hot topics are explained in a bit longer. Mostly in the field of imaging."}, {"id": "o84wg6", "title": "[D] Thieves on Sesame Street! Model Extraction of BERT-based APIs (Research Paper Walkthrough)", "score": 5, "url": "https://www.reddit.com/r/MachineLearning/comments/o84wg6/d_thieves_on_sesame_street_model_extraction_of/", "author": "prakhar21", "subreddit": "r/MachineLearning", "description": "Can we steal/extract a SOTA Machine Learning Model via just the Prediction Apis? and that too with a budget under 500$? \n\nThis paper showcases exactly that by exploiting the models trained via transfer learning methods. Author's also proposes multiple defense strategies like Membership classification and API watermarking to make these models robust to model extraction attacks.\n\nhttps://youtu.be/ueC2a3hlBVs"}, {"id": "o83n1z", "title": "[N] Facebook AI Uses Reverse Engineering Generative Models From A Single Deepfake Image To Study And Detect Deepfake", "score": 8, "url": "https://www.reddit.com/r/MachineLearning/comments/o83n1z/n_facebook_ai_uses_reverse_engineering_generative/", "author": "techsucker", "subreddit": "r/MachineLearning", "description": "Deepfakes have become increasingly convincing over the years. In collaboration with Michigan State University (MSU), Facebook has presented a research method of detecting and attributing Deepfakes based on reverse engineering from a single AI-generated image to the generative model used to produce the image. The technique will allow deepfake detection and tracing in a real-world scenario, where the often the only information available for deepfake detectors is the image itself.\n\nFull Story: [https://www.marktechpost.com/2021/06/25/facebook-ai-uses-reverse-engineering-generative-models-from-a-single-deepfake-image-to-study-and-detect-deepfake/](https://www.marktechpost.com/2021/06/25/facebook-ai-uses-reverse-engineering-generative-models-from-a-single-deepfake-image-to-study-and-detect-deepfake/)"}, {"id": "o8a5kx", "title": "[D] What are the alternatives for PCA analysis for \"direction of data set\"?", "score": 0, "url": "https://www.reddit.com/r/MachineLearning/comments/o8a5kx/d_what_are_the_alternatives_for_pca_analysis_for/", "author": "mavavilj", "subreddit": "r/MachineLearning", "description": "What are the alternatives for PCA analysis for \"direction of data set\"?\n\nSometimes I find that linear model can produce quite similar result, but not always.\n\nIf there's more curvature, then I've also thought about \"PCA on piece-wise data segments\", but is there something else?"}, {"id": "o887ou", "title": "\"[Discussion]\" Online Learning in machine learning", "score": 1, "url": "https://www.reddit.com/r/MachineLearning/comments/o887ou/discussion_online_learning_in_machine_learning/", "author": "GTKdope", "subreddit": "r/MachineLearning", "description": "Can someone suggests books or resources for online learning part in machine learning.\nI want to explore how a model is continuously updated with a data stream, not like the usual dataset.\nAlso I understand how batch learning is different from online learning."}, {"id": "o84w96", "title": "[D] Have you ever seen a good, useful, convincing ML pipeline image / infographic? Is it even possible to create a one for a ML subfield?", "score": 2, "url": "https://www.reddit.com/r/MachineLearning/comments/o84w96/d_have_you_ever_seen_a_good_useful_convincing_ml/", "author": "adenml", "subreddit": "r/MachineLearning", "description": "I think that nothing receives more hate than ML drawings that tell when and how to choose a model, when and how to do certain preprocessing steps. For example, I think we can all agree the scikit learn [https://scikit-learn.org/stable/tutorial/machine\\_learning\\_map/index.html](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html) is quite dumb and useless. Unfortunately, it is also the most famous one\n\nNow, good infographics would be gold for beginners and interns, but I have never seen one. \n\nI am not talking about the entire ML, it would be way to complex for this. I would be extremely happy to see a good pipeline for a ML sub-field like NLP. When should I use LSTMs? When should I use a Char CNN? What about transformers? When should I use SVM + string kernels? When should I use gloVe or TFIDF? What preprocessing steps are necessary? Whan should I stem / lemmatize my data? Should I remove stopwords? Should I remove rare chars? When, why? Should I do manual feature engineering on the texts? Should I convert all to lowercase? \n\nNow, what about you? Have you ever seen a good pipeline (regarding how to choose a model and what preprocessing steps to take) at least for a sub-domain of ML?"}, {"id": "o878nl", "title": "[D] Metaheuristics/Derivative Free Methods: Are any groups working in this?", "score": 0, "url": "https://www.reddit.com/r/MachineLearning/comments/o878nl/d_metaheuristicsderivative_free_methods_are_any/", "author": "Pl4yByNumbers", "subreddit": "r/MachineLearning", "description": "After reading through the excellent Essentials of Metaheuristics, I wondered if there was any conferences or research networks that actually focus on these sorts of methods? EU/ME looks like it\u2019s pretty dead based on its website. Are any of you working with developing these type of methods explicitly, and if so where are you communicating with other researchers?\n\nCheers!"}, {"id": "o7qcsl", "title": "[R] Google Research\u2019s Prediction Depth: Understanding the Laws that Govern DL Data Processing", "score": 16, "url": "https://www.reddit.com/r/MachineLearning/comments/o7qcsl/r_google_researchs_prediction_depth_understanding/", "author": "Yuqing7", "subreddit": "r/MachineLearning", "description": "A team from Google Research proposes prediction depth, a new measure of example difficulty determined from hidden embeddings. Their study reveals the surprising fact that the prediction depth of a given input has strong connections to a model\u2019s uncertainty, confidence, accuracy and speed of learning for that data point. \n\nHere is a quick read: [Google Research\u2019s Prediction Depth: Understanding the Laws that Govern DL Data Processing.](https://syncedreview.com/2021/06/25/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-49/)\n\nThe paper *Deep Learning Through the Lens of Example Difficulty* is on [arXiv](https://arxiv.org/abs/2106.09647)."}, {"id": "o7gcl0", "title": "[R] VOLO: Vision Outlooker for Visual Recognition (87.1% top 1 on Imagenet with no extra data)", "score": 111, "url": "https://www.reddit.com/r/MachineLearning/comments/o7gcl0/r_volo_vision_outlooker_for_visual_recognition/", "author": "m__ke", "subreddit": "r/MachineLearning", "description": "https://arxiv.org/abs/2106.13112\n\nhttps://github.com/sail-sg/volo"}, {"id": "o7pi20", "title": "[P] Search ML papers using ML", "score": 15, "url": "https://www.reddit.com/r/MachineLearning/comments/o7pi20/p_search_ml_papers_using_ml/", "author": "opensourcecolumbus", "subreddit": "r/MachineLearning", "description": "Created a project to search machine learning papers via text or via pdf. One fun use case - upload your CV and see the relevant machine learning papers related to your experience.\n\nHere's the [Demo](https://showcase.jina.ai) built using [Jina](https://github.com/jina-ai/jina/) search framework.\n\nIn short here's what I did - trained deep learning model, indxed embeddings, exposed search via APIs, ranking based on similarity and finally a simple frontend to use it and a complex but cool way to view the papers in 3D.\n\nThe indexed data is limited for now - 10k ML papers. So don't expect too much. Will index more data this week.\n\nLooking for your feedback. What are good sources to find more data? How was your experience trying this demo? What major problems did you notice?\n\nThank you"}, {"id": "o7s2u0", "title": "[D] Tracking ML workshops", "score": 8, "url": "https://www.reddit.com/r/MachineLearning/comments/o7s2u0/d_tracking_ml_workshops/", "author": "Alworldview", "subreddit": "r/MachineLearning", "description": "How do you track workshops in ML conferences related to your area? Is there an efficient way of doing it?"}, {"id": "o813h5", "title": "[R] Machine Learning with Applications journal on Elsevier", "score": 0, "url": "https://www.reddit.com/r/MachineLearning/comments/o813h5/r_machine_learning_with_applications_journal_on/", "author": "AkshatChat28", "subreddit": "r/MachineLearning", "description": "Does anyone have the knowledge or past experience of publishing a research paper on Elsevier (on this Machine Learning with Applications journal).\n\nIt is asking for the manuscript to upload but they didn't provide any templates to follow.\n\nIt's my first time so I could really use some help!"}, {"id": "o7x8f6", "title": "[P][R] Training a voice synthesizer from non-linguistic corpus of audio?", "score": 2, "url": "https://www.reddit.com/r/MachineLearning/comments/o7x8f6/pr_training_a_voice_synthesizer_from/", "author": "Crackgnome", "subreddit": "r/MachineLearning", "description": "Long ago I read one of William Gibson's novels in which he describes a series of characters that mask their voices by generating word-like sounds from non-linguistic sources. In particular, his description of a character who spoke using words generated from the sounds of glass breaking has really stuck with me for many years.\n\nWith the advent of accessible machine learning and voice emulation software, I recently realized this might be possible, but I'm not sure where to start.\n\nMost systems I've found, particularly deep fake voice mimicry, generally require a reference corpus of voice recordings/similar. As such, I currently believe the right first step would be finding a way to algorithmically modify sounds from a (yet-to-be-compiled) database of glass sounds in order to approach an intelligible level of word generation. I don't currently have a preference regarding voice modulation vs text-to-speech, though I'm sure they each have their specific benefits and weaknesses.\n\nSome surface-level googling lead me to this article from 2019 which feels likely to point me in the right direction, though I am too woefully undertrained to know for sure without a significant amount of time invested. Though I am willing to commit an unreasonable amount of work to this (as it is a passion project with no specific timeframe), it just feels prudent to check with folks who know better than I do before I spend months learning a system that may or may not be appropriate for what I'm trying to generate.\n\nDoes anyone have any advice, specific or general, regarding this kind of project? Additionally, if there is a more appropriate place to ask these questions, please don't hesitate to tell me to get lost (preferably with directions to where to go to stop being lost). Don't worry if recommendations are likely to go over my head, I am more than willing to do the foundational research if the process looks promising.\n\ntl;dr I want to train a program to generate speech using the sounds of broken glass as a reference, how do?"}, {"id": "o7y6k2", "title": "[D] MobileNetV3 transfer learning stuck in plateau", "score": 1, "url": "https://www.reddit.com/r/MachineLearning/comments/o7y6k2/d_mobilenetv3_transfer_learning_stuck_in_plateau/", "author": "Fragrant_Percentage3", "subreddit": "r/MachineLearning", "description": "I'm trying to train MobileNetV3-Small from tensorflow 2.5.0 to classify dog breeds (133) and since it achieved about 60-70% accuracy on ImageNet (1000 classes), I figured it would work well for my use case, but it has been quite difficult to train.\n\nAfter trying a number of different variations, I learned that Adam with 1e-5 learning rate allowed the network progress and now it has about 60% validation accuracy after about 100 epochs, but it hasn't improved much further. I've tried to use ReduceLROnPlateau but haven't achieved any better results. I was using patience of 5 epochs but I don't know if I made the right choice?\n\nI made a post on [SO](https://stackoverflow.com/questions/68131274/transfer-learning-on-mobilenetv3-reaches-plateau-and-i-cant-move-past-it) showing the model architecture and loss curves, which are separate because I needed to reload weights due to timeout issues with Google Colab. If anyone can help me troubleshoot this issue and provide some suggestions, that would be very much appreciated =)"}, {"id": "o7wvm7", "title": "[D] Are there any references for this type of meta-algorithm?", "score": 1, "url": "https://www.reddit.com/r/MachineLearning/comments/o7wvm7/d_are_there_any_references_for_this_type_of/", "author": "eeaxoe", "subreddit": "r/MachineLearning", "description": "Say you have a prediction algorithm for the risk of customer churn, for example. You use this algorithm to target the top x% riskiest customers. But you suppose instead that it may be more optimal to target the riskiest customers who also have the highest expected lifetime values, so you build a model to predict E[LTV]. The question is how you would chain these two models together (just multiply the outputs?), and how you would evaluate the policies that ensue.  \n\nOr perhaps there's a way to learn the initial prediction algorithm for churn that incorporates LTV into the loss function, so you don't have to learn two models separately. But my guess is that this isn't strictly always better than the first approach.\n\nAre there any papers that have already looked into what I'm thinking of? I've been doing lit review on and off on this for a while, but all I've managed to turn up are papers on ensembling which is not what I'm after."}, {"id": "o7o13g", "title": "[D] Best Bayesian Optimization Library in R?", "score": 4, "url": "https://www.reddit.com/r/MachineLearning/comments/o7o13g/d_best_bayesian_optimization_library_in_r/", "author": "thisisthehappylion", "subreddit": "r/MachineLearning", "description": "I'm looking for an R-library to optimize any multivariate objective function with Bayesian Optimization (BO). In python, I usually use Optuna ([https://optuna.org/](https://optuna.org/)) for BO. Do you have any recommendations for equivalent libraries in R?"}, {"id": "o7j8rk", "title": "[R] Video Swin Transformer: SOTA on Video Recognition (84.9% top 1 on Kinetics-400 and 69.6% top 1 on Something-Something V2)", "score": 7, "url": "https://www.reddit.com/r/MachineLearning/comments/o7j8rk/r_video_swin_transformer_sota_on_video/", "author": "yuecao", "subreddit": "r/MachineLearning", "description": "[https://arxiv.org/abs/2106.13230](https://arxiv.org/abs/2106.13230)\n\n[https://github.com/SwinTransformer/Video-Swin-Transformer](https://github.com/SwinTransformer/Video-Swin-Transformer)\n\n[https://paperswithcode.com/paper/video-swin-transformer](https://paperswithcode.com/paper/video-swin-transformer)"}, {"id": "o6wggh", "title": "[R] Finally, Actual Real images editing using StyleGAN", "score": 334, "url": "https://www.reddit.com/r/MachineLearning/comments/o6wggh/r_finally_actual_real_images_editing_using/", "author": "Graphics4Life", "subreddit": "r/MachineLearning", "description": "In the last years I have been interested in different technologies that enable facial editing. One of the promising directions was editing faces using StyleGAN. Nevertheless, each method that came up while succeeding in editing a small number of celebrities, always failed to edit my face and many of the faces I wanted to edit. I assumed the problem was with an inherent bias inside StyleGAN and decided to wait for its third version which just came up! See [https://nvlabs.github.io/alias-free-gan](https://nvlabs.github.io/alias-free-gan). So excited about the new opportunities it will bring to the world of graphics and editing. In the meanwhile, a very interesting paper called \u201cPivotal Tuning for Latent-based editing of Real Images\u201d was released. With many papers stating that they can edit real images, I was not much optimistic about this paper as well. But boy was I wrong. For the first time, I could actually edit facial images using StyleGAN! The authors provide an inference notebook which I used to edit 2 Machine Learning legends. See the results by yourself\u2026\n\nThe notebook: [https://colab.research.google.com/github/danielroich/PTI/blob/main/notebooks/inference\\_playground.ipynb](https://colab.research.google.com/github/danielroich/PTI/blob/main/notebooks/inference_playground.ipynb)\n\nThe github repository: [https://github.com/danielroich/PTI](https://github.com/danielroich/PTI)\n\nWhat do you think, Will this paper and the advances in the field will affect our lives? (Hollywood, DeepFake, etc) So much potential\n\n&#x200B;\n\n&#x200B;\n\n[Younger](https://preview.redd.it/fdtndqla96771.jpg?width=1024&format=pjpg&auto=webp&s=3bcd679eea8c775a9313c339891c661b562f908b)\n\n[Original Image](https://preview.redd.it/hn4rkqla96771.jpg?width=1024&format=pjpg&auto=webp&s=96a0c2568145779879601a9754a50335dd60b0ca)\n\n[Smiling](https://preview.redd.it/60qjkqla96771.jpg?width=1024&format=pjpg&auto=webp&s=84382d9b1f79dabed63017979a534634a3e5362b)\n\n&#x200B;\n\n[Younger](https://preview.redd.it/35s7l8xe96771.jpg?width=1024&format=pjpg&auto=webp&s=d56d0f26ff5596c34ac4a20a8726bf92a68e831d)\n\n[Original Image](https://preview.redd.it/ypdxr9xe96771.jpg?width=1024&format=pjpg&auto=webp&s=897bee31c23cd0944e0155f7f7f5f6bf64f468ec)\n\n[Rotation](https://preview.redd.it/jlfslgxe96771.jpg?width=1024&format=pjpg&auto=webp&s=2179ba199da6e53b5cb5c1995679547c350feca2)"}, {"id": "o7eq2j", "title": "[D] Text-to-Text transformer with memory", "score": 6, "url": "https://www.reddit.com/r/MachineLearning/comments/o7eq2j/d_texttotext_transformer_with_memory/", "author": "HStuart18", "subreddit": "r/MachineLearning", "description": "Hey guys,\n\nI followed this tutorial: [https://blog.tensorflow.org/2019/05/transformer-chatbot-tutorial-with-tensorflow-2.html](https://blog.tensorflow.org/2019/05/transformer-chatbot-tutorial-with-tensorflow-2.html) which is basically a text-to-text transformer trained on a movie dialogue dataset.\n\nHowever, I have a slight different use case, I have a log of \"**customer** vs **customer service agent**\" text conversation logs. So I am not interested in generating  a **customer** response from a **customer service agent** response, I am only interested in generating a **customer service agent** response from a **customer** response. Yes, I want to try with transformers and yes that means not explicitly parsing user intent etc. Additionally, **customer service agent** responses and **customer** responses $<i$ should influence how the model generates **customer service agent** response $i$.\n\nThe dataset contains 500 conversations and each conversation has on average 1000 interactions (ik 1000 is a lot but thats coz my dataset isnt actually customer service related but it's very analogous). The model should probably know not to correlate any temporal information between different conversations.\n\nIf anybody has any ideas or could please point me to some resources/architectures that would be greatly appreciated.\n\nThanks!"}, {"id": "o7mb83", "title": "[D] Which metric for error bars?", "score": 0, "url": "https://www.reddit.com/r/MachineLearning/comments/o7mb83/d_which_metric_for_error_bars/", "author": "Metatronx", "subreddit": "r/MachineLearning", "description": "Hey guys I have a question regarding error bars in plots.\n\nLets say i am repeating the experiment multiple times with random initialization. And I would like to report the mean but also show the variance across the repeated experiments through error bars.\n\nWhich metric would I use to define the width of the error bars.\n\nStandard Deviation?\n\nStandard Error?\n\nCofidence Interval?"}, {"id": "o6ztc3", "title": "[Research][Project] How to read more research papers?", "score": 59, "url": "https://www.reddit.com/r/MachineLearning/comments/o6ztc3/researchproject_how_to_read_more_research_papers/", "author": "OnlyProggingForFun", "subreddit": "r/MachineLearning", "description": "In this article, I am sharing the best tips and practical tools I use daily to simplify my life as a research scientist to be more efficient when looking for interesting research papers and reading them \n\n[https://www.louisbouchard.ai/research-papers/](https://www.louisbouchard.ai/research-papers/)\n\nPlease, let me know if you use any other tools that I did not mention in my article that could be of great addition.\n\nQuick summary of the tools discussed:\n\n* [42 Papers](https://42papers.com/)\n* [Arxiv Sanity Preserver](http://www.arxiv-sanity.com/)\n* [Papers With Code](https://paperswithcode.com/)\n* [Crossmind](https://crossminds.ai/video/swin-transformer-hierarchical-vision-transformer-using-shifted-windows-606d0de375292b321dd08f80/)\n* [CatalyzeX](https://www.catalyzex.com/)\n* [Yannic Kilcher](https://www.youtube.com/channel/UCZHmQk67mSJgfCCTn7xBfew)\n* [What\u2019s AI](https://www.youtube.com/channel/UCUzGQrN-lyyc0BWTYoJM_Sg)\n* [Letitia](https://www.youtube.com/channel/UCobqgqE4i5Kf7wrxRxhToQA)\n* [Two Minute Papers](https://www.youtube.com/user/keeroyz)"}, {"id": "o7kx8z", "title": "[D] Is CIFAR-100 supplied with Keras broken? No matter the model I cannot get above 67% test accuracy.", "score": 1, "url": "https://www.reddit.com/r/MachineLearning/comments/o7kx8z/d_is_cifar100_supplied_with_keras_broken_no/", "author": "Avelina9X", "subreddit": "r/MachineLearning", "description": "I've tested a million and one different models, different training scripts, different amounts of every type of regularization or augmentation, but no matter what I do I can't beat this 67% value.\n\nIn my latest experiment I decided to go back to basics and use a completely pre-built model with no modification: I've been training ResNet50V2 on CIFAR-100 using AdaBelief (which has been shown to converge for CIFAR-100 with speeds of Adam and performance of SGD) and also using mixup augmentation with 4x training examples and it still can't get above 67% test accuracy, hell it cant get above 40% accuracy, nor can DenseNet, and MobileNet can't even get over 1% accuracy. I'm literally just feeding these models to the \\`.fit\\` function, no custom training loops, nothing. To me this suggests that CIFAR-100 in Keras somehow has a fucked up test set.\n\nI'm losing my mind here. I am questioning my life choices because either Keras is wrong, or my entire understanding of machine learning is wrong. So I need to know which one of us fucked up before I decide to drop out of my machine learning PhD."}, {"id": "o7cb2d", "title": "[D] EvalAI: An Open-Source Alternative To Kaggle", "score": 4, "url": "https://www.reddit.com/r/MachineLearning/comments/o7cb2d/d_evalai_an_opensource_alternative_to_kaggle/", "author": "SubstantialRange", "subreddit": "r/MachineLearning", "description": "[https://eval.ai/](https://eval.ai/)"}, {"id": "o7epll", "title": "[R][ICRA 2021] Reducing the Deployment-Time Inference Control Costs of Deep Reinforcement Learning Agents via an Asymmetric Architecture", "score": 2, "url": "https://www.reddit.com/r/MachineLearning/comments/o7epll/ricra_2021_reducing_the_deploymenttime_inference/", "author": "ElsaLab", "subreddit": "r/MachineLearning", "description": "We proposed a methodology of DRL framework for performing cost-aware control based on an asymmetric architecture. Our methodology uses a master policy to select a small sub-policy to act when conditions are acceptable while employing a large one when necessary. [https://reurl.cc/MAOyxK](https://reurl.cc/MAOyxK?fbclid=IwAR3ExUSpXEjJ5xuncVeMrFRyTdlQKf-XaZiOyb_BgqhT0AiX5MF8VtxSx_8)\n\nAdvanced detail please visit: [https://reurl.cc/VEz4nb](https://reurl.cc/VEz4nb?fbclid=IwAR2Pq5fn6ZqrP84sGEXG8oUI1i8lKX7TpUfc6n0zswSvb90zksQsjXSNxGk)\n\nPaper Download: [https://reurl.cc/XWGmpa](https://reurl.cc/XWGmpa?fbclid=IwAR39v3HfNVgbuYfD9IhTNPL1zyq_cO6A68s69p0SENScpCwuASen828Pz8E)\n\nGithub Link: [https://reurl.cc/2rYE3m](https://reurl.cc/2rYE3m?fbclid=IwAR13xPHsuQAk2Fk5EYgFilU9CB4yBB5XRercQ-9fOZqqDZ_F92gaPHwWt5U)\n\nPresentation Link: [https://reurl.cc/xGa6o5](https://reurl.cc/xGa6o5?fbclid=IwAR0HRB-IXUc1dvgzL_NqHCD7-C_aK8G5xLjbIlWg4WJTPQlMwKXRr50kNFo)\n\nELSA Lab is a research laboratory focusing on Deep Reinforcement Learning, Intelligent Robotics, and Computer Vision. Please visit our website: [https://elsalab.ai/](https://elsalab.ai/?fbclid=IwAR393j_cpYChimDD3oONreaNnBNLZy20BThbSOb2ZAvotWLZUR4SbIIVvlY)\n\n[#ArtificialIntelligence](https://www.facebook.com/hashtag/artificialintelligence?__eep__=6&__cft__[0]=AZW22s3pu1WHCJkaYxqtMpl7sVFGLO-TDrInr6U5HB8fsEXMosNrLk9aoj7Iek-RK2Y4YBh1CgQazrKs3u5fpwVn2851ci7iJ084yEv2sSXsiHoKN83b0W3LskAhSGOU3NGWCs3hzra1NVXW1AVZUj8cbOIdINtSX8HLMvPYhg0VIUFVN2cyzeCBjNTx5GIRQuw&__tn__=*NK-R) [#ReinforcementLearning](https://www.facebook.com/hashtag/reinforcementlearning?__eep__=6&__cft__[0]=AZW22s3pu1WHCJkaYxqtMpl7sVFGLO-TDrInr6U5HB8fsEXMosNrLk9aoj7Iek-RK2Y4YBh1CgQazrKs3u5fpwVn2851ci7iJ084yEv2sSXsiHoKN83b0W3LskAhSGOU3NGWCs3hzra1NVXW1AVZUj8cbOIdINtSX8HLMvPYhg0VIUFVN2cyzeCBjNTx5GIRQuw&__tn__=*NK-R) [#MachineLearning](https://www.facebook.com/hashtag/machinelearning?__eep__=6&__cft__[0]=AZW22s3pu1WHCJkaYxqtMpl7sVFGLO-TDrInr6U5HB8fsEXMosNrLk9aoj7Iek-RK2Y4YBh1CgQazrKs3u5fpwVn2851ci7iJ084yEv2sSXsiHoKN83b0W3LskAhSGOU3NGWCs3hzra1NVXW1AVZUj8cbOIdINtSX8HLMvPYhg0VIUFVN2cyzeCBjNTx5GIRQuw&__tn__=*NK-R) [#DeepLearning](https://www.facebook.com/hashtag/deeplearning?__eep__=6&__cft__[0]=AZW22s3pu1WHCJkaYxqtMpl7sVFGLO-TDrInr6U5HB8fsEXMosNrLk9aoj7Iek-RK2Y4YBh1CgQazrKs3u5fpwVn2851ci7iJ084yEv2sSXsiHoKN83b0W3LskAhSGOU3NGWCs3hzra1NVXW1AVZUj8cbOIdINtSX8HLMvPYhg0VIUFVN2cyzeCBjNTx5GIRQuw&__tn__=*NK-R)"}, {"id": "o7b3ch", "title": "Toward Knowledge Discovery Framework for Data Science Job Market in the United States", "score": 4, "url": "https://arxiv.org/abs/2106.11077", "author": "kk7nc", "subreddit": "r/MachineLearning", "description": ""}, {"id": "o7b38d", "title": "Possible applications of Document detection/segmentation in images/photos [D]", "score": 4, "url": "https://www.reddit.com/r/MachineLearning/comments/o7b38d/possible_applications_of_document/", "author": "testerpce", "subreddit": "r/MachineLearning", "description": "So after you take pictures of documents or books or billboards and stuff what are the possible applications of Document or just text detection ? I am thought about OCR, Few shot Document classification,  image text separation in documents etc. But I cannot think of anything else for now.\nCan you guys think of any possible applications after documents detection/segmentation in images ?\n[D]"}, {"id": "o6vj1p", "title": "[R] Revisiting Deep Learning Models for Tabular Data", "score": 53, "url": "https://www.reddit.com/r/MachineLearning/comments/o6vj1p/r_revisiting_deep_learning_models_for_tabular_data/", "author": "StrausMG", "subreddit": "r/MachineLearning", "description": "Hi! We introduce our new paper \"Revisiting Deep Learning Models for Tabular Data\" and the \"rtdl\" package that enables easy access to the main models from the paper.\n\nPaper: [https://arxiv.org/abs/2106.11959](https://arxiv.org/abs/2106.11959)  \nCode:  [https://github.com/yandex-research/rtdl](https://github.com/yandex-research/rtdl)\n\n[FT-Transformer](https://preview.redd.it/sn941byiu5771.png?width=2984&format=png&auto=webp&s=f1279cb072dd503c3ff9ff84ad667ef7c37e0f6d)\n\nTL;DR:  \n\\- we show that two simple architectures can serve as strong baselines for Tabular Deep Learning: (1) a ResNet-like architecture and (2) FT-Transformer - an adaptation of the Transformer architecture for tabular data  \n\\- the problems where Gradient Boosting dominates should be prioritized when developing DL solutions targeted at beating Gradient Boosting"}, {"id": "o7gh2c", "title": "[R] What is Laplacian noise caused by?", "score": 3, "url": "https://www.reddit.com/r/MachineLearning/comments/o7gh2c/r_what_is_laplacian_noise_caused_by/", "author": "BoraaaBoraaa", "subreddit": "r/MachineLearning", "description": "Gaussian noise can be caused by high temperatures and the amount of light present, what about laplacian noise?"}, {"id": "o7qoq1", "title": "[D] FB Prophet, to use or not to use?", "score": 0, "url": "https://www.reddit.com/r/MachineLearning/comments/o7qoq1/d_fb_prophet_to_use_or_not_to_use/", "author": "jobandjethro", "subreddit": "r/MachineLearning", "description": "Hey team! I'm currently reviewing some projects at work and need some articles on FB prophet. Do you guys have any articles or resources I can use for their API?"}, {"id": "o6txxi", "title": "[N] Facebook AI Releases \u2018HuBERT\u2019: A New Approach For Learning Self-Supervised Speech Representations", "score": 71, "url": "https://www.reddit.com/r/MachineLearning/comments/o6txxi/n_facebook_ai_releases_hubert_a_new_approach_for/", "author": "ai-lover", "subreddit": "r/MachineLearning", "description": "Many AI research projects have been striving to improve their ability to detect and interpret speech merely by listening and engaging with others, much like babies learn their first language. This needs not just assessing what someone says but also a variety of other clues from how those words are delivered, such as speaker identification, emotion, hesitation, and interruptions. Furthermore, the AI system must recognize and interpret noises that overlap with the speech signal, such as laughter, coughing, background vehicles, or bird tweeting, to fully comprehend a situation as a person would do.\n\nArticle: [https://www.marktechpost.com/2021/06/23/facebook-ai-releases-hubert-a-new-approach-for-learning-self-supervised-speech-representations/](https://www.marktechpost.com/2021/06/23/facebook-ai-releases-hubert-a-new-approach-for-learning-self-supervised-speech-representations/)? \n\nPaper: https://arxiv.org/pdf/2106.07447.pdf\n\nGithub: [https://github.com/pytorch/fairseq/tree/master/examples/hubert](https://github.com/pytorch/fairseq/tree/master/examples/hubert)\n\nFB Blog: https://ai.facebook.com/blog/hubert-self-supervised-representation-learning-for-speech-recognition-generation-and-compression"}, {"id": "o76r70", "title": "[D] Research Engineers: how do you stop taking crap from Research Scientists?", "score": 5, "url": "https://www.reddit.com/r/MachineLearning/comments/o76r70/d_research_engineers_how_do_you_stop_taking_crap/", "author": "powerforward1", "subreddit": "r/MachineLearning", "description": "I see a lot of places where phd-heavy research scientists in charge and are basically the managers of non-phd research engineers.  Issue is they are not good managers and are not practical, leading to a lot of wasted time. \n\nAnother issue I see is that some research scientists refuse to involve you in the project until the last step, whether it's because it's b/c you don't have a phd, et. Thus it becomes a shit scramble to productionize the project.\n\nFurthermore, I've seen instances where the RE becomes just central dev or IT support.\n\nHow do you push back against Research Scientists who do this stuff? \n\nWhat about in the job search? How do you sniff them out?"}, {"id": "o7d9yr", "title": "[D] Isolation Forests: Do you need any information about the training set?", "score": 1, "url": "https://www.reddit.com/r/MachineLearning/comments/o7d9yr/d_isolation_forests_do_you_need_any_information/", "author": "BunchOfEnergy", "subreddit": "r/MachineLearning", "description": " In order to train an OC-SVM for example, you need a dataset with points that represent the \"regular\" or \"normal\" behavior.\n\nAs far as i understand, thats not true for iForests. You can build the iTrees - whether you have outliers in your initial dataset or not.\n\nAm i missing something?"}, {"id": "o7bai8", "title": "[D] Computer vision models for two inputs", "score": 1, "url": "https://www.reddit.com/r/MachineLearning/comments/o7bai8/d_computer_vision_models_for_two_inputs/", "author": "Lilx20", "subreddit": "r/MachineLearning", "description": "\nHello,\nI am looking a model that allows me to look for differences in a set of images that are very similar but are not the same.\n\nSay two images of an actress. In one picture her eyebrows are different than the other. So the network highlights the eyebrows.\n\nDo you know of any?? Thanks !"}, {"id": "o6u34b", "title": "[R] The Dimpled Manifold Model of Adversarial Examples in Machine Learning", "score": 23, "url": "https://www.reddit.com/r/MachineLearning/comments/o6u34b/r_the_dimpled_manifold_model_of_adversarial/", "author": "ivanevti", "subreddit": "r/MachineLearning", "description": "Very, very interesting new work by Adi Shamir et al: [https://arxiv.org/abs/2106.10151](https://arxiv.org/abs/2106.10151)\n\nIt proposes a new mental model for why adversarial examples exist. The central claim is that adversarial examples come from the fact that we fit high-dimensional decision boundaries to low-dimensional images. This leaves a lot of space for the adversarial examples to exist perpendicularly from the true location of the low-dimensional object (the natural image). \n\nMore precisely: the decision boundary is like a thin, horizontal sheet of metal that is being bent up and down to fit clusters of training examples. This creates dimples in the sheet around where the training examples lie. The sheet can be thought of as the space in which natural images exist; the dimples go into the extra dimensions because we represent those natural images in a very high-dimensional form (RGB images).\n\nAdversarial examples? Well, these are just unnatural images above and below the sheet , i.e. off the manifold of natural image.\n\nRobustness accuracy trade-off? This just comes from the fact that you have to bend the sheet way more out of shape than normal to fit adversarial examples, so you end up missing the details/small clusters.\n\nBeing able to fit a model with good clean test-set performance by training on adversarial examples with target labels? That comes from the fact that you end up recreating the manifold by moving around the labels.\n\nHumans being insensitive to adversarial examples? They have just learned to do projections into the low-dimensional space, so anything that lives above/below the dimple gets projected onto the dimple.\n\nThis mental model seems to provide a lot of compelling explanations and there is good experimental work. What do you think about it?"}, {"id": "o6ce13", "title": "[D] Machine Learning Interview book by Huyen Chip.", "score": 404, "url": "https://www.reddit.com/r/MachineLearning/comments/o6ce13/d_machine_learning_interview_book_by_huyen_chip/", "author": "lkhphuc", "subreddit": "r/MachineLearning", "description": "[https://huyenchip.com/ml-interviews-book/](https://huyenchip.com/ml-interviews-book/)\n\nI have just skimmed part of the book but it looks very good and contains lots of insight from a recruiter point of view that I would never know otherwise and is applicable to more than just ML interview IMO. What do you think?\n\nQuote from the Github page:\n\nThis book is the result of the collective wisdom of many people who  have sat on both sides of the table and who have spent a lot of time  thinking about the hiring process. It was written with candidates in  mind, but hiring managers who saw the early drafts told me that they  found it helpful to learn how other companies are hiring, and to rethink  their own process.\n\nThe book consists of two parts. The first part provides an overview  of the machine learning interview process, what types of machine  learning roles are available, what skills each role requires, what kinds  of questions are often asked, and how to prepare for them. This part  also explains the interviewers\u2019 mindset and what kind of signals they  look for.\n\nThe second part consists of over 200 knowledge questions, each noted  with its level of difficulty -- interviews for more senior roles should  expect harder questions -- that cover important concepts and common  misconceptions in machine learning."}, {"id": "o6j8qf", "title": "[R] Alias-Free GAN", "score": 139, "url": "https://www.reddit.com/r/MachineLearning/comments/o6j8qf/r_aliasfree_gan/", "author": "egnehots", "subreddit": "r/MachineLearning", "description": "[https://nvlabs.github.io/alias-free-gan/](https://nvlabs.github.io/alias-free-gan/)\n\n\nAbstract:\n\nWe observe that despite their hierarchical convolutional nature, the synthesis process of typical generative adversarial networks depends on absolute pixel coordinates in an unhealthy manner. This manifests itself as, e.g., detail appearing to be glued to image coordinates instead of the surfaces of depicted objects. We trace the root cause to careless signal processing that causes aliasing in the generator network. Interpreting all signals in the network as continuous, we derive generally applicable, small architectural changes that guarantee that unwanted information cannot leak into the hierarchical synthesis process. The resulting networks match the FID of StyleGAN2 but differ dramatically in their internal representations, and they are fully equivariant to translation and rotation even at subpixel scales. Our results pave the way for generative models better suited for video and animation.\n\n\npaper: \n[https://nvlabs-fi-cdn.nvidia.com/alias-free-gan/alias-free-gan-paper.pdf](https://nvlabs-fi-cdn.nvidia.com/alias-free-gan/alias-free-gan-paper.pdf)"}, {"id": "o6zc4f", "title": "[D] What do you think of \"Boundary Matching Networks\" - Trimming videos on actions", "score": 4, "url": "https://www.reddit.com/r/MachineLearning/comments/o6zc4f/d_what_do_you_think_of_boundary_matching_networks/", "author": "FreddyShrimp", "subreddit": "r/MachineLearning", "description": "I have been looking *\"Temporal Action Localization\".* These are methods that automatically trim longer videos into smaller videos, where the smaller video solely contains a specific action that happens. Additionally, they can also provide a class label for the action that is happening in the trimmed video.  \n\nExample: A 1 minute video of a driveway filled with snow, where at some point a person starts to shovel snow for 5 seconds, gets trimmed to just the 5 second video where a person is shoveling snow.\n\n&#x200B;\n\nWhile looking at all the different methods  available, I saw that the current \"most popular\" one already dates back from 2019. Namely: [Boundary Matching Network](https://paperswithcode.com/paper/bmn-boundary-matching-network-for-temporal), I also came across some other (more recent) methods like [MUSES](https://github.com/xlliu7/MUSES) and [Temporal Context Aggregation Network](https://github.com/qingzhiwu/Temporal-Context-Aggregation-Network-Pytorch) (TCAN). \n\n&#x200B;\n\nWhen I'm looking at these methods, they all seem fairly complex (I do not mean that I cannot understand their workings, I mean that they generally require a complex architecture). TCAN claims that it only needs 201ms to process a 9 **minute** video clip (on a 1080Ti). This sounds pretty fast to me! \n\n&#x200B;\n\nWhat I would like to have some input on:\n\n\\- Are there any other methods that I should consider for temporal action localization\n\n\\- Does anybody have experience with using this on fine-grained actions? Example: Actions within a domain -- A soccer player running on the field should not be trimmed, but a soccer player performing a tackle **or** a soccer player shooting the ball should be trimmed. (AFAIK this all just depends on how we label the training data of what actions are important). \n\n\\- I couldn't find any Google/AWS/IBM APIs that allow you to do a task like this, does anybody know whether they do exist, or are my findings correct?"}, {"id": "o748gs", "title": "Feasibility of building a reliable gun detection model [Discussion]", "score": 2, "url": "https://www.reddit.com/r/MachineLearning/comments/o748gs/feasibility_of_building_a_reliable_gun_detection/", "author": "super_possum", "subreddit": "r/MachineLearning", "description": "I'm trying to understand the barriers to building a computer vision model for detecting a gun in a person's hand in real time. I've played with a dozen or so repos for gun detection, and they all share the ability to detect a gun held directly in front of the camera (ideally in profile), however they perform poorly when tested with a typical security camera setup.\n\nIs building a reliable model to detect a \"gun in hand\" possible and how would you approach it?\n\nWould you optimize training data for a consistent camera angle and distance (ie 45 degrees and 20')? Would you include pose detection?  \nFinally, is there a framework that lends itself best for this application (ie YOLO v4)?  \n\n\n&#x200B;\n\nhttps://preview.redd.it/gisld0z8o8771.png?width=1615&format=png&auto=webp&s=11236706372b83bacab8020bb4238944e324d746"}, {"id": "o7489r", "title": "[D] Fuzzy Learning Vector Quantization for Classification", "score": 2, "url": "https://www.reddit.com/r/MachineLearning/comments/o7489r/d_fuzzy_learning_vector_quantization_for/", "author": "chikenkatsu", "subreddit": "r/MachineLearning", "description": "Hello,\n\nI'm in the middle of my final project creating classification model using Fuzzy Learning Vector Quantization [(Karayiannis, 1997)](https://www.researchgate.net/publication/3335719_An_integrated_approach_to_fuzzy_learning_vector_quantization_and_fuzzy_c-means_clustering). But i'm having a problem regarding turning it into a classification. Karayiannis created the method for clustering, but some people have been using it as a classification method by measuring the euclidean distance between test data and the final cluster which generated from the training phase.\n\nThe problem is, i couldn't find the perfect value for the initial fuzzy number (mi) and final fuzzy number (mf). Also, everytime i changed the initial cluster for the first iteration, the accuracy seems to change, but never reached past the point of 0.40 (40% accuracy). I couldn't seems to find the reason between low accuracy result.\n\nIf anyone could help me understanding the issue would be nice! I've been using human activities recognition dataset from UCI (UCI-HAR, available at kaggle). Adding PCA with n\\_component is 200."}, {"id": "o7ax6a", "title": "[D] The Usage of cutting edge ML within individual teams in big tech.", "score": 0, "url": "https://www.reddit.com/r/MachineLearning/comments/o7ax6a/d_the_usage_of_cutting_edge_ml_within_individual/", "author": "neonglint", "subreddit": "r/MachineLearning", "description": "When you talk about the big tech, everyone is like, \"They use the super cutting edge ML pipelines and methodologies\"\n\nBut the question here is, how many teams within those companies actually leverage those \"cutting edge methods\"?\n\nIn my opinion the ratio of \n\"Teams leveraging modern ml practices : those still trying to hop on the newer ML bandwagon\"  is about 2:10 or 4:10?\n\nAny thoughts?"}, {"id": "o6x88y", "title": "[D] Deep clustering survery", "score": 6, "url": "https://www.reddit.com/r/MachineLearning/comments/o6x88y/d_deep_clustering_survery/", "author": "MoreGPUVeryNice", "subreddit": "r/MachineLearning", "description": " Hi!\n\n<new here>\n\nI am performing image (deep) clustering survey and I am trying to pinpoint impactful papers from recent years.\n\nI am aware of the great survey [here](https://github.com/zhoushengisnoob/DeepClustering) but I have a hard time discriminating which paper is impactful and which isn't. Checking number of citations is helping, but I was hoping the community here could pinpoint me to important papers from the field.\n\n&#x200B;\n\nThanks :)"}, {"id": "o6xwtr", "title": "[P] Live Demo: Analyze Product Hunt Data Using Plain English Questions (NL to SQL)", "score": 6, "url": "https://www.reddit.com/r/MachineLearning/comments/o6xwtr/p_live_demo_analyze_product_hunt_data_using_plain/", "author": "till-veezoo", "subreddit": "r/MachineLearning", "description": "Hello everyone\n\nAs part of our Product Hunt launch we just released a live demo that lets you explore and analyze Product Hunt data using plain English questions - which get automatically translated into SQL using a Semantic Parser that we've built.\n\nYou could ask for instance:\n\n* What were the top 10 posts in Crypto this year\n* What's the average number of votes for Analytic posts?\n* Which weekday has the most posts?\n* Show me the top 10 Hunters in Analytics\n\nLive Demo: [https://app.veezoo.com/chat?id=veezoo-producthunt&demo=true](https://app.veezoo.com/chat?id=veezoo-producthunt&demo=true)\n\nProduct Hunt post: [https://www.producthunt.com/posts/veezoo](https://www.producthunt.com/posts/veezoo)\n\n&#x200B;\n\nLooking forward to hearing your feedback!"}, {"id": "o71xr9", "title": "[R] Distilling the Knowledge from Normalizing Flows", "score": 2, "url": "https://www.reddit.com/r/MachineLearning/comments/o71xr9/r_distilling_the_knowledge_from_normalizing_flows/", "author": "dbaranchuk", "subreddit": "r/MachineLearning", "description": "[Paper](https://openreview.net/pdf?id=fEPhiuZS9TV) (ICML workshop INNF+2021)\n\n[Code](https://github.com/yandex-research/distill-nf)\n\nConditional normalizing flows demonstrate competitive performance in several vision and speech synthesis tasks. In contrast to other generative models, normalizing flows are latent variable models with tractable marginal likelihoods and stable training. However, these benefits usually come at the cost of inefficient model architectures, compared to feed-forward alternatives (e.g. VAEs, GANs).\n\nThis work allows us to significantly speed up the inference of normalizing flows by transferring knowledge from flow-based models to efficient feed-forward architectures.\n\n[Overall scheme of the proposed knowledge distillation approach.](https://preview.redd.it/8ohgh2vqd7771.png?width=1331&format=png&auto=webp&s=86d22f1b0f121677b9d53387cb2fc5bef32c1e6d)"}, {"id": "o75vpu", "title": "[D] Classification of Natural Language Search vs Keyword Search", "score": 1, "url": "https://www.reddit.com/r/MachineLearning/comments/o75vpu/d_classification_of_natural_language_search_vs/", "author": "ml_head", "subreddit": "r/MachineLearning", "description": "For search engines, in the past Microsoft was recommending users to continue using keyword search:\n\n[https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/QvQ\\_MSRTR.pdf](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/QvQ_MSRTR.pdf)\n\nHowever, end users didn't get the message, and they like searching using natural language, also known as semantic search. The truth is that technology has improved and maybe we can return better results using natural language queries (NLQ). Yet, some people still prefer using keyword queries (KQ).\n\n[https://lucidworks.com/post/what-is-natural-language-search/](https://lucidworks.com/post/what-is-natural-language-search/)\n\nIf you have 2 different internal search engines, one optimized for KQ and another optimized for NLQ. How do I inspect the query received and make a decision about what internal search engine to send the query to? Obviously, if you send a KQ to an NLQ-optimized search engine you will get bad results. The same will happen if you send an NLQ to an KQ-optimized search engine. Therefore, an query-type detector is necessary. However, I haven't found any relevant information about solving this problem.\n\nDo you know any prior work on it? If now, how to you suggest approaching it?"}, {"id": "o71awl", "title": "[D] Transformer model which predicts probability distribution", "score": 3, "url": "https://www.reddit.com/r/MachineLearning/comments/o71awl/d_transformer_model_which_predicts_probability/", "author": "backspacejan", "subreddit": "r/MachineLearning", "description": "Hi everyone,\n\nI'm working on a problem, where  I need to predict human activities in a time window. \n\n I have a context, which is a sequence of human activities (and objects) and I want to predict the human activities in the next 20 seconds. The prediction should be a distribution like (0.2, 0.6, 0.1, 0.05, ...) which can be interpretated as: The predicted time window consists of activity A (20\\*0.2) 4 seconds, activity B (20\\*0.6) 12 seconds, etc.\n\nSo far, I have fed the activities in the form of word embeddings into various neural networks (e.g. LSTMs) and used Softmax to give me probabilities. That has worked well so far.\n\nI was now wondering if there are pre-trained transformer models that I can use to achieve something similar. I have already found multi-label classification models, but these always have a target vector consisting of 0 or 1 per class and are not able to predict a distribution like I want.\n\nI am grateful for any advice!"}, {"id": "o7092t", "title": "[P] Advanced Python + NLP introduction course", "score": 3, "url": "https://www.reddit.com/r/MachineLearning/comments/o7092t/p_advanced_python_nlp_introduction_course/", "author": "juditacs", "subreddit": "r/MachineLearning", "description": "We are publicly releasing all class material for our **Advanced Python + NLP introduction** course at the Budapest University of Technology and Economics.\n\nFeel free to share it. All feedback is welcome.\n\n[Course page on Github](https://github.com/bmeaut/python_nlp_2021_spring)"}, {"id": "o6hkt8", "title": "[P] YOLOR (Scaled-YOLOv4-based): The best speed/accuracy ratio for Waymo autonomous driving challenge", "score": 339, "url": "https://arxiv.org/abs/2106.08713", "author": "AlexeyAB", "subreddit": "r/MachineLearning", "description": ""}, {"id": "o70swj", "title": "[R] Improving Genomic Discovery with Machine Learning", "score": 1, "url": "https://www.reddit.com/r/MachineLearning/comments/o70swj/r_improving_genomic_discovery_with_machine/", "author": "rshpkamil", "subreddit": "r/MachineLearning", "description": "Original source:  [https://ai.googleblog.com/2021/06/improving-genomic-discovery-with.html](https://ai.googleblog.com/2021/06/improving-genomic-discovery-with.html)\n\n&#x200B;\n\nSimilar AI / ML / Data Science articles in a form of a newsletter [here](https://thereshape.co/join?utm_source=reddit_ml_3)."}, {"id": "o70oyn", "title": "[R] Google Survey Explores Methods for Making DL Models \u2018Smaller, Faster, and Better\u2019", "score": 0, "url": "https://www.reddit.com/r/MachineLearning/comments/o70oyn/r_google_survey_explores_methods_for_making_dl/", "author": "Yuqing7", "subreddit": "r/MachineLearning", "description": "Researchers from Google conduct a survey on how to make Deep Learning models smaller, faster, and better. The team focuses on core areas of model efficiency, from modelling techniques to hardware support, and open-sources an experiment-based guide and code to help practitioners optimize their model training and deployment. \n\nHere is a quick read: [Google Survey Explores Methods for Making DL Models \u2018Smaller, Faster, and Better\u2019.](https://syncedreview.com/2021/06/24/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-48/)\n\nThe paper *Efficient Deep Learning: A Survey on Making Deep Learning Models Smaller, Faster, and Better* is on [arXiv](https://arxiv.org/abs/2106.08962)."}, {"id": "o70afq", "title": "[R] Seminal papers on machine learning for audio processing", "score": 1, "url": "https://www.reddit.com/r/MachineLearning/comments/o70afq/r_seminal_papers_on_machine_learning_for_audio/", "author": "struppel-", "subreddit": "r/MachineLearning", "description": "Anyone have recommendations for seminal/interesting papers or researchers in the field of machine learning for audio processing (mainly neural networks to perform modeling of nonlinear audio effects)? \n\nAlso papers that are not explicit on audio processing, but that might also be helpful!\n\nThanks!"}, {"id": "o6dxz1", "title": "[D] Should we trust cloud ML platforms (aws, azure, gcp)?", "score": 27, "url": "https://www.reddit.com/r/MachineLearning/comments/o6dxz1/d_should_we_trust_cloud_ml_platforms_aws_azure_gcp/", "author": "ai_ocular", "subreddit": "r/MachineLearning", "description": "Hi all,  \nI am in the early stages of a deep learning project with medical images and videos. Due to the large size and quantity of these, the use of cloud computing platforms such as AWS, Azure, GCP is imposed.\n\nHowever, in my team we have the well-founded suspicion that at least google and probably the others are *very interested in our data*. For this reason we are reluctant to use them. \n\nDo any of you know how are the contractual policies of these big platforms with the data deposited in them? Should we trust them? Are some of them more trustable than others? \n\nThanks"}, {"id": "o644ei", "title": "[D] How are computational neuroscience and machine learning overalapping?", "score": 177, "url": "https://www.reddit.com/r/MachineLearning/comments/o644ei/d_how_are_computational_neuroscience_and_machine/", "author": "fallszero_5", "subreddit": "r/MachineLearning", "description": "Hi, I am an undergrad with a background in neuroscience and math. I have been very much interested in the problem of AGI, how the human mind even exists, and how the brain fundamentally works. I think computational neuroscience is making a lot of headwinds on these questions (except AGI). Recently, I have been perusing some ML labs that have been working on the problems within cognitive neuroscience as well. I was wondering how these fields interact. If I do a PhD in comp neuro, is there a possibility for me to work in the ML and AI field if teach myself a lot of these concepts and do research that uses these concepts?"}, {"id": "o6bzvd", "title": "[D] Purchasing a $40K gpu server for a new lab", "score": 25, "url": "https://www.reddit.com/r/MachineLearning/comments/o6bzvd/d_purchasing_a_40k_gpu_server_for_a_new_lab/", "author": "HackZisBotez", "subreddit": "r/MachineLearning", "description": "I was tasked with finding our lab a good GPU server that is **built by an outside company** (I know it is much cheaper and more cost-effective to build it ourselves. Unfortunately the outside assembly part is a requirement).\n\nSo far I found Lambda Labs, ThinkMate and System76, but I'm sure there are more. Based on [Tim Dettmers' helpful blog](https://timdettmers.com/2020/09/07/which-gpu-for-deep-learning/), I think we should combine 67% RTX 3080 and 33% RTX 3090, but I welcome other opinions.\n\nDo you have a recommendation for any company that builds small (\\~$40K, servicing 2-5 students working mostly on CV and generative models) GPU servers, and how should such a server look like?"}, {"id": "o6ayi6", "title": "[D] How many ideas do you \"try out\" before finding something that's actually pursuable to something that's publishable?", "score": 26, "url": "https://www.reddit.com/r/MachineLearning/comments/o6ayi6/d_how_many_ideas_do_you_try_out_before_finding/", "author": "Seankala", "subreddit": "r/MachineLearning", "description": "As a master's student who's just now starting to take on research projects on my own, I've always heard that one of the most important qualities to doing research (in machine learning) is to \"try ideas out.\" Many people have given me this advice and have told me that it's the same advice their supervisors or seniors gave them.\n\nI'd follow accordingly and think of ideas that I could try (e.g., performing error analysis and trying out a new modeling approach) but for the past 4-5 months it's usually been ending up in results that are not pursuable (performance is mediocre and/or there aren't any surprising/interesting points to analyze) and definitely not publishable.\n\nI'm starting to wonder if I'm even approaching research correctly. Btw, before anyone says it, I _have_ been going to my PI for help. Many of the ideas that I've tried out were his, but he's more of an entrepreneur than a researcher and has been out of touch with the current research landscape for a while.\n\nAny tips or opinions are appreciated. Thanks."}, {"id": "o6kjin", "title": "[R] Should I submit a critique paper to CVPR?", "score": 7, "url": "https://www.reddit.com/r/MachineLearning/comments/o6kjin/r_should_i_submit_a_critique_paper_to_cvpr/", "author": "NarutoAI", "subreddit": "r/MachineLearning", "description": "I have a critique paper that pretty convincingly critiques papers one of which was published in CVPR.\n\nShould I submit my critique paper to CVPR? \n\nMore context: the papers I am critiquing mostly introduced very marginal changes to existing methodology and applied it to an Affective AI task. I critique the dataset (show there is large dataset bias) and show several methodological flaws (e.g. significant random errors unreported)."}, {"id": "o6c42r", "title": "[D] Paper Explained - XCiT: Cross-Covariance Image Transformers (Full Video Analysis)", "score": 20, "url": "https://www.reddit.com/r/MachineLearning/comments/o6c42r/d_paper_explained_xcit_crosscovariance_image/", "author": "ykilcher", "subreddit": "r/MachineLearning", "description": "[https://youtu.be/g08NkNWmZTA](https://youtu.be/g08NkNWmZTA)\n\nAfter dominating Natural Language Processing, Transformers have taken over Computer Vision recently with the advent of Vision Transformers. However, the attention mechanism's quadratic complexity in the number of tokens means that Transformers do not scale well to high-resolution images. XCiT is a new Transformer architecture, containing XCA, a transposed version of attention, reducing the complexity from quadratic to linear, and at least on image data, it appears to perform on par with other models. What does this mean for the field? Is this even a transformer? What really matters in deep learning?\n\n&#x200B;\n\nOUTLINE:\n\n0:00 - Intro & Overview\n\n3:45 - Self-Attention vs Cross-Covariance Attention (XCA)\n\n19:55 - Cross-Covariance Image Transformer (XCiT) Architecture\n\n26:00 - Theoretical & Engineering considerations\n\n30:40 - Experimental Results\n\n33:20 - Comments & Conclusion\n\n&#x200B;\n\nPaper: [https://arxiv.org/abs/2106.09681](https://arxiv.org/abs/2106.09681)\n\nCode: [https://github.com/facebookresearch/xcit](https://github.com/facebookresearch/xcit)"}, {"id": "o6m9bf", "title": "Randon forest - can I identify the best tree? [R]", "score": 2, "url": "https://www.reddit.com/r/MachineLearning/comments/o6m9bf/randon_forest_can_i_identify_the_best_tree_r/", "author": "Key-Government-3157", "subreddit": "r/MachineLearning", "description": "So I have a large database of patients with long follow-up and high mortality. I ran random forest and obviously it performs better than conventional clinical score for mortality prediction. In addition to area under roc curve and importance of variables, can I identify the best tree that predicts the best the outcome?\n\nIf not, what other tangible result could I get using RF? I need more materials to add to a paper I plan on writing.\nMany thanks!"}, {"id": "o6awsa", "title": "[D] Oversampling for Multivariate Time Series", "score": 10, "url": "https://www.reddit.com/r/MachineLearning/comments/o6awsa/d_oversampling_for_multivariate_time_series/", "author": "DazzlingPollution3", "subreddit": "r/MachineLearning", "description": " Currently working on oversampling for time series and have looked into methods like SPO(Structure-Preserving Oversampling) and OHIT, but both of them are univariate/working only for time series with one feature. Are there any oversampling techniques for multivariate time series (I am unable to find any)? Also, please suggest some papers that I can look into."}, {"id": "o6h9vb", "title": "[D] How to do multi-task learning intelligently", "score": 2, "url": "https://www.reddit.com/r/MachineLearning/comments/o6h9vb/d_how_to_do_multitask_learning_intelligently/", "author": "The_Gradient_Pub", "subreddit": "r/MachineLearning", "description": "We have a new article out, [How to Do Multi-Task Learning Intelligently](https://thegradient.pub/how-to-do-multi-task-learning-intelligently/), that may be of interest to you - it covers the concept of Multi-Task Learning, and provides a summary of some cool recent papers about it (AdaShare: Learning What To Share For Efficient Deep Multi-Task Learning , End-to-End Multi-Task Learning with Attention , and Which Tasks Should Be Learned Together in Multi-task Learning?).\n\nWould love feedback!"}, {"id": "o6moi5", "title": "How to decouple classifier output from associated risk [D]", "score": 1, "url": "https://www.reddit.com/r/MachineLearning/comments/o6moi5/how_to_decouple_classifier_output_from_associated/", "author": "Tokukawa", "subreddit": "r/MachineLearning", "description": "Hi all, I have the following problem and I hope you can help me. I have a classifier that needs to be retrained every once in a while. As the model take action on a stream of payments, every time we perform a retrain, we need to recalibrate all the thresholds of the business logic. I would like to decouple the classifier output score from the risk associated, in order to provide always the same risk associated with a score and avoiding the need of recalibrating every time. The only thing that comes to my mind is to stacking my model with k-means and fixing once for all the number k, in order to create buckets of risk. I would like to know what is the proper way to solve this problem in your experience/opinion?"}, {"id": "o6mjyi", "title": "[D] Image search, chatbot and chemical structure demo", "score": 0, "url": "https://www.reddit.com/r/MachineLearning/comments/o6mjyi/d_image_search_chatbot_and_chemical_structure_demo/", "author": "SouthBayDev", "subreddit": "r/MachineLearning", "description": "Hi all,  we recently updated our demo page showcasing examples of vector search for image search, chatbot and chemical structures using Milvus open-source software.  I'm looking for feedback, suggestions or questions you may have to make this relevant for any applications or use cases you may be developing.  Thank you all, I look forward to hearing your feedback!\n\n[Demo Page](https://zilliz.com/milvus-demos)\n\n[GitHub Repo](https://github.com/milvus-io/)"}, {"id": "o672vz", "title": "[D] I\u2019m Ilya, Team Principal of Acronis SIT Autonomous racing team, current N1 in Roborace \u2013 Ask Me Anything!", "score": 12, "url": "https://www.reddit.com/r/MachineLearning/comments/o672vz/d_im_ilya_team_principal_of_acronis_sit/", "author": "sit_autonomous", "subreddit": "r/MachineLearning", "description": "Hi Reddit!\n\nSIT Autonomous is a machine intelligence consulting firm, based at the Schaffhausen Institute of Technology (SIT) in Switzerland. We also have an autonomous racing team \u2013 Acronis SIT Autonomous \u2013 of which I am Team Principal. If you\u2019ve heard of Roborace, the world\u2019s first extreme competition of teams developing self-driving AI for autonomous vehicles, you\u2019ve probably heard about us. \n\n**Tomorrow**, **June 24, from 4 pm to 7 pm CEST (10 am EDT to 1 pm EDT)**, I\u2019ll be answering all your questions about ML in self-driving cars and autonomous racing. Already eager to ask? Send me your questions now so I can be sure to answer them all!\n\n**The discussion is happening in** r/SelfDrivingCars **:** [https://www.reddit.com/r/SelfDrivingCars/comments/o484jg/im\\_ilya\\_team\\_principal\\_of\\_acronis\\_sit\\_autonomous/h2fsko0/?context=3](https://www.reddit.com/r/SelfDrivingCars/comments/o484jg/im_ilya_team_principal_of_acronis_sit_autonomous/h2fsko0/?context=3)\n\nSee you there!"}, {"id": "o68dmg", "title": "[D] Why is LSTM/GRU not mentioned in time series classification state-of-the-art review?", "score": 12, "url": "https://www.reddit.com/r/MachineLearning/comments/o68dmg/d_why_is_lstmgru_not_mentioned_in_time_series/", "author": "jerha202", "subreddit": "r/MachineLearning", "description": "I'm reading up on state of the art of time series classification, and I just read [Deep learning for time series classification: a review](https://arxiv.org/abs/1809.04356) (Fawaz et al, 2019) which summarizes and compares different modern deep learning approaches. However it doesn't mention LSTM or GRU, which surprises me a lot, since they would be among the first approaches you'd read about in any recent introductory textbook or course on sequence modelling. I cite the only section that mentions RNN:\n\n>\"Another popular type of architectures for deep learning models is the Recurrent Neural Network (RNN). Apart from time series forecasting, we found that these neural networks were rarely applied for time series classification which is mainly due to three factors: (1) the type of this architecture is designed mainly to predict an output for each element (time stamp) in the time series (L\u00e4ngkvist et al., 2014); (2) RNNs typically suffer from the vanishing gradient problem due to training on long time series (Pascanu et al., 2012); (3) RNNs are considered hard to train and parallelize which led the researchers to avoid using them for computational reasons (Pascanu et al., 2013).\"\n\nDo you agree? And do you think LSTM and GRU are becoming obsolete with the emergence of convolutional approaches?"}, {"id": "o6kguq", "title": "[R] The Difference Between a Blurring Matrix and a PSF in Image Reconstruction", "score": 1, "url": "https://www.reddit.com/r/MachineLearning/comments/o6kguq/r_the_difference_between_a_blurring_matrix_and_a/", "author": "BoraaaBoraaa", "subreddit": "r/MachineLearning", "description": "I'm working on a research project related to deblurring images and I don't fully understand the difference between a blurring matrix and a PSF. I understand that to apply them to an image you use two different operators, but why is that, what is the difference between them. Is a PSF not a matrix? How are they related to each other? Do they affect the image the same way?"}, {"id": "o6f0bw", "title": "[R] Facebook AI & Mila Propose ALMA: Anytime Learning at Macroscale", "score": 2, "url": "https://www.reddit.com/r/MachineLearning/comments/o6f0bw/r_facebook_ai_mila_propose_alma_anytime_learning/", "author": "Yuqing7", "subreddit": "r/MachineLearning", "description": "A research team from Facebook AI Research and Mila - McGill University explores deep learning model accuracy versus time trade-offs in anytime learning, which they term Anytime Learning at Macroscale (ALMA). The team evaluates various models to gain insights on how to strike different trade-offs between accuracy and time to obtain a good learner. \n\nHere is a quick read: [Facebook AI & Mila Propose ALMA: Anytime Learning at Macroscale.](https://syncedreview.com/2021/06/23/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-47/)\n\nThe paper *On Anytime Learning at Macroscale* is on [arXiv](https://arxiv.org/abs/2106.09563)."}, {"id": "o69ao4", "title": "[D] Is there any feature for ranking most popular arXiv.org papers?", "score": 4, "url": "https://www.reddit.com/r/MachineLearning/comments/o69ao4/d_is_there_any_feature_for_ranking_most_popular/", "author": "pddpro", "subreddit": "r/MachineLearning", "description": "It would be useful to see which papers were most read or downloaded in [arxiv.org](https://arxiv.org), which has been a de-facto publishing platform for CS-related research. So I was wondering if there was any portal or platform that ranked popular [arxiv.org](https://arxiv.org) submissions (or maybe there's some feature in [arXiv.org](https://arXiv.org) itself that I don't know about). If there isn't any feature of this sort, then perhaps it should be?"}, {"id": "o6hs1z", "title": "[D] 5 minute paper digest: Towards Real-World Blind Face Restoration with Generative Facial Prior (GFP-GAN) by Xintao Wang et al", "score": 1, "url": "https://www.reddit.com/r/MachineLearning/comments/o6hs1z/d_5_minute_paper_digest_towards_realworld_blind/", "author": "KirillTheMunchKing", "subreddit": "r/MachineLearning", "description": "Have you ever tried restoring old photos? It is a long tedious process since the degradation artifacts are complex, and the poses and expressions are diverse. Luckily the authors from ARC Tencent came up with GFP-GAN - a new method for real-world blind face restoration that leverages a pretrained GAN and spatial feature transform to restore facial details with a single forward pass.\n\nRead the [full paper digest](https://t.me/casual_gan/54) (reading time \\~5 minutes) to learn about the degradation removing module, generative face prior, and channel-split feature transform.\n\nMeanwhile, check out the paper digest poster by [Casual GAN Papers](https://t.me/casual_gan)!\n\n[GFP-GAN](https://preview.redd.it/hrb8mr1pw1771.png?width=975&format=png&auto=webp&s=29e7cbc4621c7f532300f0d2158827f2094c6237)\n\n\\[[Full Explanation Post](https://t.me/casual_gan/54)\\] \\[[Arxiv](https://arxiv.org/pdf/2101.04061.pdf)\\] \\[[Code](https://github.com/TencentARC/GFPGAN)\\]\n\nMore recent popular computer vision paper breakdowns:\n\n>[CIPS](https://t.me/casual_gan/51)  \n[SimSwap](https://t.me/casual_gan/52)  \n[GANs N' Roses](https://t.me/casual_gan/53)"}, {"id": "o6hh3i", "title": "Most Effective Algorithms For Multi-Output Classification Tasks [Discussion]", "score": 1, "url": "https://www.reddit.com/r/MachineLearning/comments/o6hh3i/most_effective_algorithms_for_multioutput/", "author": "renitold", "subreddit": "r/MachineLearning", "description": " \n\nBased on benchmark datasets what algorithms perform the best in an array of multi-output classification problems? I have found limited research papers exploring a diverse spread of algorithm families on multioutput problems.\n\nI would like to guess the the best in the industry is a convolutional neural network or simply a deep neural network. All the same I would like some input! Thank you."}, {"id": "o6hgba", "title": "Most Effective Algorithms For Multi-Output Classification Tasks", "score": 0, "url": "https://www.reddit.com/r/MachineLearning/comments/o6hgba/most_effective_algorithms_for_multioutput/", "author": "renitold", "subreddit": "r/MachineLearning", "description": " Based on benchmark datasets what algorithms perform the best in an array of multi-output classification problems? I have found limited research papers exploring a diverse spread of algorithm families on multioutput problems.\n\nI would like to guess the the best in the industry is a convolutional neural network or simply a deep neural network. All the same I would like some input! Thank you."}, {"id": "o5yfcl", "title": "[D] neural network based \"association rules\"", "score": 30, "url": "https://www.reddit.com/r/MachineLearning/comments/o5yfcl/d_neural_network_based_association_rules/", "author": "SQL_beginner", "subreddit": "r/MachineLearning", "description": "https://en.m.wikipedia.org/wiki/Association_rule_learning\n\nHas anyone ever looked into more advanced applications of \"association rules mining\"? \"Association rules\" can also be used for prediction/classification purposes (e.g. https://rdrr.io/cran/arulesCBA/man/CBA.html) - this allows you to obtain a fully interpertable algorithm that can provides \"a set of conditions\" for making predictions. However, these rules are usually not very \"powerful\".\n\nDoes anyone know if there are more recent spinoffs of this algorithm, perhaps where a neural network or ensemble models can be used to \"learn\" these rules? Or in general, does anyone know any machine learning based algorithms that provide \"rules\"?\n\nThanks"}, {"id": "o5lmwo", "title": "[P] FastDS - Quality-of-life wrapper for Git and DVC", "score": 130, "url": "https://www.reddit.com/r/MachineLearning/comments/o5lmwo/p_fastds_qualityoflife_wrapper_for_git_and_dvc/", "author": "Tolstoyevskiy", "subreddit": "r/MachineLearning", "description": "I want to share a [new open source command line tool, FDS](https://github.com/dagshub/fds), which aims to help users do Fast Data Science by streamlining version control for ML projects. FDS is a command line wrapper around Git and [DVC](https://dvc.org/), meant to minimize the chances of human error, automate repetitive tasks, and provide a smoother landing for new users.\n\n# Quickstart:\n\n[https://github.com/dagshub/fds](https://github.com/dagshub/fds)\n\n    pip install fastds \n    fds -h \n\nFull blog post on the motivations and goals of the project:\n\n[https://dagshub.com/blog/fds-fast-data-science-with-git-and-dvc](https://dagshub.com/blog/fds-fast-data-science-with-git-and-dvc)\n\n# Summary:\n\nWhy is it called fds?\n\nJust take a look at your keyboard - it's so silky smoove to type fds! This is important for a command line tool that exists to improve ease of use and delight users.\n\n**In fact, due to popular demand, you can also type** **sdf** **instead of** **fds** **for an even more epic experience!** \ud83e\udd29\n\n[so smoove](https://preview.redd.it/h2clzec16t671.png?width=659&format=png&auto=webp&s=8654e4f757032dec54313a2b84c9c90fe7380480)\n\n# Why did we do this?\n\nAs we were developing Open Source Data Science projects using DVC, we often found ourselves making the same mistakes over and over again, and constantly repeating pairs of commands like \"git status\" and \"dvc status\"\n\nSo, we set about creating FDS with these goals in mind:\n\n1. Automate common tasks when working with git, DVC, and, later on, potentially other tools which work well together.\n2. Provide a more interactive and opinionated UI and UX. Git and DVC are low level utilities which need to work well in scripts and support all possible use cases - this means interacting with them feels like interacting with a command line API, rather than a wizard or app. FDS orients itself to be used by humans, for convenience rather than total flexibility.\n3. Provide a smoother landing for new users by making things easy by default and explaining what's going on.\n\nWe took inspiration from [gitless](https://gitless.com/) \\- \"a Git-compatible version control system, that is easy to learn and use\" - a project which works on top of Git and attempts to make it more intuitive. Check it out!\n\nWould love to get your feedback, feature requests are welcome, pull requests even more so!"}, {"id": "o5nmoz", "title": "[R] Regularization is all you Need: Simple Neural Nets can Excel on Tabular Data", "score": 88, "url": "https://arxiv.org/abs/2106.11189", "author": "koolaidman123", "subreddit": "r/MachineLearning", "description": ""}, {"id": "o6d02g", "title": "[N] Open Catalyst Challenge: Using AI to Find Catalysts for Renewable Energy Storage", "score": 1, "url": "https://www.reddit.com/r/MachineLearning/comments/o6d02g/n_open_catalyst_challenge_using_ai_to_find/", "author": "SubstantialRange", "subreddit": "r/MachineLearning", "description": "[https://opencatalystproject.org/](https://opencatalystproject.org/)\n\n&#x200B;\n\n>The  Open Catalyst Project is a collaborative research effort between  Facebook AI Research (FAIR) and Carnegie Mellon University\u2019s (CMU)  Department of Chemical Engineering.The  aim is to use AI to model and discover new catalysts for use in  renewable energy storage to help in addressing climate change.  Scalable  and cost-effective solutions to renewable energy storage are essential  to addressing the world\u2019s rising energy needs while reducing climate  change.  \n>  \n>As we increase our  reliance on renewable energy sources such as wind and solar, which  produce intermittent power, storage is needed to transfer power from  times of peak generation to peak demand. This may require the storage of  power for hours, days, or months.  \n>  \n>One  solution that offers the potential of scaling to nation-sized grids is  the conversion of renewable energy to other fuels, such as hydrogen. To  be widely adopted, this process requires cost-effective solutions to  running chemical reactions.  \n>  \n>An  open challenge is finding low-cost catalysts to drive these reactions at  high rates. Through the use of quantum mechanical simulations (density  functional theory), new catalyst structures can be tested and evaluated.  Unfortunately, the high computational cost of these simulations limits  the number of structures that may be tested.  \n>  \n>The  use of AI or machine learning may provide a method to efficiently  approximate these calculations, leading to new approaches in finding  effective catalysts.  \n>  \n>To enable the broader research community to participate in this  important project, we are releasing the  [Open Catalyst Dataset](https://github.com/Open-Catalyst-Project/ocp/blob/master/DATASET.md)  for training ML models. The dataset contains 1.2 million molecular  relaxations  with results from over 250 million DFT calculations. In  addition to the data, baseline models and code are provided on [our Github page](https://github.com/Open-Catalyst-Project/ocp). View the [leaderboard](https://opencatalystproject.org/leaderboard_is2re.html) to see the latest results and to submit your own to the [evaluation server](https://eval.ai/web/challenges/challenge-page/712/overview)! Join the  [discuss forum](https://discuss.opencatalystproject.org/) to join the discussion with the community and ask any questions.\n\n&#x200B;"}, {"id": "o604zo", "title": "[D] Mixed Precision Training Tips", "score": 12, "url": "https://www.reddit.com/r/MachineLearning/comments/o604zo/d_mixed_precision_training_tips/", "author": "tpapp157", "subreddit": "r/MachineLearning", "description": "Recently upgraded to a 3000 series card and have been playing around with mixed precision training. I notice some model architectures train just fine while others become unstable and collapse. Outside of a couple basic tutorials to implement mixed precision training, I haven't been able to find any general tips on how to keep training stable. Things to change or avoid about a model or training loop when switching to mixed precision.\n\nDoes anyone know of any good references like these?"}, {"id": "o679rm", "title": "[R] Calliar: An Online Handwritten Dataset for Arabic Calligraphy", "score": 3, "url": "https://arxiv.org/abs/2106.10745", "author": "hardmaru", "subreddit": "r/MachineLearning", "description": ""}, {"id": "o6b9sr", "title": "[D] Make use of validation data ( without label ) when predicting values.", "score": 0, "url": "https://www.reddit.com/r/MachineLearning/comments/o6b9sr/d_make_use_of_validation_data_without_label_when/", "author": "sunotlac", "subreddit": "r/MachineLearning", "description": "Lets suppose a model where:  \n\\- I have a train dataset (labeled) which I can use freely  \n\\-  Can only predict values in batch ( you have to feed it N validation tests and the model will attribute a label to all of them ), which means the model cannot predict each validation case, but need to predict all of them at once ( taking into consideration their values but OBVIOUSLY not their label - which may be unknown).  \n\n\n  \nWould you guys consider this model biased? I really would like a discussion about this or maybe even some literature references. I honestly dont think this should be considered biased."}, {"id": "o5r3rc", "title": "[R] New Milestone for Deep Potential Application: Predicting the Phase Diagram of Water", "score": 38, "url": "https://www.reddit.com/r/MachineLearning/comments/o5r3rc/r_new_milestone_for_deep_potential_application/", "author": "Yuqing7", "subreddit": "r/MachineLearning", "description": "A research team from Princeton University, the Institute of Applied Physics and Computational Mathematics and the Beijing Institute of Big Data Research uses the Deep Potential (DP) method to predict the phase diagram of water from ab initio quantum theory, from low temperature and pressure to about 2400 K and 50 GPa. The paper was published in leading physics journal Physical Review Letters and represents an important milestone in the application of DP.   \n\nHere is a quick read: [New Milestone for Deep Potential Application: Predicting the Phase Diagram of Water.](https://syncedreview.com/2021/06/22/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-46/)\n\nThe paper *The Phase Diagram of a Deep Potential Water Model* is on[*Physical Review Letters*](https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.126.236001) and [arXiv](https://arxiv.org/abs/2102.04804)."}, {"id": "o6akl5", "title": "[D] Spacy Transformers wrapper for newer transformer model", "score": 0, "url": "https://www.reddit.com/r/MachineLearning/comments/o6akl5/d_spacy_transformers_wrapper_for_newer/", "author": "that_s_me", "subreddit": "r/MachineLearning", "description": "Hello All,\n\nI am working with Spacy Transformers, which is essentially a wrapper for huggingface transformers. It already supports few transformers model out of the box, My question is how can I use this for other transformer models within spacy?\n\nIf you need any other input from me, please let me know.\n\nYour help is really appreciated, thank you for you time."}, {"id": "o6a3lz", "title": "Is it possible for a model to increase overfitting when seeing new training examples for the first time? [D]", "score": 0, "url": "https://www.reddit.com/r/MachineLearning/comments/o6a3lz/is_it_possible_for_a_model_to_increase/", "author": "Pseudoabdul", "subreddit": "r/MachineLearning", "description": "So I was running a CNN over a large image dataset (94k images) and I was concerned about overfitting, so I implemented early stopping. But if I set 1 epoch to be all the training data, I found it overfit a lot before early stopping had a chance to stop it. So I reduced the epoch steps to about 10k so it would check after each time. I found that even after 3 epochs, the early stopping trigger when it measured that the validation MSE was increasing(even with a patience of 2 epochs).\n\nBut 3 epochs is only 30k samples, so the model hasn't even seen all the data points.  It's being given new data points and some how the validation MSE is increasing, even as the training MSE is decreasing. \n\nIf I were training over the same training data many times, I understand why the model would overfit, but I don't see how giving it new training examples is reducing its ability to generalize. My guess is one of two things is happening:\n\n1. The model overfitting isn't increasing, and it just got lucky on the first run somehow. \n\n2. The model is overfitting due to some other purposes. \n\nThe validation set I'm using is the same each epoch, so that shouldn't be an issue. Has anyone experienced this before?\n\nUpdate: I just did another test, and the validation MAE is way better than the training MAE in the first epoch, but stays roughly consistent and the training MAE eventually overtakes it. Similar situation with the MSE."}, {"id": "o68pxh", "title": "NLP Feedback System [R]", "score": 0, "url": "https://www.reddit.com/r/MachineLearning/comments/o68pxh/nlp_feedback_system_r/", "author": "TheLoneWolf020", "subreddit": "r/MachineLearning", "description": "For eg, if we are provided with an NLP sentiment analysis model that has already been trained. like if we provide this model with some word like \u201cExcellent\u201d, it knows that this particular word is to be classified as positive. Now, if we provide this model with a new set of words that might contain new as well as words that the model already knows, but with some different labels. for eg, the excellent word which the model knows to be positive is labelled as negative in this new set. Then can the model adapt and change?   \nIs there any work done similar to this or can someone explain to me a way to achieve this?"}, {"id": "o68mxa", "title": "[N] AI Researchers From MIT Lincoln Lab Developed RIO (Reconnaissance of Influence Operations) System That Would Counter The Spread Of Disinformation By Making Use Of Machine Learning", "score": 1, "url": "https://www.reddit.com/r/MachineLearning/comments/o68mxa/n_ai_researchers_from_mit_lincoln_lab_developed/", "author": "ai-lover", "subreddit": "r/MachineLearning", "description": "Disinformation manipulates accurate information deliberately to mislead the masses, and the spread of such information is not new. It has been in practice for quite some time, right from the imperial war propaganda and now in this digitalized world. Social media has brought with it its perils, and one of them is its usage to spread false information. It has the power to change opinions altogether, for example, the entire dynamics of the public elections. However, it is now being claimed that artificial intelligence systems could efficiently detect and simultaneously counter the spread of this disinformation on digital platforms. The Reconnaissance of Influence Operations (RIO) program built at the MIT Lincoln Laboratory promises to do just the same. It would automatically detect and analyze all the social media accounts used to spread disinformation across a network.\n\n[https://www.marktechpost.com/2021/06/23/ai-researchers-from-mit-lincoln-lab-developed-rio-reconnaissance-of-influence-operations-system-that-would-counter-the-spread-of-disinformation-by-making-use-of-machine-learning/](https://www.marktechpost.com/2021/06/23/ai-researchers-from-mit-lincoln-lab-developed-rio-reconnaissance-of-influence-operations-system-that-would-counter-the-spread-of-disinformation-by-making-use-of-machine-learning/) \n\nPaper: https://www.pnas.org/content/118/4/e2011216118"}, {"id": "o5y8mq", "title": "[D] ML workstation vs Google Colab Pro", "score": 5, "url": "https://www.reddit.com/r/MachineLearning/comments/o5y8mq/d_ml_workstation_vs_google_colab_pro/", "author": "__Julia", "subreddit": "r/MachineLearning", "description": "Hi,\n\nI have been using Google Colab for some time now and it was convenient. However, we would like to check if building a workstation (e.g can be found [here](https://www.youtube.com/watch?v=S0-lM6mZJn0)) worth it.\n\nThose who build their own workstation, could you please chime in on what are the benefits that you gain of using your own workstation vs Google Colab/Amazon SageMaker"}, {"id": "o59p0f", "title": "[D] Karpathy @ CVPR 2021 Workshop on Autonomous Vehicles", "score": 209, "url": "https://www.reddit.com/r/MachineLearning/comments/o59p0f/d_karpathy_cvpr_2021_workshop_on_autonomous/", "author": "AristocraticOctopus", "subreddit": "r/MachineLearning", "description": "[Video here](https://www.youtube.com/watch?v=NSDTZQdo6H8). [Full workshop stream here](https://youtu.be/eOL_rCK59ZI?t=28293).\n\nKarpathy discusses how radar bugs were holding Autopilot performance back, and how removing it from the stack to go vision-only ultimately improved performance (though it seems they did use radar to some degree in automatic labeling/triggering events to train on in an active learning style).\n\nHe also discusses Tesla's supercomputer for training (5,760 A100 GPUs), and some data engine and team management processes at Tesla. Still nothing new about their Dojo project (NN training accelerator chip).\n\nIt's still unclear to me to what extent they use all these predicted semantic features like object detection/other agent kinematics to plan with classically, and how much they use things like the \"predicted future path\", or how these are merged to actually act in the FSD beta."}, {"id": "o5tw5o", "title": "[D] Struggles when reading AI/ML papers", "score": 6, "url": "https://www.reddit.com/r/MachineLearning/comments/o5tw5o/d_struggles_when_reading_aiml_papers/", "author": "ka_doom", "subreddit": "r/MachineLearning", "description": "Hey everyone, \n\nI was wondering with what are you struggling the most when reading AI/ML papers? What do you find are the biggest obstacles in understanding those papers and what is your way of dealing with them?"}, {"id": "o5h8za", "title": "[Discussion] C++ Usage for Machine Learning", "score": 35, "url": "https://www.reddit.com/r/MachineLearning/comments/o5h8za/discussion_c_usage_for_machine_learning/", "author": "mha_1992", "subreddit": "r/MachineLearning", "description": " Hi all,\n\nI'm wondering if I should learn C++ if I want to get a job as an ML engineer or other DS role. I am currently comfortable with Python and R. \n\n1. Would learning C++ be beneficial to getting a job in the ML industry?\n2. Are more and more ML jobs today requiring C++ proficiency?"}, {"id": "o5sbkh", "title": "[D] Statistics refresher: Bengio & Goodfellow, Bishop, or Murphy?", "score": 4, "url": "https://www.reddit.com/r/MachineLearning/comments/o5sbkh/d_statistics_refresher_bengio_goodfellow_bishop/", "author": "91o291o", "subreddit": "r/MachineLearning", "description": "All these books have a couple of chapters devoted to statistic, which one would you pick as an introduction to statistics?"}, {"id": "o5lehy", "title": "[D] Similar Image Retrieval", "score": 7, "url": "https://www.reddit.com/r/MachineLearning/comments/o5lehy/d_similar_image_retrieval/", "author": "grid_world", "subreddit": "r/MachineLearning", "description": "I am trying to build a similar image retrieval system where given an image, the system is able to show top 'k' most similar images to it. For this particular example, I am using the [DeepFashion](http://mmlab.ie.cuhk.edu.hk/projects/DeepFashion.html) dataset where given an image containing say a shirt, you show top 5 clothes most similar to a shirt. A subset of this has 289,222 diverse clothes images in it. Each image is of shape: (300, 300, 3).\n\nThe approach I have includes:\n\n1. Train an autoencoder\n2. Feed each image in the dataset through the encoder to get a reduced n-dimensional latent space representation. For example, it can be 100-d latent space representation\n3. Create a table of shape m x (n + 2) where 'm' is the number of images and each image is compressed to n-dimensions. One of the column is the image name and the other column is a path to where the image is stored on your local system\n4. Given a new image, you feed it through the encoder to get the n-dimensional latent space representation\n5. Use something like cosine similarity, etc to compare the n-d latent space for new image with the table m x (n + 2) obtained in step 3 to find/retrieve top k closest clothes\n\nHow do I create the table mentioned in step 3?\n\nI am planning on using TensorFlow 2.5 with Python 3.8 and the code for getting an image generator is as follows:\n\n    image_generator = ImageDataGenerator(\n        rescale = 1./255, rotation_range = 135)\n    \n    train_data_gen = image_generator.flow_from_directory(\n        directory = train_dir, batch_size = batch_size,\n        shuffle = False, target_size = (IMG_HEIGHT, IMG_WIDTH),\n        class_mode = 'sparse'\n\nHow can get image name and path to image to create the m x (n + 2) table in step 3?\n\nAlso, is there any other better way that I am missing out on?\n\nThanks!"}, {"id": "o5j2hj", "title": "[R] New work linking Partial Differential Equations and Graph Neural Networks", "score": 11, "url": "https://www.reddit.com/r/MachineLearning/comments/o5j2hj/r_new_work_linking_partial_differential_equations/", "author": "bpchamberlain", "subreddit": "r/MachineLearning", "description": "Hi everyone, \n\nSharing some work from the graph ML team at Twitter showing how a new class of GNNs can be constructed by discretising diffusion PDEs.  Was  on arxiv today and it will be at ICML21.\n\nThinking of GNNs as partial differential equations leads to a new broad class of GNNs that are able to address in a principled way some of the prominent issues of current Graph ML models such as depth, oversmoothing, bottlenecks, and graph rewiring.\u00a0\n\nMany popular GNNs can be formalised as discretised diffusion PDEs with explicit single-step Euler scheme with a time step of 1, where an iteration corresponds to a convolutional or attentional layer of the graph neural network, and running the diffusion for multiple iterations amounts to applying a GNN layer multiple times. In the Neural PDEs formalism, the diffusion time parameter acts as a continuous analogy of the layers\u2014an interpretation allowing us to exploit more efficient and stable numerical schemes that use adaptive steps in time.\n\nBlog post: [https://bit.ly/3gUOEL8](https://t.co/dNaibcliBR?amp=1)   \nPaper: [https://arxiv.org/abs/2106.10934](https://t.co/wSyHNemQ9x?amp=1)   \nCode: [https://github.com/twitter-research/graph-neural-pde](https://github.com/twitter-research/graph-neural-pde)  \n\n\nhttps://preview.redd.it/68zohy4ebs671.png?width=647&format=png&auto=webp&s=439f7b1428bd5fd9b5751ecbe4975140084eb677"}, {"id": "o5ggz9", "title": "[D] What best-practices are folks using to make sure their data-labeling guidelines are effective?", "score": 19, "url": "https://www.reddit.com/r/MachineLearning/comments/o5ggz9/d_what_bestpractices_are_folks_using_to_make_sure/", "author": "SuperCanape", "subreddit": "r/MachineLearning", "description": "My team's helping computer-vision and NLP companies label training data and it's quite common for team's to not have comprehensive guidelines. \n\nIssue here is that there's often a lag between deciding what type of labels you need and having them prepared while you hash out all the details. \n\n&#x200B;\n\nQuestion if your annotating videos, images, texts, how long do you spend preparing your labeling guidelines and discussing edge-cases with your annotators? \n\n&#x200B;\n\nhttps://preview.redd.it/qy3tjb1bir671.jpg?width=720&format=pjpg&auto=webp&s=0fa6bbc3816d6114200d813c45fe56d128444ae6"}, {"id": "o626og", "title": "[P] Jina - Cross Modal Search System (Open source search engine)", "score": 0, "url": "https://www.reddit.com/r/MachineLearning/comments/o626og/p_jina_cross_modal_search_system_open_source/", "author": "JEUNGHWAN", "subreddit": "r/MachineLearning", "description": "Jina is a Neural Search Framework that provides large-scale indexing and querying of different kinds of data, including video, images, text, music, source code, and PDFs.  \nTry it now!\n\nGoogle I/O and Jina Video :\n\n[https://www.youtube.com/watch?v=lJogWSnl5Es](https://www.youtube.com/watch?v=lJogWSnl5Es)"}, {"id": "o5ojq5", "title": "[D] Most valuable skills for Data Scientists and Machine Learning Engineering roles", "score": 5, "url": "https://www.reddit.com/r/MachineLearning/comments/o5ojq5/d_most_valuable_skills_for_data_scientists_and/", "author": "siriono", "subreddit": "r/MachineLearning", "description": "Hi, I have almost finished my master degree in CS (with a special  curriculum on Data Science and Machine Learning). The majority of my  knowledge are theoretical and at academic level. Since I want to be  prepared for my future jobs, what are the most valuable skills for Data  Scientists and ML Engineering roles that are not taught or  underestimated? Or what are those skills and knowledge that you would have learned before starting to work? \n\n I have two months off this summer and I want to use this  time at the best."}, {"id": "o5rhmx", "title": "[R] Causal Analysis of Syntactic Agreement Mechanisms in Neural Language Models", "score": 2, "url": "https://arxiv.org/abs/2106.06087", "author": "bert4QA", "subreddit": "r/MachineLearning", "description": ""}, {"id": "o4wd3f", "title": "[N] CVPR '21 Best Paper: GIRAFFE: Representing Scenes as Compositional Generative Neural Feature Fields", "score": 177, "url": "https://www.reddit.com/r/MachineLearning/comments/o4wd3f/n_cvpr_21_best_paper_giraffe_representing_scenes/", "author": "creiser", "subreddit": "r/MachineLearning", "description": "Michael Niemeyer's work **GIRAFFE: Representing Scenes as Compositional Generative Neural Feature Fields** has just been given the Best Paper award at CVPR 2021. 10k submissions but you made it. Congrats Michael and Andreas! It's an honor to work with you.\n\nAbstract:\n\nDeep generative models allow for photorealistic image synthesis at high resolutions. But for many applications, this is not enough: content creation also needs to be controllable. While several recent works investigate how to disentangle underlying factors of variation in the data, most of them operate in 2D and hence ignore that our world is three-dimensional. Further, only few works consider the compositional nature of scenes. Our key hypothesis is that incorporating a compositional 3D scene representation into the generative model leads to more controllable image synthesis. Representing scenes as compositional generative neural feature fields allows us to disentangle one or multiple objects from the background as well as individual objects' shapes and appearances while learning from unstructured and unposed image collections without any additional supervision. Combining this scene representation with a neural rendering pipeline yields a fast and realistic image synthesis model. As evidenced by our experiments, our model is able to disentangle individual objects and allows for translating and rotating them in the scene as well as changing the camera pose.\n\n&#x200B;\n\n* Project page: [https://m-niemeyer.github.io/project-pages/giraffe/index.html](https://m-niemeyer.github.io/project-pages/giraffe/index.html)\n* Paper: [http://www.cvlibs.net/publications/Niemeyer2021CVPR.pdf](http://www.cvlibs.net/publications/Niemeyer2021CVPR.pdf)\n* Twitter: [https://twitter.com/AutoVisionGroup/status/1406973670740922368](https://twitter.com/AutoVisionGroup/status/1406973670740922368)"}, {"id": "o5bnjp", "title": "[D] Advice on Dodging Grad School Poverty?", "score": 21, "url": "https://www.reddit.com/r/MachineLearning/comments/o5bnjp/d_advice_on_dodging_grad_school_poverty/", "author": "RSchaeffer", "subreddit": "r/MachineLearning", "description": "Does anyone have any advice for how to make decent income while earning one's PhD in ML? I'm open to all suggestions - consulting, year-round company funding, blogging, etc. Personal experiences would be most appreciated!"}, {"id": "o5h157", "title": "[N] Extensive (N=1250) survey on ML/DS salaries in Israel. Thorough analysis!", "score": 8, "url": "https://www.reddit.com/r/MachineLearning/comments/o5h157/n_extensive_n1250_survey_on_mlds_salaries_in/", "author": "LSTMeow", "subreddit": "r/MachineLearning", "description": "Disclaimer: Unaffiliated with MDLI, although I am an active member of that community.\n(Arabic and Hebrew versions available through the link!)\n\nFrom the link:\nAs in previous years, this year we ran the MDLI community\u2019s annual survey, so as to map various trends among those who work in the data science and machine learning fields. This year, an exceptional number of respondents completed our annual survey \u2013 1,250 people \u2013 a respectable achievement by all counts. Omri Goldstein, an algorithm developer, data scientist, and creator of the \u201cData-driven\u201d blog analyzed the survey\u2019s findings. We used his analysis to generate the MDLI community\u2019s 2021 annual payroll report. We also developed a dedicated salary calculator for data professionals in Israel.\n\nhttps://machinelearning.co.il/8280/mdli2021reportenglish/"}, {"id": "o5qrub", "title": "[D] 5 minute paper digest: GANs N\u2019 Roses: Stable, Controllable, Diverse Image to Image Translation (works for videos too!) by Min Jin Chong et al.", "score": 1, "url": "https://www.reddit.com/r/MachineLearning/comments/o5qrub/d_5_minute_paper_digest_gans_n_roses_stable/", "author": "KirillTheMunchKing", "subreddit": "r/MachineLearning", "description": "[Your dream Anime waifu!](https://i.redd.it/mn3ty4acdu671.gif)\n\nDid you ever want to see what you look like as an anime waifu? Thanks to the authors of GANs N' Roses you can animefy (pretty sure this isn't a real word) selfies and even videos with a multitude of unique styles. The authors from the University of Illinois propose a new generator architecture that combines a content code computed from a face image and a randomly chosen style to produce consistent, diverse and controllable anime faces with attributes matching the content image.\n\nRead the [full paper digest](https://t.me/casual_gan/53) (reading time \\~5 minutes) to learn about the encoder-decoder architecture of the authors' content-style image generation method, the tricks for ensuring style diversity, and the losses required for high fidelity anime image synthesis.\n\nMeanwhile, check out the paper digest poster by [Casual GAN Papers](https://t.me/casual_gan)!\n\n[GANs N' Roses](https://preview.redd.it/yy1w8k42du671.png?width=1877&format=png&auto=webp&s=f2607d49f1b6aea21edec9f15870ae01aad5f4a9)\n\n\\[[Full Explanation Post](https://t.me/casual_gan/53)\\] \\[[Arxiv](https://arxiv.org/pdf/2106.06561v1.pdf)\\] \\[[Code](https://github.com/mchong6/GANsNRoses)\\]\n\nMore recent popular computer vision paper breakdowns:\n\n>[CIPS](https://t.me/casual_gan/51)  \n[SimSwap](https://t.me/casual_gan/52)  \n[Decision Transformer](https://t.me/casual_gan/50)"}, {"id": "o5dm09", "title": "Just a fun idea [Project]", "score": 7, "url": "https://www.reddit.com/r/MachineLearning/comments/o5dm09/just_a_fun_idea_project/", "author": "BestUCanIsGoodEnough", "subreddit": "r/MachineLearning", "description": "This is not super high on my list of priorities, but if nobody tries it, I\u2019ll give it a shot at some point. I\u2019d like to see a manifold or gradient 3D printed (with support structure) from an actual AI model you\u2019ve fit or a function that\u2019s been optimized. And then...a marble may be involved... it would be kind of next level awesome to compare two optimization methods, like Adam and SGD. I would like to see how the path an object rolling down the gradient would differ between optimizers. If you send me an ndarray of some project you ran, that you could represent the fitting of in Euclidean space, I\u2019ll happily turn it into a mesh and print it."}, {"id": "o5k7v0", "title": "[D] Object detection: Is it detrimental to retrain but only focusing on one object type?", "score": 1, "url": "https://www.reddit.com/r/MachineLearning/comments/o5k7v0/d_object_detection_is_it_detrimental_to_retrain/", "author": "CaptainI9C3G6", "subreddit": "r/MachineLearning", "description": "Hi,\n\nI'm currently working on an object detection model using gluoncv/autogluon. My model is at about 80% for some objects, but much worse for others.\n\nWhen I'm updating/retraining my model is it harmful to only train with certain image types? So if I know I have a set of images which contain objects A and B, will only tagging and retraining with images that tag object B increase accuracy for B, and/or decrease accuracy for object A?\n\nThanks!\n\nP.S. if it helps here are some of the current model parameters:\n\n`{ 'amp': False,`\n\n`'base_network': 'resnet50_v1',`\n\n`'data_shape': 512,`\n\n`'filters': None,`\n\n`'nms_thresh': 0.45,`\n\n`'nms_topk': 400,`\n\n`'ratios': ( [1, 2, 0.5],`\n\n`[1, 2, 0.5, 3, 0.3333333333333333],`\n\n`[1, 2, 0.5, 3, 0.3333333333333333],`\n\n`[1, 2, 0.5, 3, 0.3333333333333333],`\n\n`[1, 2, 0.5],`\n\n`[1, 2, 0.5]),`\n\n`'sizes': (30, 60, 111, 162, 213, 264, 315),`\n\n`'steps': (8, 16, 32, 64, 100, 300),`\n\n`'syncbn': False,`\n\n`'transfer': 'ssd_512_resnet50_v1_coco'}`"}, {"id": "o5je80", "title": "[D] How would you approach creating a paraphrase dataset?", "score": 2, "url": "https://www.reddit.com/r/MachineLearning/comments/o5je80/d_how_would_you_approach_creating_a_paraphrase/", "author": "maroxtn", "subreddit": "r/MachineLearning", "description": "I have been trying to create a paraphrasing dataset for a non-English language (Arabic), which has no datasets for this task. However I am not sure on how to approach it.\n\nMy hacky way was to take a translation dataset (Arabic - English), set the Arabic sentences as the target, and translate English sentences (using pretrained models, or goolgle translation) to Arabic and set them as the source. My impression was that this might workout since in translation phrasing might changed but the meaning is preserved.\n\nHowever when fine tuning gpt on that dataset, the model does not perform well, as it changes the meaning of text often, and add extra info that didn't exist in the first place. My theory is that this is coming for the dataset - maybe many mistranslated sentences ?\n\nWhat I am thinking of trying now is overfitting a pretrained translation model on my dataset, so I be sure the translations are not wrong, and generate a new dataset.\n\n&#x200B;\n\nWhat are you thoughts on my approach? And how would you approach it yourself? \n\nThank you."}, {"id": "o5tok1", "title": "[D] Deep Learning for AI by Bengio, Lecun, and Hinton", "score": 0, "url": "https://www.reddit.com/r/MachineLearning/comments/o5tok1/d_deep_learning_for_ai_by_bengio_lecun_and_hinton/", "author": "rshpkamil", "subreddit": "r/MachineLearning", "description": "Discussion of the origins of deep learning, a few of the more recent advances, and future challenges by deep learning godfathers.\n\nOriginal article here: [https://cacm.acm.org/magazines/2021/7/253464-deep-learning-for-ai/fulltext](https://cacm.acm.org/magazines/2021/7/253464-deep-learning-for-ai/fulltext)\n\nMore hard-to-find, independent stuff related to AI & Data Science [here](https://thereshape.co/?utm_source=reddit_ml_1)."}, {"id": "o5kar0", "title": "[D] How long of sequence to train LSTM/GRU on?", "score": 0, "url": "https://www.reddit.com/r/MachineLearning/comments/o5kar0/d_how_long_of_sequence_to_train_lstmgru_on/", "author": "chasep255", "subreddit": "r/MachineLearning", "description": "Wondering if there is a general rule of thumb for how long of a sequence I should train my GRU/LSTM on?  Also is there any research on the upper limit of how far back in time a GRU and LSTM can realistically look?\n\nWhat I am trying to do is create a decoder for my VQVAE audio auto encoder using raw audio.  I know RNNs are not the best for generating raw audio due to the extremely long range dependencies in audio signals, however, my decoder consists of a bunch of convolutions before I expand out to the original length of the audio and use an auto regressive RNN network to finish the decoding.  This way I think the RNN can focus only on the higher frequency short term signals.  \n\nCurrently I have a stack of 2 GRUs with 512 cells each.  I am training it on sequences of 1024 sampled from the output of the expander portion of my decoder.  When it comes to actually running this in inference I plan to just carry the state of the RNN forward from one prediction to the next never resetting the state for hundreds of thousands of samples.  I am unsure if this is a valid thing to do since it was only trained on sequences of 1024.  Maybe I should run it for 1024 steps.  Then reset the state, go back 256 steps for a warm up and repeat this over the full sequence."}, {"id": "o4wbgg", "title": "[R] Disrupting Model Training with Adversarial Shortcuts", "score": 20, "url": "https://www.reddit.com/r/MachineLearning/comments/o4wbgg/r_disrupting_model_training_with_adversarial/", "author": "ofirpress", "subreddit": "r/MachineLearning", "description": "It\u2019s not always great that people can train machine learning models on your data! In this new work, we create adversarial shortcuts to prevent neural network training. Adversarial shortcuts are hand-crafted modifications to images in the training set that exploit simplicity biases in models to prevent them from capturing the semantics of the dataset. Adversarial shortcuts are also easily ignored by human perception.\n\n&#x200B;\n\nhttps://preview.redd.it/h8wc1miapm671.png?width=1850&format=png&auto=webp&s=e2132be680902b5d891fe6bd6e7bc3812c13e1e2\n\n&#x200B;\n\n&#x200B;\n\nWhile this idea is more broadly applicable, this work begins its study in the context of a well-known machine learning problem: supervised classification. Adversarial shortcuts all share a common idea: fixing a pattern for each image in a particular class encourages models to fit that pattern over anything else. It turns out that even fixing a few pixels prevents the model from fitting the semantics. Here is an example of an ImageNet-sized image with a pixel-based adversarial shortcut.\n\n&#x200B;\n\nhttps://preview.redd.it/043p4z75qm671.jpg?width=1024&format=pjpg&auto=webp&s=456cf50b5c34dbb2c6d89e45f6f4f36bfaf484b1\n\nThis is neatly illustrated by these plots of ImageNet validation and training accuracy progression: notice how, with the adversarial shortcuts applied, the training acc@1 reaches close to 100% while the validation is stuck close to 0.\n\n&#x200B;\n\n&#x200B;\n\n&#x200B;\n\nhttps://preview.redd.it/olrx44fbpm671.png?width=432&format=png&auto=webp&s=3ac13ed1092af77b402d41084272e1a65bff8e22\n\n&#x200B;\n\n&#x200B;\n\nOf course, the pixel-based pattern may be easily disrupted, so we also explore more complicated patterns: watermarks with the class index made up of MNIST digits and brightness modulations.\n\n&#x200B;\n\nhttps://preview.redd.it/73uyv9bcpm671.jpg?width=1024&format=pjpg&auto=webp&s=43e9dced8ce51329d03095d45b071576dab4f4a7\n\n&#x200B;\n\n&#x200B;\n\n&#x200B;\n\nhttps://preview.redd.it/r5gsm7tcpm671.jpg?width=1024&format=pjpg&auto=webp&s=c492d705a94e4b8eb7051a3f9c6c5a4c340f059b\n\n&#x200B;\n\nRead more - including ablation studies and comparisons to related work - in the new arXiv preprint: [https://arxiv.org/abs/2106.06654](https://arxiv.org/abs/2106.06654) by Ivan Evtimov, Ian Covert, Aditya Kusupati, and Tadayoshi Kohno.\n\n&#x200B;\n\n\\[I'm posting this for my friend /u/ivanevti (who is the first author of the paper) whose posts keep getting mysteriously removed. He'll be reading the comments here and will be able to answer any questions you might have.\\]"}, {"id": "o53b03", "title": "[P] Skim : Platform to help people skim through papers in this fast moving research world", "score": 5, "url": "https://www.reddit.com/r/MachineLearning/comments/o53b03/p_skim_platform_to_help_people_skim_through/", "author": "akshaypithadiya", "subreddit": "r/MachineLearning", "description": "\ud83c\udf89 FEATURE UPDATE \ud83c\udf89\n\nWe are extremely excited to announce a new feature! [Skim](https://skimhq.tech/) now shows Acceptance Rates of more than 50 conferences \ud83d\udcca. Take a sneak peak into the updated conference page \ud83d\udc47\n\nAlso we are sending out 100 invites this week, request an invite by filling this form: [http://tiny.cc/7zg2uz](http://tiny.cc/7zg2uz)\n\nhttps://preview.redd.it/d4znnbk77o671.png?width=1770&format=png&auto=webp&s=fb1150875b5943e0a4d4a1962528220ccfec09c7\n\nhttps://preview.redd.it/ceo474k77o671.png?width=1777&format=png&auto=webp&s=beab74cfaf1658d490446a5791a7039a9fc3731f"}, {"id": "o5i6nv", "title": "[D] Looking for a paper/result about NN trained on language doing math", "score": 0, "url": "https://www.reddit.com/r/MachineLearning/comments/o5i6nv/d_looking_for_a_paperresult_about_nn_trained_on/", "author": "Hippie_Eater", "subreddit": "r/MachineLearning", "description": "I have a vague memory of a result where a NN was trained on language (I can't recall whether it was for generation or classification etc.) that when prompted with mathematical expressions did surprizingly well. Does anyone have a link for this sort of paper/article?\n\nEdit: u/InterArtiGen correctly identified the result as section 3.9.1 from the [GPT3 results](https://arxiv.org/pdf/2005.14165.pdf#page=21)"}, {"id": "o4mlle", "title": "[D] managing compute for long running ML training jobs", "score": 94, "url": "https://www.reddit.com/r/MachineLearning/comments/o4mlle/d_managing_compute_for_long_running_ml_training/", "author": "nqnielsen", "subreddit": "r/MachineLearning", "description": "Hi r/MachineLearning\n\nI have been curious to see what the community\u2019s biggest issues are around running large training jobs. We run a modest sized GPU kubernetes cluster but there seem to be a fair gap of functionality when it comes to fair resource access (HPC style backfill, time estimates etc) and general usability to many of our scientists. \n\nFor instance some of the issues I\u2019ve seen;\n\n - GPU machines being fragmented (lots of single GPU jobs, making full width jobs very hard to land)\n - Interactive sessions being much harder than traditional HPC schedulers like SLURM\n - in the absence of a cloud parallel file system like lustre, dealing with dataset location and loading.\n\nWhat are some things you guys have run into re: resource access for training?"}, {"id": "o4ykq8", "title": "[P] Train a GAN and Keep Both Your Kidneys", "score": 8, "url": "https://www.reddit.com/r/MachineLearning/comments/o4ykq8/p_train_a_gan_and_keep_both_your_kidneys/", "author": "neverboosh", "subreddit": "r/MachineLearning", "description": "Hey guys!\n\n&#x200B;\n\nA while ago I trained StyleGAN2 to generate artificial overhead imagery on a dataset of aerial imagery of Italy which I compiled. It was a fun project and the results are kind of neat, so I thought I'd share the process.\n\n&#x200B;\n\nLink to post: [https://jakenicholasward.medium.com/train-a-gan-and-keep-both-your-kidneys-bcf672e94e81](https://jakenicholasward.medium.com/train-a-gan-and-keep-both-your-kidneys-bcf672e94e81)\n\n&#x200B;\n\nThis is also my first time writing a blog post and posting it publicly, which was arguably harder than training the actual network. I'd love some feedback on writing style and content!"}, {"id": "o4qu07", "title": "[D] Self-supervised learning in vision recent papers (DINO/Barlow Twins/PAWS etc) video interview", "score": 28, "url": "https://www.reddit.com/r/MachineLearning/comments/o4qu07/d_selfsupervised_learning_in_vision_recent_papers/", "author": "timscarfe", "subreddit": "r/MachineLearning", "description": "Dr. Ishan Misra is a Research Scientist at Facebook AI Research where he works on Computer Vision and Machine Learning. His main research interest is reducing the need for human supervision, and indeed, human knowledge in visual learning systems. He finished his PhD at the Robotics Institute at Carnegie Mellon. He has done stints at Microsoft Research, INRIA and Yale. \n\nToday though we will be focusing an exciting cluster of recent papers around unsupervised representation learning for computer vision released from FAIR. These are; DINO: Emerging Properties in Self-Supervised Vision Transformers, BARLOW TWINS: Self-Supervised Learning via Redundancy Reduction and PAWS: Semi-Supervised Learning of Visual Features by Non-Parametrically Predicting View Assignments with Support Samples. All of these papers are hot off the press, just being officially released in the last month or so. Many of you will remember PIRL: Self-Supervised Learning of Pretext-Invariant Representations which Ishan was the primary author of in 2019. \n\nYouTube: [https://youtu.be/EXJmodhu4\\_4](https://youtu.be/EXJmodhu4_4)\n\nPod:  https://anchor.fm/machinelearningstreettalk/episodes/55-Self-Supervised-Vision-Models-Dr--Ishan-Misra---FAIR-e1355js"}, {"id": "o4x5lj", "title": "[R] J\u00fcrgen Schmidhuber & Swiss AI Lab Team Boost Linear Transformers With Recurrent Fast Weight Programmers", "score": 9, "url": "https://www.reddit.com/r/MachineLearning/comments/o4x5lj/r_j\u00fcrgen_schmidhuber_swiss_ai_lab_team_boost/", "author": "Yuqing7", "subreddit": "r/MachineLearning", "description": "A research team from the Swiss AI Lab IDSIA leverages fast weight programmers (FWPs) to advance linear transformers and explores the connection between linearised transformers and outer product-based FWPs to release the power of improved FWPs. \n\nHere is a quick read: [J\u00fcrgen Schmidhuber & Swiss AI Lab Team Boost Linear Transformers With Recurrent Fast Weight Programmers.](https://syncedreview.com/2021/06/21/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-45/)\n\nThe paper *Going Beyond Linear Transformers With Recurrent Fast Weight Programmers* is on [arXiv](https://arxiv.org/abs/2106.06295)."}, {"id": "o4tsss", "title": "[D] Multiple Products Time Series with Singlevariate LSTM", "score": 13, "url": "https://www.reddit.com/r/MachineLearning/comments/o4tsss/d_multiple_products_time_series_with/", "author": "glassAlloy", "subreddit": "r/MachineLearning", "description": "**Project Description**\n\n\\- My project is to pre-order movies online (people receive them at release date) and also to sell them after release date as well.\n\n\\- I have historical for many movies.\n\n\\- Their historic sales plot looks like a bell shape like curve with the symmetric center at the middle when the movie is released.\n\n\\- I use LSTM from the following \\[project\\]([https://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/](https://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/)) with Python 3.7, Keras, TensorFlow.\n\n\\- The goal would be to train the model based on all the previouse plots without just averaging all the sales.\n\n\\- All of my historical data contains 2 parameters: 1.) sales date in the following format:  negative integers represent dates before release, 0 date when the release of the movie happening and than positive numbers maxing out at +30 to count the dates after release.\n\n&#x200B;\n\n**How to do this**\n\n\\- What I want to do is multiple products timeline with the only feature numebr of sales. With LSTM, Python 3.7, Keras, TensorFlow.\n\n\\- Prediction should be on a brand new product until day +30, Not just continuing each product.\n\n&#x200B;\n\n**What I have found so far**\n\n\\- My issue is that all the recomendation and project that I have found are for 1 product so 1 time line and maybe for multiple features (temperature, revenue, GDP at that time of the country ...) \\[example\\]([https://machinelearningmastery.com/multivariate-time-series-forecasting-lstms-keras/](https://machinelearningmastery.com/multivariate-time-series-forecasting-lstms-keras/))\n\n&#x200B;\n\n**What I have done at the end I am happy to get feedback on my solution**\n\nI have tried to connect the time scales after each other in training and if the training data have a wide variety of events than the test sets have pretty good response.\n\nWhit this methodology I am going to turn the problem to a Multivariant problem to get better results with tons of features like \"added to favorites list, viewed, movie rank...\""}, {"id": "o4ko8j", "title": "[R] How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers", "score": 74, "url": "https://www.reddit.com/r/MachineLearning/comments/o4ko8j/r_how_to_train_your_vit_data_augmentation_and/", "author": "init__27", "subreddit": "r/MachineLearning", "description": "Paper Link: [https://arxiv.org/abs/2106.10270](https://arxiv.org/abs/2106.10270)\n\nJAX Code: [https://github.com/google-research/vision\\_transformer](https://github.com/google-research/vision_transformer)\n\nPyTorch Code: [https://github.com/rwightman/pytorch-image-models](https://github.com/rwightman/pytorch-image-models)\n\nA study in order to better understand the interplay between the amount of training data, AugReg, model size and compute budget for ViTs. \n\nThe authors have trained ViT on ImageNet-21k with \"AugReg\", which either matches or outperforms their counterparts trained on the larger, but not publicly available JFT-300M dataset."}, {"id": "o4td2l", "title": "[D] Minimum number of devices for a federated learning environment", "score": 8, "url": "https://www.reddit.com/r/MachineLearning/comments/o4td2l/d_minimum_number_of_devices_for_a_federated/", "author": "dieselVeasel", "subreddit": "r/MachineLearning", "description": "Hi all, I am currently researching Federated learning on TinyML. I would like to know the minimum amount of devices you would suggest I have for researching. I'm currently working with 2 devices. Would it suffice? If not, would you suggest I emulate a few raspberry Pis? Or purchase a few extras?"}, {"id": "o4onlz", "title": "[D] What is the current SOTA for extracting a knowledge graph from images?", "score": 13, "url": "https://www.reddit.com/r/MachineLearning/comments/o4onlz/d_what_is_the_current_sota_for_extracting_a/", "author": "Shai_Meital", "subreddit": "r/MachineLearning", "description": "I am looking to get a knowledge graph of an image using some pre-trained model. \n\nWhat is the current SOTA work that has a reproducible implementation? \n\nCan you share some pointers from your experience trying to extract KG from images? \n\nThank you"}, {"id": "o4o0xw", "title": "[D]What are the current ways to compress time series data into a feature?", "score": 11, "url": "https://www.reddit.com/r/MachineLearning/comments/o4o0xw/dwhat_are_the_current_ways_to_compress_time/", "author": "DolantheMFWizard", "subreddit": "r/MachineLearning", "description": "I'm working in taking time series data over a span of 4 months and compressing it into a single feature as an input into another model. I know there are things like LSTM and GRU, but I don't know if the memory cell is large enough to hold a good latent representation. I was considering VAE, but I think converting tabular data to an image is probably tricky."}, {"id": "o4wb73", "title": "Gpt 2 - fine tuning 124 M vs 355M [D]", "score": 2, "url": "https://www.reddit.com/r/MachineLearning/comments/o4wb73/gpt_2_fine_tuning_124_m_vs_355m_d/", "author": "arkhamrising", "subreddit": "r/MachineLearning", "description": "Can someone please explain me what difference does it make to fine tune gpt2 124M and 355M.\nI get it that 355m was pretrained on larger dataset.\nBut when I am fine tuning it, how does the model size matter.?\nThanks"}, {"id": "o50nh7", "title": "[D] Shap value for LSTM model", "score": 1, "url": "https://www.reddit.com/r/MachineLearning/comments/o50nh7/d_shap_value_for_lstm_model/", "author": "Mariam_Dundua", "subreddit": "r/MachineLearning", "description": " \n\nI have lstm model named lstm\\_model  \n and I am using shap value to explain model. have tabular data.\n\n     import shap    explainer = shap.DeepExplainer(lstm_model, X_train)   shap_values = explainer.shap_values(X_test) \n\nFrom My knowledge in order to calculate the shap value, It uses a 2\\^(number of features) model. I am interesting in what is kind of algorithm it uses for each individual model.  \nThis question comes from the following fact:\n\nWhen I am plotting of effect x  \n feature on output, this effect is linear(increasing or decreasing). For example when x  \n increasing the effect increases (decreasing). What is the reason behind this?\n\nOne additional question:  \nIs it possible to use shap to plot partial dependence plot for the LSTM model?"}, {"id": "o4507g", "title": "[R] Adversarial Transferability and Beyond - Link to free zoom lecture by the authors in comments", "score": 193, "url": "https://i.redd.it/cq5711373f671.png", "author": "pinter69", "subreddit": "r/MachineLearning", "description": ""}, {"id": "o559zx", "title": "Implementation of AI into healthcare systems - pros and cons [D]", "score": 0, "url": "https://www.reddit.com/r/MachineLearning/comments/o559zx/implementation_of_ai_into_healthcare_systems_pros/", "author": "tiglet2", "subreddit": "r/MachineLearning", "description": "I am working on a school project centred around the effect of AI on the healthcare system (specifically the NHS but can be viewed in a more general context). I wanted to explore different opinions on this - do you think AI could potentially lead to a poorer quality of healthcare? There is no doubt that the use of AI in these systems will only increase but are we ready for this? I have been researching a bit about issues with AI (eg. in the context of a diagnostics tool) and have found a bit about biased algorithms which arise from a lack of input data (please correct me if this is wrong). What sorts of repercussions would an issue like this have on peoples' health?\n\nWhat do you think the greatest danger that AI poses on patients and clinicians is? And are there certain groups of people who are more at risk to these problems?\n\nAny opinions and discussions would be greatly appreciated to help me gain more insight into this topic! Thank you."}, {"id": "o3z63e", "title": "[N] Facebook AI Open Sources AugLy: A New Python Library For Data Augmentation To Develop Robust Machine Learning Models", "score": 366, "url": "https://www.reddit.com/r/MachineLearning/comments/o3z63e/n_facebook_ai_open_sources_augly_a_new_python/", "author": "ai-lover", "subreddit": "r/MachineLearning", "description": "Facebook has recently open-sourced AugLy, a new Python library that aims to help AI researchers use data augmentations to evaluate and improve the durability of their machine learning models. AugLy provides sophisticated data augmentation tools to create samples to train and test different systems.\n\nAugLy is a new open-source data augmentation library that combines audio, image, video, and text, becoming increasingly significant in several AI research fields. It offers over 100 data augmentations based on people\u2019s real-life images and videos on platforms like Facebook and Instagram.\n\nArticle: [https://www.marktechpost.com/2021/06/19/facebook-ai-open-sources-augly-a-new-python-library-for-data-augmentation-to-develop-robust-machine-learning-models/](https://www.marktechpost.com/2021/06/19/facebook-ai-open-sources-augly-a-new-python-library-for-data-augmentation-to-develop-robust-machine-learning-models/) \n\nGithub: [https://github.com/facebookresearch/AugLy](https://github.com/facebookresearch/AugLy)\n\nFacebook Blog: https://ai.facebook.com/blog/augly-a-new-data-augmentation-library-to-help-build-more-robust-ai-models/"}, {"id": "o4vfuh", "title": "[R] Disrupting Model Training with Adversarial Shortcuts", "score": 1, "url": "https://arxiv.org/abs/2106.06654", "author": "ivanevti", "subreddit": "r/MachineLearning", "description": ""}, {"id": "o4v52e", "title": "[R] Disrupting Model Training with Adversarial Shortcuts", "score": 1, "url": "https://www.reddit.com/r/MachineLearning/comments/o4v52e/r_disrupting_model_training_with_adversarial/", "author": "ivanevti", "subreddit": "r/MachineLearning", "description": "It\u2019s not always great that people can train machine learning models on your data! In this new work, we create adversarial shortcuts to prevent neural network training. Adversarial shortcuts are hand-crafted modifications to images in the training set that exploit simplicity biases in models to prevent them from capturing the semantics of the dataset. Adversarial shortcuts are also easily ignored by human perception. \n\n&#x200B;\n\nhttps://preview.redd.it/4k4ehbupem671.png?width=1850&format=png&auto=webp&s=bdfdbced226c52de7143ae99fbd1c8822b66e091\n\nWhile this idea is more broadly applicable, we begin its study in the context of a well-known machine learning problem: supervised classification. Adversarial shortcuts all share a common idea: fixing a pattern for each image in a particular class encourages models to fit that pattern over anything else. It turns out that even fixing a few pixels prevents the model from fitting the semantics. Here is an example of an ImageNet-sized image with a pixel-based adversarial shortcut. \n\nThis is neatly illustrated by these plots of ImageNet validation and training accuracy progression: notice how, with the adversarial shortcuts applied, the training acc@1 reaches close to 100% while the validation is stuck close to 0.\n\n&#x200B;\n\nhttps://preview.redd.it/4pkaynunem671.png?width=432&format=png&auto=webp&s=0add3889230dfc05ec943d999f0064a682e1b659\n\n&#x200B;\n\nOf course, the pixel-based pattern may be easily disrupted, so we also explore more complicated patterns: watermarks with the class index made up of MNIST digits and brightness modulations.\n\n&#x200B;\n\nhttps://preview.redd.it/s47ztm9vem671.jpg?width=1024&format=pjpg&auto=webp&s=2435b29833457911997b3b3114aba8afc4b2a844\n\nhttps://preview.redd.it/0sozgh9vem671.jpg?width=1024&format=pjpg&auto=webp&s=a06087f309594d1ab2bdef2bc0b939d4a051bcff\n\nRead more - including ablation studies and comparisons to related work - in our new arXiv preprint: [https://arxiv.org/abs/2106.06654](https://t.co/l6D6rysdxw?amp=1) Joint work with Ian Covert, Aditya Kusupati, and Tadayoshi Kohno."}, {"id": "o4uoyx", "title": "[P] ML Model Server", "score": 1, "url": "https://www.reddit.com/r/MachineLearning/comments/o4uoyx/p_ml_model_server/", "author": "aniketmaurya", "subreddit": "r/MachineLearning", "description": "\ud83d\udce2 You can \ud835\uddd6\ud835\uddff\ud835\uddf2\ud835\uddee\ud835\ude01\ud835\uddf2 \ud835\uddd4\ud835\udde3\ud835\udddc \ud835\uddf3\ud835\uddfc\ud835\uddff \ud835\uddd4\ud835\uddfb\ud835\ude06 \ud835\udddf\ud835\uddf2\ud835\uddee\ud835\uddff\ud835\uddfb\ud835\uddf6\ud835\uddfb\ud835\uddf4 \ud835\udde0\ud835\uddfc\ud835\uddf1\ud835\uddf2\ud835\uddf9 (ML, DL, Image Classification, NLP, Tensorflow or PyTorch) \ud835\uddfc\ud835\uddfb\ud835\uddf9\ud835\ude06 \ud835\uddf6\ud835\uddfb \ud835\uddf3\ud835\uddf2\ud835\ude04 \ud835\uddf9\ud835\uddf6\ud835\uddfb\ud835\uddf2\ud835\ude00 \ud835\uddfc\ud835\uddf3 \ud835\uddf0\ud835\uddfc\ud835\uddf1\ud835\uddf2\ud835\ude00 \ud835\ude04\ud835\uddf6\ud835\ude01\ud835\uddf5 \ud835\uddee\ud835\uddf9\ud835\uddf9 \ud835\uddfb\ud835\uddf2\ud835\ude04 \ud835\uddd6\ud835\udddb\ud835\udddc\ud835\udde7\ud835\udde5\ud835\uddd4 \ud835\udfec.\ud835\udfed.\ud835\udfec \ud83d\udd25\n\nPowered by FastAPI and Pydantic \ud83e\udd13\n\n\ud835\udddc\ud835\uddfb\ud835\ude00\ud835\ude01\ud835\uddee\ud835\uddf9\ud835\uddf9: pip install chitra==0.1.0a0\n\n\ud835\udde6\ud835\uddfc\ud835\ude02\ud835\uddff\ud835\uddf0\ud835\uddf2: [https://git.io/Jn6Hv](https://git.io/Jn6Hv)\n\n\ud835\uddd8\ud835\ude05\ud835\uddee\ud835\uddfa\ud835\uddfd\ud835\uddf9\ud835\uddf2 \ud835\ude01\ud835\uddfc \ud835\uddf5\ud835\uddf2\ud835\uddf9\ud835\uddfd \ud835\ude06\ud835\uddfc\ud835\ude02 \ud835\uddda\ud835\uddf2\ud835\ude01 \ud835\udde6\ud835\ude01\ud835\uddee\ud835\uddff\ud835\ude01\ud835\uddf2\ud835\uddf1: [https://chitra.readthedocs.io/en/latest/examples/model-server/model-server.html](https://chitra.readthedocs.io/en/latest/examples/model-server/model-server.html)"}, {"id": "o4dph1", "title": "[D] Machine Learning - WAYR (What Are You Reading) - Week 115", "score": 22, "url": "https://www.reddit.com/r/MachineLearning/comments/o4dph1/d_machine_learning_wayr_what_are_you_reading_week/", "author": "ML_WAYR_bot", "subreddit": "r/MachineLearning", "description": "This is a place to share machine learning research papers, journals, and articles that you're reading this week. If it relates to what you're researching, by all means elaborate and give us your insight, otherwise it could just be an interesting paper you've read.\n\nPlease try to provide some insight from your understanding and please don't post things which are present in wiki.\n\nPreferably you should link the arxiv page (not the PDF, you can easily access the PDF from the summary page but not the other way around) or any other pertinent links.\n\nPrevious weeks :\n\n|1-10|11-20|21-30|31-40|41-50|51-60|61-70|71-80|81-90|91-100|101-110|111-120|\n|----|-----|-----|-----|-----|-----|-----|-----|-----|------|-------|-------|\n|[Week 1](https://www.reddit.com/4qyjiq)|[Week 11](https://www.reddit.com/57xw56)|[Week 21](https://www.reddit.com/60ildf)|[Week 31](https://www.reddit.com/6s0k1u)|[Week 41](https://www.reddit.com/7tn2ax)|[Week 51](https://reddit.com/9s9el5)|[Week 61](https://reddit.com/bfsx4z)|[Week 71](https://reddit.com/d7vno3)|[Week 81](https://reddit.com/f1f0iq)|[Week 91](https://reddit.com/hlt38o)|[Week 101](https://reddit.com/k81ywb)|[Week 111](https://reddit.com/myg8sm)||||||||||\n|[Week 2](https://www.reddit.com/4s2xqm)|[Week 12](https://www.reddit.com/5acb1t)|[Week 22](https://www.reddit.com/64jwde)|[Week 32](https://www.reddit.com/72ab5y)|[Week 42](https://www.reddit.com/7wvjfk)|[Week 52](https://reddit.com/a4opot)|[Week 62](https://reddit.com/bl29ov)|[Week 72](https://reddit.com/de8h48)|[Week 82](https://reddit.com/f8fs6z)|[Week 92](https://reddit.com/hu6zq9)|[Week 102](https://reddit.com/kh27nx)|[Week 112](https://reddit.com/n8m6ds)||\n|[Week 3](https://www.reddit.com/4t7mqm)|[Week 13](https://www.reddit.com/5cwfb6)|[Week 23](https://www.reddit.com/674331)|[Week 33](https://www.reddit.com/75405d)|[Week 43](https://www.reddit.com/807ex4)|[Week 53](https://reddit.com/a8yaro)|[Week 63](https://reddit.com/bqlb3v)|[Week 73](https://reddit.com/dkox1s)|[Week 83](https://reddit.com/ffi41b)|[Week 93](https://reddit.com/iaz892)|[Week 103](https://reddit.com/kpsxtc)|[Week 113](https://reddit.com/njfsc6)||\n|[Week 4](https://www.reddit.com/4ub2kw)|[Week 14](https://www.reddit.com/5fc5mh)|[Week 24](https://www.reddit.com/68hhhb)|[Week 34](https://www.reddit.com/782js9)|[Week 44](https://reddit.com/8aluhs)|[Week 54](https://reddit.com/ad9ssz)|[Week 64](https://reddit.com/bw1jm7)|[Week 74](https://reddit.com/dr6nca)|[Week 84](https://reddit.com/fn62r1)|[Week 94](https://reddit.com/ijjcep)|[Week 104](https://reddit.com/kzevku)|[Week 114](https://reddit.com/ntu6lq)||\n|[Week 5](https://www.reddit.com/4xomf7)|[Week 15](https://www.reddit.com/5hy4ur)|[Week 25](https://www.reddit.com/69teiz)|[Week 35](https://www.reddit.com/7b0av0)|[Week 45](https://reddit.com/8tnnez)|[Week 55](https://reddit.com/ai29gi)|[Week 65](https://reddit.com/c7itkk)|[Week 75](https://reddit.com/dxshkg)|[Week 85](https://reddit.com/fvk7j6)|[Week 95](https://reddit.com/is5hj9)|[Week 105](https://reddit.com/l9lvgs)||\n|[Week 6](https://www.reddit.com/4zcyvk)|[Week 16](https://www.reddit.com/5kd6vd)|[Week 26](https://www.reddit.com/6d7nb1)|[Week 36](https://www.reddit.com/7e3fx6)|[Week 46](https://reddit.com/8x48oj)|[Week 56](https://reddit.com/ap8ctk)|[Week 66](https://reddit.com/cd7gko)|[Week 76](https://reddit.com/e4nmyk)|[Week 86](https://reddit.com/g4eavg)|[Week 96](https://reddit.com/j0xr24)|[Week 106](https://reddit.com/ljx92n)||\n|[Week 7](https://www.reddit.com/52t6mo)|[Week 17](https://www.reddit.com/5ob7dx)|[Week 27](https://www.reddit.com/6gngwc)|[Week 37](https://www.reddit.com/7hcc2c)|[Week 47](https://reddit.com/910jmh)|[Week 57](https://reddit.com/auci7c)|[Week 67](https://reddit.com/cj0kyc)|[Week 77](https://reddit.com/eb4lxk)|[Week 87](https://reddit.com/gcx3uf)|[Week 97](https://reddit.com/j9cbfs)|[Week 107](https://reddit.com/luqbxl)||\n|[Week 8](https://www.reddit.com/53heol)|[Week 18](https://www.reddit.com/5r14yd)|[Week 28](https://www.reddit.com/6jgdva)|[Week 38](https://www.reddit.com/7kgcqr)|[Week 48](https://reddit.com/94up0g)|[Week 58](https://reddit.com/azjoht)|[Week 68](https://reddit.com/cp1jex)|[Week 78](https://reddit.com/ehbfst)|[Week 88](https://reddit.com/glm6sv)|[Week 98](https://reddit.com/jhzz9v)|[Week 108](https://reddit.com/m52u5z)||\n|[Week 9](https://www.reddit.com/54kvsu)|[Week 19](https://www.reddit.com/5tt9cz)|[Week 29](https://www.reddit.com/6m9l1v)|[Week 39](https://www.reddit.com/7nayri)|[Week 49](https://reddit.com/98n2rt)|[Week 59](https://reddit.com/b50r5y)|[Week 69](https://reddit.com/cvde5a)|[Week 79](https://reddit.com/entcxy)|[Week 89](https://reddit.com/gu5t0d)|[Week 99](https://reddit.com/jqjgo2)|[Week 109](https://reddit.com/mf8m6u)||\n|[Week 10](https://www.reddit.com/56s2oa)|[Week 20](https://www.reddit.com/5wh2wb)|[Week 30](https://www.reddit.com/6p3ha7)|[Week 40](https://www.reddit.com/7qel9p)|[Week 50](https://reddit.com/9cf158)|[Week 60](https://reddit.com/bakew0)|[Week 70](https://reddit.com/d1g1k9)|[Week 80](https://reddit.com/euctyw)|[Week 90](https://reddit.com/hddf7j)|[Week 100](https://reddit.com/jz3evt)|[Week 110](https://reddit.com/moy40m)||\n\nMost upvoted papers two weeks ago:\n\n/u/nerdninja: [deep reinforcement learning platform at Facebook](https://arxiv.org/abs/1811.00260)\n\n/u/DL_updates: [ByT5: Towards a token-free future with pre-trained byte-to-byte models](https://arxiv.org/abs/2105.13626)\n\nBesides that, there are no rules, have fun."}, {"id": "o4jtf9", "title": "[P] Find Your Hacker News Doppelg\u00e4nger - Uses average word embeddings and vector similarity search", "score": 6, "url": "https://share.streamlit.io/pinecone-io/playground/hacker_news/src/server.py", "author": "gregory_k", "subreddit": "r/MachineLearning", "description": ""}, {"id": "o46gbk", "title": "[P] TorchSR, Image superresolution for pytorch", "score": 36, "url": "https://www.reddit.com/r/MachineLearning/comments/o46gbk/p_torchsr_image_superresolution_for_pytorch/", "author": "ggouvine", "subreddit": "r/MachineLearning", "description": "Hi all,\n\nI started [torchSR](https://github.com/Coloquinte/torchsr/), a package for super-resolution networks written in Pytorch. It's inspired by torchvision, and should feel familiar to torchvision users. Check it out!\n\n[Low-resolution image, super-resolution \\(x4\\) and ground truth](https://preview.redd.it/11pkdv2cif671.png?width=804&format=png&auto=webp&s=a92835a1727d2925b1317a5b324ac9b6eee2d387)\n\nAt the moment, I implemented many datasets, the most popular models (EDSR, RCAN, ...), and a number of network improvements and data augmentation method. Plus the training script for people who want to develop their own!\n\nNext steps: GAN training and multiscale networks\n\nGithub repo: [https://github.com/Coloquinte/torchsr/](https://github.com/Coloquinte/torchsr/)\n\nPython package: [https://pypi.org/project/torchsr/](https://pypi.org/project/torchsr/)"}, {"id": "o4w4cb", "title": "[D] Are machine learning models theoretically designed to make predictions about individuals?", "score": 0, "url": "https://www.reddit.com/r/MachineLearning/comments/o4w4cb/d_are_machine_learning_models_theoretically/", "author": "blueest", "subreddit": "r/MachineLearning", "description": "Are statistical models in theory able to make predictions about individuals? Suppose you have an individual with observed covariate information (x = a, y = b, z = c) : in theory, can a regression model (trained well on some data) predict the expected value of this individual's response variable?\n\nI heard today that statistical models are not designed to make predictions about individuals. They are only designed to predict the average behavior of a large group of individuals - and in theory, should not be used to make predictions about individuals.\n\nIs this correct? Does this mean that any time statistical models are used to make individual predictions, this is going against the intended use of statistical models?\n\nIf I understand correctly: this means that when a statistical model makes a prediction about an individual with observed covariate information (x = a, y = b, z = c) - it's making a prediction for the behavior of ALL individuals in the universe with observed covariate information (x = a, y = b, z = c) . Is this correct? Does this mean by definition, the idea of  making predictions for individuals is a fallacy?\n\nThanks"}, {"id": "o4bbwt", "title": "[P] GAN Prior Embedded Network for Blind Face Restoration in the Wild", "score": 9, "url": "https://github.com/yangxy/GPEN", "author": "beleeee_dat", "subreddit": "r/MachineLearning", "description": ""}, {"id": "o43wgj", "title": "[D] Deepmind's 'Reward is enough' or 'Environment is enough'", "score": 16, "url": "https://www.reddit.com/r/MachineLearning/comments/o43wgj/d_deepminds_reward_is_enough_or_environment_is/", "author": "lolo168", "subreddit": "r/MachineLearning", "description": "Deepmind's paper about 'Reward is enough'\\[1\\] only gives half the answer. We need the appropriate Environment(E)\u2014an (E) that can simulate all the feedback to the agent. Humans already live in the real world that is the (E) to learn starting from the baby. What will be World-like (E) for Human-like A.I.?\n\nWe can achieve narrow A.I. because the (E) is easy to create, such as Chess-like or Game-like. But I am not sure how we can create this World-like (E) if we can't even understand all the physics phenomena. If we found a Human-like algorithm ready for training, we need a World-like environment to provide the so-called 'Reward.' And do we have enough computation power to run the World-Like(E)?\n\nIf 'Reward is enough' for Human-Like AI(or Artificial General Intelligence). Why can't be '(World-Like)Environment is enough'?\n\nLet me know your comment :)\n\n\\[1\\] [https://deepmind.com/research/publications/Reward-is-Enough](https://deepmind.com/research/publications/Reward-is-Enough)"}, {"id": "o40t48", "title": "[P] ML Optimizers from scratch using JAX", "score": 35, "url": "https://www.reddit.com/r/MachineLearning/comments/o40t48/p_ml_optimizers_from_scratch_using_jax/", "author": "shreyansh26", "subreddit": "r/MachineLearning", "description": "Github link (includes a link to a Kaggle notebook to run it directly) -  [shreyansh26/ML-Optimizers-JAX](https://github.com/shreyansh26/ML-Optimizers-JAX)\n\nImplementations\u00a0of\u00a0some\u00a0popular\u00a0optimizers\u00a0from\u00a0scratch\u00a0for\u00a0a\u00a0simple\u00a0model\u00a0like Linear\u00a0Regression.\u00a0The\u00a0goal\u00a0of\u00a0this\u00a0project\u00a0was\u00a0to\u00a0understand\u00a0how\u00a0these\u00a0optimizers\u00a0work\u00a0under\u00a0the\u00a0hood\u00a0and\u00a0try\u00a0to\u00a0do\u00a0a\u00a0toy\u00a0implementation\u00a0myself.\u00a0I\u00a0also\u00a0use\u00a0a\u00a0bit\u00a0of\u00a0JAX\u00a0magic\u00a0to\u00a0perform\u00a0the\u00a0differentiation\u00a0of\u00a0the\u00a0loss\u00a0function\u00a0w.r.t\u00a0to\u00a0the\u00a0weights\u00a0and\u00a0the\u00a0bias\u00a0without\u00a0explicitly\u00a0writing\u00a0their\u00a0derivatives\u00a0as\u00a0a\u00a0separate\u00a0function.\u00a0\n\nThis can serve as an excellent tutorial for beginners who want to explore optimization algorithms in more detail."}, {"id": "o40tqx", "title": "[P] \ud83d\udc1d CVPR Buzz - Discover Trending Papers at CVPR 2021", "score": 20, "url": "https://www.reddit.com/r/MachineLearning/comments/o40tqx/p_cvpr_buzz_discover_trending_papers_at_cvpr_2021/", "author": "mattdeitke", "subreddit": "r/MachineLearning", "description": "Website: [https://mattdeitke.com/cvpr-buzz/](https://mattdeitke.com/cvpr-buzz/) (Best viewed on Desktop)\n\nGitHub: [https://github.com/mattdeitke/cvpr-buzz](https://github.com/mattdeitke/cvpr-buzz) | MIT License\n\n&#x200B;\n\n[Demo](https://preview.redd.it/q2jgvyznld671.png?width=3552&format=png&auto=webp&s=7b9a700cd2e84dbeaca2191a5e94bf2b646e4d91)\n\nWith CVPR 2021 starting this week, I scraped the interwebs to put together a quick site to help discover some of the papers that have been talked about and cited the most.\n\nPretty excited with how it came out! It's helped me skim through some new work that I hadn't previously seen.\n\nLet me know if you have any thoughts/suggestions. (You can also missing data on the [GitHub repo](https://github.com/mattdeitke/cvpr-buzz).)"}, {"id": "o3804y", "title": "[R] GANs N' Roses: Stable, Controllable, Diverse Image to Image Translation (works for videos too!)", "score": 1897, "url": "https://i.redd.it/3e3m6nvef5671.gif", "author": "Illustrious_Row_9971", "subreddit": "r/MachineLearning", "description": ""}, {"id": "o44dnp", "title": "[N] Image Similarity Challenge - Facebook AI", "score": 5, "url": "https://www.reddit.com/r/MachineLearning/comments/o44dnp/n_image_similarity_challenge_facebook_ai/", "author": "SubstantialRange", "subreddit": "r/MachineLearning", "description": "[https://www.drivendata.org/competitions/79/competition-image-similarity-1-dev/](https://www.drivendata.org/competitions/79/competition-image-similarity-1-dev/)\n\n&#x200B;\n\n>Welcome to the Image Similarity Challenge! In this competition, you  will be building models that help detect whether a given query image is  derived from any of the images in a large reference set.  \n>  \n>Content tracing is a crucial component on all social media platforms  today, used for such tasks as flagging misinformation and manipulative  advertising, preventing uploads of graphic violence, and enforcing  copyright protections. But when dealing with the billions of new images  generated every day on sites like Facebook, manual content moderation  just doesn't scale. They depend on algorithms to help automatically flag  or remove bad content.  \n>  \n>This competition allows you to test your skills in building a key  part of that content tracing system, and in so doing contribute to  making social media more trustworthy and safe for the people who use it."}, {"id": "o4dz50", "title": "Master thesis - Multidimensional Spectral Clustering [Project], [Research]", "score": 1, "url": "https://www.reddit.com/r/MachineLearning/comments/o4dz50/master_thesis_multidimensional_spectral/", "author": "Gibikis", "subreddit": "r/MachineLearning", "description": "I am about to choose my topic for Master thesis and I just thought about spectral clustering algorithm. I am not much into this topic and I am wondering if somebody has more experience with this and could help me.I was thinking about taking this algorithm to multidimensional level e.g. take multiple pairs of classes in huge dataset and try to take decision of clusters based on results of each pair.\n\nMaybe somebody know if there are some algorithms using this or similar approach with additional weights.Maybe you could tell me shortly if mine thinking of using this algorithm is correct and is there any sense to dive deeper into this topic. There are some ideas in my mind but I am not sure if this can give any positive clustering outcome.\n\nI am also wondering if PCA method not using similar concepts?"}, {"id": "o41bz4", "title": "[Research] PriorGrad: Improving Conditional Denoising Diffusion Models with Data-Driven Adaptive Prior", "score": 8, "url": "https://arxiv.org/pdf/2106.06406.pdf", "author": "tobyoup", "subreddit": "r/MachineLearning", "description": ""}, {"id": "o4ajvl", "title": "[D]What do you think about this \u201eDon\u2018t learn Deep Learning\u201c post?", "score": 0, "url": "https://www.reddit.com/r/MachineLearning/comments/o4ajvl/dwhat_do_you_think_about_this_dont_learn_deep/", "author": "Peter2448", "subreddit": "r/MachineLearning", "description": "Here is an interesting post I have found:\n\nhttps://towardsdatascience.com/dont-learn-deep-learning-d23485e4c1c4\n\nI am grad student by the way. I thought about taking a course on DL after two ML courses but now i am not quite sure if it wouldn\u2018t be better to take something else\u2026"}, {"id": "o48klb", "title": "PhD Studentship for UK/International Applicants in AI Driven Population Health Study", "score": 0, "url": "https://www.reddit.com/r/MachineLearning/comments/o48klb/phd_studentship_for_ukinternational_applicants_in/", "author": "smzhou", "subreddit": "r/MachineLearning", "description": "  \n\n**PhD Studentship in AI Driven Population Health Study : Improving medication verification for cancer patients**\n\n  Applications are invited for a three-year PhD studentship. The studentship will start on **1 October, 2021,** or as soon as possible after that.\n\n **Project Description**\n\nMedication errors, including those in prescribing, dispensing, or administration of a drug, are the single most preventable cause of patient harm. They have a significant impact on the efficiency of the workflow in pharmacy, raise safety concerns for patients, and result in a financial burden on the healthcare systems. Within cancer treatment, emphasis on reducing the number of medication errors has been an active research area for many years, with understanding that interdisciplinary approaches are vital to assure continuous improvement. Opportunities created by the reduction of transaction times for complex computational processes and use of machine learning to support clinical decision making, create a potential catalyst for the development of tools for reduction in medication errors.\n\nThis PhD studentship offers an exciting opportunity of exploring AI and machine learning with large clinical data sets residing within electronic health records to create methods to assure the effective use of systemic anticancer treatment (including traditional cytotoxic chemotherapy, immunotherapy, novel oral therapies etc.) without compromising patient safety. The studentship will require application of interdisciplinary skills to enable cooperation between the research, clinical, industry and patient communities in the development of a novel approach which could enhance clinical outcomes.\n\n  \n\n**Supervision Team**\n\n&#x200B;\n\n* [Professor Shang-Ming Zhou](https://eur03.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.plymouth.ac.uk%2Fstaff%2Fshang-ming-zhou&data=04%7C01%7Cshangming.zhou%40plymouth.ac.uk%7C291e7ebd5d8b4393ae2b08d933fc9309%7C5437e7eb83fb4d1abfd3bb247e061bf1%7C1%7C0%7C637597982156834666%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=NU%2B2%2FwhEbyaHJGWCUqR88H6gqpDPNcNkZ4%2FJj6vivLw%3D&reserved=0) ([shangming.zhou@plymouth.ac.uk](mailto:shangming.zhou@plymouth.ac.uk))      \n\n[https://scholar.google.com/citations?user=iWiXGWMAAAAJ&hl](https://eur03.safelinks.protection.outlook.com/?url=https%3A%2F%2Fscholar.google.com%2Fcitations%3Fuser%3DiWiXGWMAAAAJ%26hl&data=04%7C01%7Cshangming.zhou%40plymouth.ac.uk%7C291e7ebd5d8b4393ae2b08d933fc9309%7C5437e7eb83fb4d1abfd3bb247e061bf1%7C1%7C0%7C637597982156844629%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=Kn%2BzLefaEB6GOF%2B2us6NB4827AyxCrO9yhn%2B6u%2FbLBM%3D&reserved=0) \n\n&#x200B;\n\n* [Dr Edward Meinert](mailto:Dr%20Edward%20Meinert)([edward.meinert@plymouth.ac.uk](mailto:edward.meinert@plymouth.ac.uk))\n\n[https://scholar.google.com/citations?hl=en&user=7V-WsrwAAAAJ](https://eur03.safelinks.protection.outlook.com/?url=https%3A%2F%2Fscholar.google.com%2Fcitations%3Fhl%3Den%26user%3D7V-WsrwAAAAJ&data=04%7C01%7Cshangming.zhou%40plymouth.ac.uk%7C291e7ebd5d8b4393ae2b08d933fc9309%7C5437e7eb83fb4d1abfd3bb247e061bf1%7C1%7C0%7C637597982156854582%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=BXVX6qyDbxbjm1XZMtA25hWh4qKAKGbKI6riCcK3Jnw%3D&reserved=0) \n\n&#x200B;\n\n* Mrs Andrea Preston ([Andrea.Preston@uhbw.nhs.uk](mailto:Andrea.Preston@uhbw.nhs.uk))\n\nThis PhD student will be academically advised by Professor Shang-Ming Zhou and Dr Edward Meinert, research scientists with research interests in applied artificial intelligence and machine learning, computing science in health and care. The student will also be advised by Mrs Andrea Preston, a Macmillan Divisional Lead Haematology & SW Cancer Commissioning Pharmacist. This supervision team will assure the execution of a world-class PhD embedded into the wider digital health ecosystem at the University of Plymouth.\n\n**Eligibility**\n\n\u00b7 This PhD studentship is offered for UK and international applicants.\n\n\u00b7 Applicants should have: \n\n1) A first or upper second-class honours degree, and a relevant Master\u2019s qualification in Computing Science, Data Science, Statistics, Health Informatics, Medical Informatics, Bioinformatics, or any areas related;\n\n2) Interest in working with real-world problems and large data sets;\n\n3) Excellent proficiency in English and outstanding communication skills;\n\n4) Strong analytical and programming skills;\n\n5) A \u201ccan do\u201d, positive attitude with an aspiration to change the world.\n\n\u00b7 Experience in machine learning is advantageous.\n\n\u00b7 Experience in publication of peer-reviewed literature is desirable.\n\n**International Students**\n\nInternational applicants should meet the English language requirements, please see the details from the University\u2019s website [https://www.plymouth.ac.uk/international/how-to-apply/english-language-requirements](https://eur03.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.plymouth.ac.uk%2Finternational%2Fhow-to-apply%2Fenglish-language-requirements&data=04%7C01%7Cshangming.zhou%40plymouth.ac.uk%7C291e7ebd5d8b4393ae2b08d933fc9309%7C5437e7eb83fb4d1abfd3bb247e061bf1%7C1%7C0%7C637597982156854582%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=4FaDHBSrZV6ltDctnE8my9o5Ik2yuYjfYSQ9riBA%2BMo%3D&reserved=0). IELTS Academic 6.5 or above (or equivalent) with 5.5 in each individual category is commonly required by the University\u2019s Doctoral College.\n\n**How to Apply**\n\n**To apply for this position,** please visit: [https://www.plymouth.ac.uk/student-life/your-studies/research-degrees/postgraduate-research-studentships/improving-medication-verification-for-cancer-patients-a-pragmatic-ai-driven-population-health-study](https://eur03.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.plymouth.ac.uk%2Fstudent-life%2Fyour-studies%2Fresearch-degrees%2Fpostgraduate-research-studentships%2Fimproving-medication-verification-for-cancer-patients-a-pragmatic-ai-driven-population-health-study&data=04%7C01%7Cshangming.zhou%40plymouth.ac.uk%7C291e7ebd5d8b4393ae2b08d933fc9309%7C5437e7eb83fb4d1abfd3bb247e061bf1%7C1%7C0%7C637597982156864540%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=evqRV6atr9XH02c9YiKStoxc4uPVEn2DPIC2ti82Ojo%3D&reserved=0). \n\n**Please clearly state the name of the studentship that you are applying for on your Personal Statement.**\n\nA research proposal is required. \n\nPlease see: [https://www.plymouth.ac.uk/student-life/your-studies/research-degrees/applicants-and-enquirers](https://eur03.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.plymouth.ac.uk%2Fstudent-life%2Fyour-studies%2Fresearch-degrees%2Fapplicants-and-enquirers&data=04%7C01%7Cshangming.zhou%40plymouth.ac.uk%7C291e7ebd5d8b4393ae2b08d933fc9309%7C5437e7eb83fb4d1abfd3bb247e061bf1%7C1%7C0%7C637597982156864540%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=6Jh5krrUDKqNz3B%2BjWs3Hff5ItLuUCY2KxONptebPAE%3D&reserved=0) for a list of supporting documents to upload with your application. \n\n**Enquiry** \n\nIf you wish to discuss this project further informally, please contact Professor Shang-Ming Zhou ([shangming.zhou@plymouth.ac.uk](mailto:shangming.zhou@plymouth.ac.uk)), Dr Edward Meinert ([edward.meinert@plymouth.ac.uk](mailto:edward.meinert@plymouth.ac.uk)), or Mrs Andrea Preston ([Andrea.Preston@uhbw.nhs.uk](mailto:Andrea.Preston@uhbw.nhs.uk)).\n\nFor more information on the admissions process, please contact [doctoralcollege@plymouth.ac.uk](mailto:doctoralcollege@plymouth.ac.uk). \n\n**Closing Date**\n\n**The closing date for applications is 30 July 2021.** Shortlisted candidates will be invited for interview."}, {"id": "o3i4gh", "title": "[D] The PDLT document from Facebook is currently worthless as a scientific text and exemplifies the serious problem we are facing with reproducibility within the community.", "score": 73, "url": "https://www.reddit.com/r/MachineLearning/comments/o3i4gh/d_the_pdlt_document_from_facebook_is_currently/", "author": "Tsadkiel", "subreddit": "r/MachineLearning", "description": "https://ai.facebook.com/blog/advancing-ai-theory-with-a-first-principles-understanding-of-deep-neural-networks/\n\n\nFrom the preface:\n\n\n\"\rFirst and foremost, in this book we\u2019ve strived for pedagogy in every choice we\u2019ve\rmade, placing intuition above formality. This doesn\u2019t mean that calculations are incomplete or sloppy; quite the opposite, we\u2019ve tried to provide full details of every calculation\r\u2013 of which there are certainly very many \u2013 and place a particular emphasis on the tools\rneeded to carry out related calculations of interest. In fact, understanding how the calculations are done is as important as knowing their results, and thus often our pedagogical\rfocus is on the details therein.\n\n\r\nSecond, while we present the details of all our calculations, **we\u2019ve kept the experimental confirmations to the privacy of our own computerized notebooks**. Our reason\rfor this is simple: while there\u2019s much to learn from explaining a derivation, there\u2019s not\rmuch more to learn from printing a verification plot that shows two curves lying on top\rof each other. **Given the simplicity of modern deep-learning codes and the availability\rof compute, it\u2019s easy to verify any formula on your own**; we certainly have thoroughly\rchecked them all this way, so if knowledge of the existence of such plots are comforting\rto you, **know at least that they do exist on our personal and cloud-based hard drives**.\"\n\n\nI can't speak for everyone reading this.  I can only speak for myself. That said, I have a PhD in physics and I currently work as a deep learning research scientist and engineer for a major tech company. I have dedicated my entire life to the study of natural law, science, and the nature of scientific inquiry as a whole. Never once in more than two decades of my time as a graduate student, a scientist, and an engineer have I ever read a text book that began...\n\n\n\"We know you want to see the evidence.  We know you want to know what experiments we ran to appropriately test these theories.  We know you want to know to what degree of significance we have rejected various other hypotheses, and how that was done.  We know you want to know **but we aren't going to tell you.  We leave the verification of this work as an exercise to the reader. Trust  us** \"\n\n\nWhat are we even supposed to do with this?! \n\n\nNo WONDER we have an issue with reproducibility in the field when deep learning \"scientists\" are publishing documents without any actual science in it! No wonder we have literal collusion rings in our peer review process! \n\n\nThere is a full litany of mathematical assumption that form the foundation of the deductions made in this document and the author's claim to their validity stand solely on their trustworthyness.  Need I remind you all that they work for a corporation, Facebook specifically, that isn't exactly known for its honesty.\n\n\nI am reminded of the motto of the Royal Society, one of the most well established scientific institutions in modern history.\n\n\nNullius in verba\n\nWords are empty.\n\n\nTake nobody's word for it"}, {"id": "o45df5", "title": "[D] How to create a pre-training model for three different datasets?", "score": 1, "url": "https://www.reddit.com/r/MachineLearning/comments/o45df5/d_how_to_create_a_pretraining_model_for_three/", "author": "mrtac96", "subreddit": "r/MachineLearning", "description": " \n\nI know a simple way, if I a dataset, I train a CNN model and I get a pretraining model.\n\nBut if I have three different datasets, x-ray, ct-scan, MRI. I cant combine them to a single dataset as the model will learn the dataset instead of classes as data characteristics are different. So if I train the model on one dataset, then 2nd, then third. Will all the training weights of three different datasets is preserved or weights will be overwritten.\n\nOr what is the right approach to get overall embeddings"}, {"id": "o41in6", "title": "[R] Looking for Data Labelling Tool for Volumetric Multifeature Datasets", "score": 3, "url": "https://www.reddit.com/r/MachineLearning/comments/o41in6/r_looking_for_data_labelling_tool_for_volumetric/", "author": "KeineAhnungDavonViel", "subreddit": "r/MachineLearning", "description": " \n\nDear all,\n\nwe are currently working on 3D medical imagery with 6 layers of different information, a sample has a size of around 120 Gb.\n\nWe are looking for a way to label these conveniently according to the following criteria:\n\n\\- extraction of individual z-sections of the XYZ volume (we have this covered)\n\n\\- labelling of one of these z-sections with the ability to turn on- and off- the individual 6 layers while there is always a merged picture of all the activated layers visible\n\n\\- adjusting gamme & histogram of these layers individually\n\nLater on we would like to do this with volumetric data the same way, labelling structures with a volume based on several z-sections of the volume.\n\nWe couldn't find a standard solution and are evaluating we should adjust one of the open source solutions out there.\n\nDoes anyone know if any other data labelling tool has some of the above mentioned features?"}, {"id": "o3mjt9", "title": "[R] Knowledgeable or Educated Guess? Revisiting Language Models as Knowledge Bases", "score": 12, "url": "https://arxiv.org/abs/2106.09231", "author": "bert4QA", "subreddit": "r/MachineLearning", "description": ""}, {"id": "o3iolj", "title": "[D] GPEN - \"Restores\" Extremely Degraded Faces That Is Previously Impossible", "score": 20, "url": "https://youtu.be/ZzCDFA328-Q", "author": "cloud_weather", "subreddit": "r/MachineLearning", "description": ""}, {"id": "o3c9eu", "title": "[P] Playing a Neural Network's version of GTA: GAN Theft Auto", "score": 52, "url": "https://github.com/sentdex/GANTheftAuto/", "author": "KarlKani44", "subreddit": "r/MachineLearning", "description": ""}, {"id": "o3btta", "title": "[P] I built an art installation using a Nvidia Xavier and a StyleGAN trained on abstract art", "score": 57, "url": "https://v.redd.it/jokxmy8ft6671", "author": "maktattengil", "subreddit": "r/MachineLearning", "description": ""}, {"id": "o3l8vx", "title": "[R] Multi-head or Single-head? An Empirical Comparison for Transformer Training", "score": 11, "url": "https://arxiv.org/abs/2106.09650", "author": "bert4QA", "subreddit": "r/MachineLearning", "description": ""}, {"id": "o3qch1", "title": "[D]Is knowledge about Probabilistic Graphical Models a core competence in ML?", "score": 4, "url": "https://www.reddit.com/r/MachineLearning/comments/o3qch1/dis_knowledge_about_probabilistic_graphical/", "author": "Peter2448", "subreddit": "r/MachineLearning", "description": "I have read a thread a while ago where the question was whether one should take deep learning as a course in university after taking a ML course.\n\nLot of people said, that it is better to focus on the foundations and to learn more about ML in general.\n\n\"Deep Leanring isn't the universal remedy people are making it out be. Stick to the basics and learn them very well and if your job wants you to do something with DL than you can begin to learn it. \"\n\nIs the same true for PGMs?"}, {"id": "o3lo2y", "title": "[R] DouZero: Mastering DouDizhu with Self-Play Deep Reinforcement Learning", "score": 6, "url": "https://i.redd.it/t4wlmrgsf9671.gif", "author": "zdcfrank", "subreddit": "r/MachineLearning", "description": ""}, {"id": "o3bxrl", "title": "[R] The Principles of Deep Learning Theory: An Effective Theory Approach to Understanding Neural Networks", "score": 28, "url": "https://deeplearningtheory.com/PDLT.pdf", "author": "pcaversaccio", "subreddit": "r/MachineLearning", "description": ""}, {"id": "o3j6zz", "title": "[R] An enriched category theory of language: from syntax to semantics", "score": 5, "url": "https://arxiv.org/abs/2106.07890", "author": "bert4QA", "subreddit": "r/MachineLearning", "description": ""}, {"id": "o3hkd5", "title": "[D] Understanding of batch renormalization", "score": 8, "url": "https://www.reddit.com/r/MachineLearning/comments/o3hkd5/d_understanding_of_batch_renormalization/", "author": "perliczka13", "subreddit": "r/MachineLearning", "description": " \n\nHello!\n\nI was going in details through paper about batch renormalization [(arxiv link)](https://arxiv.org/abs/1702.03275).  I don't quite understand two things there. Maybe there is anyone who  faced similar issues / knows the answer and could give me some hints?\n\n1. Why  in the formula here we don't multiply by the derivative of sigma\\_b over  mi\\_b? The standard deviation is not a constant but it's a function of  the mean (mi\\_b). However the final formula for dl/dx\\_i it's the same as I  get, so the equations are overall fine.\n\n&#x200B;\n\nhttps://preview.redd.it/2ndycfmdi8671.png?width=249&format=png&auto=webp&s=1e3e83549a105b64da9a2a835d2dbd545364fad0\n\n2.  Also I have a problem with the underlined sentences. It would be nice  to know at least what should I calculate step by step to reach this  conclusions.\n\n&#x200B;\n\nhttps://preview.redd.it/oxwmwr7ei8671.png?width=991&format=png&auto=webp&s=f7989136b58e6ff336e1dcf62dcd6a6f39377ffb\n\nMy  idea is that we have to find the ortogonal basis of the kernel of a  matrix with 2 columns: p0 and p1 and later project scaled dl/dx\\_dashed  onto it? Or maybe there is better way to do it?\n\nThe next sentence is also not clear to me."}, {"id": "o3meco", "title": "[P] VkFFT now supports Discrete Cosine Transforms on GPU", "score": 4, "url": "https://www.reddit.com/r/MachineLearning/comments/o3meco/p_vkfft_now_supports_discrete_cosine_transforms/", "author": "xdtolm", "subreddit": "r/MachineLearning", "description": "Hello, I am the creator of the [VkFFT](https://github.com/DTolm/VkFFT) \\- GPU Fast Fourier Transform library for Vulkan/CUDA/HIP and OpenCL. In the latest update, I have added support for the computation of Discrete Cosine Transforms of types II, III and IV. This is a very exciting addition to what VkFFT can do as DCTs are of big importance to image processing, data compression and numerous scientific tasks. And so far there has not been a good GPU alternative to FFTW3 in this regard.\n\nVkFFT calculates DCT-II and III by mapping them to the Real-to-Complex FFT of the same size and applying needed pre and post-processing on-flight, without additional uploads/downloads. This way, VkFFT is able to achieve bandwidth-limited calculation of DCT, similar to the ordinary FFT.\n\nDCT-IV was harder to implement algorithm-wise - it is decomposed in DCT-II and DST-II sequences of half the original size. These sequences are then used to perform a single Complex-to-Complex FFT of half-size where they are used as the real and imaginary parts of a complex number. Everything is done in a single upload from global memory (with a very difficult pre/post-processing), so DCT-IV is also bandwidth-limited in VkFFT.\n\nDCTs support FP32 and FP64 precision modes and work for multidimensional systems as well. So far DCTs can be computed in a single upload configuration, which limits the max length to 8192 in FP32 for 64KB shared memory systems, but this will be improved in the future. DCT-I will also be implemented later on, as three other types of DCT are used more often and were the main target for this update.\n\nHope this will be useful to the community and feel free to ask any questions about the DCT implementation and VkFFT in general!"}, {"id": "o395fy", "title": "[Research][Discussion] Roadmap for a Research Scientist position", "score": 27, "url": "https://www.reddit.com/r/MachineLearning/comments/o395fy/researchdiscussion_roadmap_for_a_research/", "author": "bewilderedatom", "subreddit": "r/MachineLearning", "description": "Hello everyone,  I'm currently in my 4th year of a PhD in CSE(AI/ML). I'm very much interested in the position of a research scientist. Since I have hardly one year for graduation, I would like know the roadmap to achieve a good role after graduation. I would like to ask  suggestions, advices and recommendations from the experts."}, {"id": "o3kd5g", "title": "[R] Of Moments and Matching: A Game-Theoretic Framework for Closing the Imitation Gap", "score": 2, "url": "https://www.reddit.com/r/MachineLearning/comments/o3kd5g/r_of_moments_and_matching_a_gametheoretic/", "author": "0ccamsCha1nsaw", "subreddit": "r/MachineLearning", "description": "&#x200B;\n\n[When attempting to mimic an expert, a learner could learn by \\(a\\) rolling out their policy and comparing generated trajectories to expert trajectories, \\(b\\) producing actions on expert states and attempting to match action-conditionals, or \\(c\\) performing rollouts and attempting to match corrections provided by a queryable expert. We provide, for each of these settings, bounds for how well the learner can do, reduction-based algorithms for efficiently finding strong policies, and simple yet competitive practical instantiations that can scale to high-dimensional tasks.](https://preview.redd.it/8s24ad6016671.png?width=678&format=png&auto=webp&s=61d9a5518a9912cd97b8ddf5d7005e233f51e328)\n\nPaper: [https://arxiv.org/abs/2103.03236](https://arxiv.org/abs/2103.03236)\n\nCode: [https://github.com/gkswamy98/pillbox](https://github.com/gkswamy98/pillbox)\n\nVideos: [https://www.youtube.com/playlist?list=PL51kEpt5uSsbZSaGyUMsLsOoFP8-hyx0R](https://www.youtube.com/playlist?list=PL51kEpt5uSsbZSaGyUMsLsOoFP8-hyx0R)"}, {"id": "o3crak", "title": "[P] Paper recommendation on causal reinforcement learning", "score": 6, "url": "https://www.reddit.com/r/MachineLearning/comments/o3crak/p_paper_recommendation_on_causal_reinforcement/", "author": "parz01000101", "subreddit": "r/MachineLearning", "description": " \n\nHi guys,\n\nSo I'm currently a master's student in machine learning (computer vision specialisation) and it's about time I start with my thesis. The thing is, my knowledge so far has been heavily emphasized on computer-vision-related deep learning but I'm still a virgin regarding causal inference.\n\nHowever, my ultimate goal, for now, is to do PhD in causal AI, especially in causal reinforcement learning. I am very interested in AI in games as well so ideally, I would love to try to teach AI how to play basic games in a simulated world and learn through reinforcement learning as well as having the concept of causality. So I want my master thesis to be the first stepping stone towards my PhD proposal. The thing is, I'm not sure if this sounds too simple or if its sounds too much. I have read and studied about causal inference and I have done projects on reinforcement learning, but I think I'm still stuck on how to combine the two together at this point.\n\nI have been looking through papers in the past few days but I don't think I have looked deep enough yet. If anyone here has any recommended papers, interesting projects that could help me, or even ideas on how to improve my current master's proposal, I'll be extremely appreciated it. Thank you in advance."}, {"id": "o2rxd7", "title": "[R] Game On! MIT, Allen AI & Microsoft Open-Source a Suite of AI Programming Puzzles", "score": 148, "url": "https://www.reddit.com/r/MachineLearning/comments/o2rxd7/r_game_on_mit_allen_ai_microsoft_opensource_a/", "author": "Yuqing7", "subreddit": "r/MachineLearning", "description": "A research team from MIT, Allen Institute for AI and Microsoft Research open-sources Python Programming Puzzles (P3), a novel programming challenge suite that captures the essence of puzzles and can be used to teach and evaluate an AI's programming proficiency. \n\nHere is a quick read: [Game On! MIT, Allen AI & Microsoft Open-Source a Suite of AI Programming Puzzles.](https://syncedreview.com/2021/06/18/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-44/)\n\nThe paper *Programming Puzzles* is on [arXiv](https://arxiv.org/abs/2106.05784)."}, {"id": "o3m3ah", "title": "[R] Trash/Garbage dataset for waste detection", "score": 1, "url": "https://www.reddit.com/r/MachineLearning/comments/o3m3ah/r_trashgarbage_dataset_for_waste_detection/", "author": "Kananvyas2", "subreddit": "r/MachineLearning", "description": "Hi Guys, We've published Trash/Garbage dataset which can be used for waste detection and sustainibility based projects.\n\nCheck then now: [https://www.kaggle.com/dataclusterlabs/domestic-trash-garbage-dataset](https://www.kaggle.com/dataclusterlabs/domestic-trash-garbage-dataset)\n\nIf you like it, you can give upvotes into our kaggle platform. We're trying to make custom dataset and open-sourcing on Kaggle to make AI models more robust.\n\nThanks"}, {"id": "o3cowk", "title": "[R] Physion: Evaluating Physical Prediction from Vision in Humans and Machines", "score": 4, "url": "https://arxiv.org/abs/2106.08261", "author": "hardmaru", "subreddit": "r/MachineLearning", "description": ""}, {"id": "o38fgq", "title": "[R] Scalable Marginal Likelihood Estimation for Model Selection in Deep Learning", "score": 9, "url": "https://arxiv.org/abs/2104.04975", "author": "hardmaru", "subreddit": "r/MachineLearning", "description": ""}, {"id": "o2oa34", "title": "[R] Event-Based Backpropagation can compute Exact Gradients for Spiking Neural Networks", "score": 129, "url": "https://arxiv.org/abs/2009.08378", "author": "caprica", "subreddit": "r/MachineLearning", "description": ""}, {"id": "o3g30j", "title": "[R] MLP Mixer paper explained", "score": 0, "url": "https://www.reddit.com/r/MachineLearning/comments/o3g30j/r_mlp_mixer_paper_explained/", "author": "Combination-Fun", "subreddit": "r/MachineLearning", "description": "MLP Mixers is a recent [paper](https://arxiv.org/pdf/2105.01601.pdf) from Google Brain team which shows vanila neural networks designed wisely can perform as good as Convolutional Neural Networks or Transformers. We think the idea is quite promising. So we have made a video on MLP Mixer. Hope its useful:\n\n[https://youtu.be/GStFwC\\_Cr88](https://youtu.be/GStFwC_Cr88)"}, {"id": "o3c7p0", "title": "[P] KubeSurvival - Easy K8s Cost Optimization - Useful for clusters with a lot of ML training jobs & model servers", "score": 2, "url": "https://www.reddit.com/r/MachineLearning/comments/o3c7p0/p_kubesurvival_easy_k8s_cost_optimization_useful/", "author": "alongub", "subreddit": "r/MachineLearning", "description": "Just wanted to share a cool new open-source tool I built to\u00a0**significantly reduce Kubernetes compute costs**, by finding the cheapest machines that successfully run your workloads.\n\nIt's designed for clusters with a lot of ML training jobs & model servers - these can get really expensive (especially if you use GPUs).\n\nCheck it out: [https://github.com/aporia-ai/kubesurvival](https://github.com/aporia-ai/kubesurvival)"}, {"id": "o2q1h8", "title": "[R] Complex-Valued Neural Networks", "score": 52, "url": "https://www.reddit.com/r/MachineLearning/comments/o2q1h8/r_complexvalued_neural_networks/", "author": "NEGU93", "subreddit": "r/MachineLearning", "description": "So what do you think about Complex Valued Neural Networks? Can it be a new interesting field to look at? Mostly for the Signal Processing or Physics community.[https://arxiv.org/abs/2009.08340](https://arxiv.org/abs/2009.08340)"}, {"id": "o37iqb", "title": "[D] How do models like ARMA and ARIMA fare against \"sporadic memory\"?", "score": 6, "url": "https://www.reddit.com/r/MachineLearning/comments/o37iqb/d_how_do_models_like_arma_and_arima_fare_against/", "author": "ottawalanguages", "subreddit": "r/MachineLearning", "description": "Is it fair to assume that standard time series models like the ARMA and the ARIMA model are not well designed to handle \"sporadic and irregular\" memory patterns? As I understand, these models are usually used to handle data with well-behaved notions of \"trends\" and \"seasonality\" (e.g. you specify these in a given ARIMA model) . When you start to deal with more complicated and irregular patterns, do ARMA/ARIMA models tend to perform poorly? Was this the motivation for eventually moving towards neural network based models (e.g. RNN, LSTM) for time series analysis?\n\n&#x200B;\n\nThanks"}, {"id": "o3cohj", "title": "[P] I extend OpenAI CLIP to multilingual for both text and image search", "score": 0, "url": "https://unisearch.cloud/", "author": "gradientpenalty", "subreddit": "r/MachineLearning", "description": ""}, {"id": "o3chs3", "title": "[P] An experimental machine learning package for easy and fast prototyping", "score": 0, "url": "https://www.reddit.com/r/MachineLearning/comments/o3chs3/p_an_experimental_machine_learning_package_for/", "author": "nidhaloff", "subreddit": "r/MachineLearning", "description": "Hi all,\n\n[igel](https://github.com/nidhaloff/igel) is a fairly new machine learning package that allows you to create ML prototypes on the fly. You can use igel from the terminal without writing any code or from python if you want to. I tried to keep the API simple enough and flexible as possible. \n\nRecently, igel supports serving trained models by exposing a REST server (using FastAPI and uvicorn for production use). Hence, you can train, evaluate, test, generate predictions, serve and use your model in production. I'm pretty excited what users will think about the last release. I wanted to share it with you all.\n\nGithub repo: [https://github.com/nidhaloff/igel](https://github.com/nidhaloff/igel)"}, {"id": "o2gpjk", "title": "[N] AugLy: a new multimodal data augmentation lib from FB Research", "score": 190, "url": "https://www.reddit.com/r/MachineLearning/comments/o2gpjk/n_augly_a_new_multimodal_data_augmentation_lib/", "author": "Cubbee_wan", "subreddit": "r/MachineLearning", "description": "FB Research just released a new data augmentation library!\n\nIt supports audio, image, video, and text with over 100 augmentations.\n\nIt was developed with near-duplicate detection use case in mind and features unique augmentations like:\n\n> one of our augmentations takes an image or video and overlays it onto a social media interface to make it look like the image or video was screenshotted by a user on a social network like Facebook and then reshared\n\n\n[Post](https://ai.facebook.com/blog/augly-a-new-data-augmentation-library-to-help-build-more-robust-ai-models/)\n\n[Code](https://github.com/facebookresearch/AugLy)\n\nYou can find docs for each domain in respective dirs README https://github.com/facebookresearch/AugLy/tree/main/augly"}, {"id": "o2pqfa", "title": "[D]: Random Forest vs Gradient Boosting out of distribution", "score": 28, "url": "https://www.reddit.com/r/MachineLearning/comments/o2pqfa/d_random_forest_vs_gradient_boosting_out_of/", "author": "sirpopiel", "subreddit": "r/MachineLearning", "description": "Hello everyone,\n\nI'm working on a classification task where I have data from a certain company for years between 2017 and 2020. Trying to train different models (Random Forest, XgBoost, LightGBM, Catboost, Explainable Boosting Machines) on separate data with one year at a time from 2017 to 2019 and looking at the results for 2020, I see a curious behavior and I would like to understand whether it is a normal one in the literature or dependent on the particular data.\n\nIn particular, while training with data from 2019, all the boosting algorithms obtain better performances than random forest (0.78-0.79 AUC vs 0.76). This dramatically changes, when I train a model on 2017 or 2018 data for 2020. This data is slightly out of distribution, as there is for sure label shift and data is quite different. (and the learned models' feature importances/PdP are quite different between the years). But here Random Forest still learns to generalize decently (for 2020 data we have a AUC of 0.704 if trained on 2017 and 0.706 if trained on 2018), while the boosting algorithms have on average worse performance, with a big difference for LightGbm between the two datasets ( For 2017 Xgboost 0.567, LGBM, 0.565, Catboost 0.639, EBM 0.521; for 2018 Xgboost 0.661, LightGBM 0.734 (??), Catboost 0.639, EBM 0.685).\n\nProvided I have not performed extensive hyperparameter tuning and further testing and this might be a really particular case dependent on data and hyperparameters, still, I was wondering:\n\n**Does there exist some literature (I cannot find) on the robustness out of distribution of Random Forest vs Boosting algorithms which might explain this behavior?**\n\nBecause intuitively, it might make sense that the variance reduction obtained by bagging would help even out of distribution, as some learners might still have learnt something relevant, but I am not sure it is enough.\n\nPS As a sanity check I also tried with a logistic regression and a gaussian NB, which have the same consistent decrease in performance (0.7 to 0.45-0.6)."}, {"id": "o2qaoo", "title": "[R] Sleeper Agent: Scalable Hidden Trigger Backdoors for Neural Networks Trained from Scratch", "score": 22, "url": "https://arxiv.org/abs/2106.08970", "author": "hardmaru", "subreddit": "r/MachineLearning", "description": ""}, {"id": "o38thr", "title": "[D] CNN on mel spectrograms vs. WaveNet for audio recognition?", "score": 1, "url": "https://www.reddit.com/r/MachineLearning/comments/o38thr/d_cnn_on_mel_spectrograms_vs_wavenet_for_audio/", "author": "Mjjjokes", "subreddit": "r/MachineLearning", "description": "So I am creating an app in which I intend to rank a user-inputed song according to its similarity to Jimi Hendrix songs. Similar projects have been done before and, from what I've seen, the go-to approach is to train a CNN on mel spectrograms. This is also what was suggested to me. \n\nWell, today I was talking about this in an AI discord and someone told me that the CNN on mel spectrogram approach is dead, and that using WaveNet is superior. However, I have found no literature nor resources on using WaveNet for audio recognition. \n\nI'm still in the stage of needing my hand held on these types of projects. I don't have enough experience under my belt to extrapolate for this kind of thing, having only done a few machine learning projects. Is WaveNet truly superior for audio problems? Its focus seems to be on generating audio.\n\nWhat are you all's thoughts?"}, {"id": "o2j9st", "title": "[R] Efficient Deep Learning: A Survey on Making Deep Learning Models Smaller, Faster, and Better", "score": 54, "url": "https://arxiv.org/abs/2106.08962", "author": "hardmaru", "subreddit": "r/MachineLearning", "description": ""}, {"id": "o2yrx7", "title": "[R][D] LoRA: Low-Rank Adaptation of Large Language Models", "score": 5, "url": "https://arxiv.org/abs/2106.09685", "author": "hotpot_ai", "subreddit": "r/MachineLearning", "description": ""}, {"id": "o2sdls", "title": "[D] Serverless GPU?", "score": 9, "url": "https://www.reddit.com/r/MachineLearning/comments/o2sdls/d_serverless_gpu/", "author": "Daddy_Long_Legs", "subreddit": "r/MachineLearning", "description": "I need to deploy a vision model that will run a small inference job (1-5s) per request. Is there a serverless offering out there that does this?\n\nThere are other previous posts on this subreddit but I know this space is moving fast so thought it worth asking again.\n\nSo far I\u2019ve seen AWS Sagemaker kind of allows for a situation like this, but would rather not deal with all that config. Algorithmia and Nuclio are too enterprise focused. [Neuro](https://getneuro.ai) is new and looks great, but from my understanding I would still need to create a lambda instance myself that then calls neuro\u2019s servers - too indirect. Is there a total solution out there for this?\n\nIdeally something that is as straightforward to use as neuro but also handles http requests."}, {"id": "o2nba9", "title": "\"[Project]\" My own Open-Source AutoML Library", "score": 12, "url": "https://www.reddit.com/r/MachineLearning/comments/o2nba9/project_my_own_opensource_automl_library/", "author": "Danin4ik", "subreddit": "r/MachineLearning", "description": "My name is Daniel, and I'm excited to introduce you my (and another great developer) school diploma project. Fully open-source, Automated Machine Learning Library! We are beating built-in AutoML in SAP famous product.  \nGitHub repository:  \nhttps://github.com/dan0nchik/SAP-HANA-AutoML  \nWeb-application for users who don't want to code:  \nhttps://share.streamlit.io/dan0nchik/sap-hana-automl/main/web.py"}, {"id": "o2qrho", "title": "[D] can someone please explain what the \"white color shades\" mean in this picture?", "score": 3, "url": "https://www.reddit.com/r/MachineLearning/comments/o2qrho/d_can_someone_please_explain_what_the_white_color/", "author": "jj4646", "subreddit": "r/MachineLearning", "description": "[https://martin-thoma.com/images/2016/01/ml-classifiers-2.png](https://martin-thoma.com/images/2016/01/ml-classifiers-2.png)\n\nThese pictures are supposed to show the decision boundaries of different machine learning algorithms on a binary classification task. There are two classes for the response variable: \"red\" and \"blue\".\n\nShouldn't all the decision boundaries either be fully red or fully blue? What do the shades of white mean? Does this mean \"an overlapping decision boundary\"?\n\nThanks"}, {"id": "o2eoi7", "title": "[D] Anyone attended the Oxford Machine Learning Summer School?", "score": 29, "url": "https://www.reddit.com/r/MachineLearning/comments/o2eoi7/d_anyone_attended_the_oxford_machine_learning/", "author": "tmabraham", "subreddit": "r/MachineLearning", "description": "Has anyone attended the Oxford Machine Learning Summer School in the past? I got accepted and am curious what the experience is like. It's a little bit difficult for me to attend since it's occurring in a difficult time zone for more than two weeks and I am wondering if it's worth it to attend. To those who have attended, what was your experience like? Is it more than just a bunch of seminars? Are there hands-on tutorials? Also, overall, is the summer school well-known or prestigious and would it be valuable to a graduate student who is interested in a career in academia?"}, {"id": "o303nw", "title": "[P] Megatron-LM - Annotated Paper!!", "score": 1, "url": "https://www.reddit.com/r/MachineLearning/comments/o303nw/p_megatronlm_annotated_paper/", "author": "shreyansh26", "subreddit": "r/MachineLearning", "description": "Megatron-LM provides a simple yet innovative approach on how to parallelize models to train large (multi-billion parameters) language models and efficiently use GPU memory during scaling. The key point is that it does not require any major modifications (like compilation or an entirely new framework) to implement this in the existing code. It also suggested a small modification in the BERT architecture which allowed BERT to scale effectively to parameter sizes that did not perform well on before.\n\nI will focus more on papers on model scaling techniques in the upcoming few annotated papers as I want to gain more idea about this area. Check out the annotated paper below - \n\nAnnotated Paper -  [https://github.com/shreyansh26/Annotated-ML-Papers/blob/main/MegatronLM.pdf](https://github.com/shreyansh26/Annotated-ML-Papers/blob/main/MegatronLM.pdf)"}, {"id": "o28n63", "title": "[N] Introducing Distributed XGBoost Training with Ray", "score": 72, "url": "https://www.reddit.com/r/MachineLearning/comments/o28n63/n_introducing_distributed_xgboost_training_with/", "author": "kfricke", "subreddit": "r/MachineLearning", "description": "In the past months we've been working on a Ray-based backend for distributed XGBoost. Features include **multi node/multi GPU training**, advanced **fault tolerance** (e.g. **elastic training**), loading from **distributed data sources**, as well as integration with **hyperparameter optimization** framework [Ray Tune](https://docs.ray.io/en/latest/tune/index.html).\n\nSee here for the full blog post: [https://www.anyscale.com/blog/distributed-xgboost-training-with-ray](https://www.anyscale.com/blog/distributed-xgboost-training-with-ray)\n\nAnd the GitHub link here: [https://github.com/ray-project/xgboost\\_ray](https://github.com/ray-project/xgboost_ray)\n\nHappy to hear any comments!"}, {"id": "o1z8x8", "title": "[D] CVPR Panels with Richard Socher, Olga Russakovsky, HuggingFace, W&B, Anyscale, MSFT, Google, etc. What should we ask them?", "score": 222, "url": "https://www.reddit.com/r/MachineLearning/comments/o1z8x8/d_cvpr_panels_with_richard_socher_olga/", "author": "davidbun", "subreddit": "r/MachineLearning", "description": "Hi [r/MachineLearning](https://www.reddit.com/r/MachineLearning/)! :)\n\nCVPR is starting this week! We're holding two CVPR panels on the future of datasets and next-gen ML infrastructure. If you could ask one question to one of those attendees, what would it be? Comment below if you want to ask a question on the topics mentioned and we will do our best to include it in the discussion!\n\n**1. CVPR pre-game: the Future of Datasets**When: **tomorrow, June 18th at 12 pm EDT / 9 am PST**. [**Clubhouse link**](https://www.joinclubhouse.com/event/PAD2Nnen)**.**\n\n*Topic: Currently, when companies train their ML models, they focus on optimizing their models rather than the actual data. But data sits at the core of a good model. How can we be more data-centric in ML?*\n\nGuests include:\n\n* Olga Russakovsky, ImageNet Challenge co-author, Princeton,\n* Richard Socher, ImageNet co-creator, CEO You.com,\n* Jeff Boudier, HuggingFace Chief of Product,\u00a0\n* Joseph Gonzalez, UC Berkley RiseLab,\n* Jianing Wei, Google AI,\n* Siddhartha Sen, Microsoft Research.\n\n**2. CVPR Panel: Next-Gen ML Infrastructure For Computer Vision**\n\nWhen: **Monday, June 21st at 3 pm EDT / 12 pm PST**. [**Clubhouse link**](https://www.clubhouse.com/event/mWV902w6).\n\n*Topic: There are few existing solutions for data-centric ML. In this discussion, we explore tooling and infrastructure to get the most out of data*Guests include:\n\n* Tobi Knaup, CEO & Co-Founder at D2iQ,\n* Lukas Biewald, CEO at Weights & Biases,\u00a0\n* Waleed Kadous, Head of Engineering at Anyscale,\n* Glenn Jocher YOLOv5 creator, CEO at Ultralytics,\n* Tianqi Chen, CTO at OctoML,\n* Dillon Erb, CEO at Paperspace,\n* Josh Tobin, ex-Open AI, CEO at Gantry,\n* Davit Buniatyan, CEO at Activeloop,\n\nBTW, Clubhouse is now available both on [Android](https://play.google.com/store/apps/details?id=com.clubhouse.app&hl=en&gl=US) and [Apple](https://apps.apple.com/us/app/clubhouse-drop-in-audio-chat/id1503133294). If you'd like your question to be asked, please comment below and we'll pick the most voted questions/try to cover as many of them as possible. Also let me know if you'd like to attend and need an invite!\n\nSee you at CVPR and thanks for the tips!"}, {"id": "o2i730", "title": "[D] Modern Machine Learning Models for Time Series Analysis", "score": 11, "url": "https://www.reddit.com/r/MachineLearning/comments/o2i730/d_modern_machine_learning_models_for_time_series/", "author": "ottawalanguages", "subreddit": "r/MachineLearning", "description": "Does anyone know what are the most modern statistical models being used for time series analysis? I have heard of transformer and attention mechanisms models that are used for modelling sequential data - but these seem to be more relevant for modelling data from the NLP domain. When it comes to classical time series modelling (e.g. a vector of temperature measurements) : does anyone know what are some of the more modern models being used for this? I did some searching online : it seems like ARIMA style models were some of the first ones, followed by state space models/hidden markov, and the more recent ones being RNN and LSTM. \n\nAre LSTM and RNN the most modern models that are being used for classical time series problems?\n\nThanks"}, {"id": "o2mrbu", "title": "[D] Darknet YOLO v4 with CUDA 11", "score": 4, "url": "https://www.reddit.com/r/MachineLearning/comments/o2mrbu/d_darknet_yolo_v4_with_cuda_11/", "author": "simpleRetard420", "subreddit": "r/MachineLearning", "description": "Hello everyone! Has someone managed to build [yolo\\_v4](https://github.com/AlexeyAB/darknet) (darknet) with latest Cuda versions (11.2 or later)? I am stuck in dependency loop of older drivers not supporting 3000 series and newer Cuda requiring latest drivers."}, {"id": "o2k254", "title": "[R] A learning agent that acquires social norms from public sanctions in decentralized multi-agent settings", "score": 4, "url": "https://arxiv.org/abs/2106.09012", "author": "hardmaru", "subreddit": "r/MachineLearning", "description": ""}, {"id": "o2u2cm", "title": "[P]wyGPT: improved small GPT model in C++ from scratch", "score": 0, "url": "https://www.reddit.com/r/MachineLearning/comments/o2u2cm/pwygpt_improved_small_gpt_model_in_c_from_scratch/", "author": "wangyi_fudan", "subreddit": "r/MachineLearning", "description": "Dear All:\n\n&#x200B;\n\n  I coded a small GPT model for CPU from scratch in C++. See the following links:\n\n&#x200B;\n\n[https://github.com/wangyi-fudan/wyGPT](https://github.com/wangyi-fudan/wyGPT)\n\n&#x200B;\n\nThe major improvement compared to minGPT is that the MLP layer increase to 3 hidden layers.\n\n&#x200B;\n\n2012.14913 says the MLP layer is a key-value map. 2010.14075 says three hidden layer is enough. Thus now we have a \"enough key-value map\". \n\n&#x200B;\n\nAlso sin activation is used as suggested by [https://vsitzmann.github.io/siren/](https://vsitzmann.github.io/siren/)\n\n&#x200B;\n\nHave funs!"}, {"id": "o2jmrc", "title": "[D] Network science applied to images (structure/community analysis)", "score": 4, "url": "https://www.reddit.com/r/MachineLearning/comments/o2jmrc/d_network_science_applied_to_images/", "author": "vlfom", "subreddit": "r/MachineLearning", "description": "I am working with a large image dataset embedded via a CNN model with the goal of clustering the images in an unsupervised manner.\n\nThe majority of works on clustering in the embedding space tend to use classical clustering methods (e.g. K-means, agglomerative), but those are often limited in the types of structures they can detect in the data. I have also looked into Deep Clustering, but it seems like there the methods assume non-overlapping or non-hierarchical structure, which is quite a limitation. I was therefore wondering if someone tried converting the image embeddings into a graph and applying some network analysis methods on top of it, e.g. community detection ones. \n\nSurprisingly, after a week or so of research, I couldn't find papers that did so (especially on a large scale). Has someone encountered a similar idea or can help me understand why are there no or just a few works on it? Would appreciate any links or hints."}, {"id": "o1r795", "title": "[D] Schmidhuber's blog post on Kurt G\u00f6del's 1931 paper which laid the foundations of theoretical computer science, identifying fundamental limitations of algorithmic theorem proving, computing, and artificial intelligence.", "score": 263, "url": "https://www.reddit.com/r/MachineLearning/comments/o1r795/d_schmidhubers_blog_post_on_kurt_g\u00f6dels_1931/", "author": "hardmaru", "subreddit": "r/MachineLearning", "description": "link to the article: https://people.idsia.ch/~juergen/goedel-1931-founder-theoretical-computer-science-AI.html\n\n**Abstract.** In 2021, we are celebrating the 90th anniversary of Kurt G\u00f6del's groundbreaking 1931 paper which laid the foundations of theoretical computer science and the theory of artificial intelligence (AI). G\u00f6del sent shock waves through the academic community when he identified the fundamental limits of theorem proving, computing, AI, logics, and mathematics itself. This had enormous impact on science and philosophy of the 20th century. Ten years to go until the G\u00f6del centennial in 2031!"}, {"id": "o2t66t", "title": "[D] CPU AMD and GPU NVIDIA RTX for machine learning, is it OK?", "score": 0, "url": "https://www.reddit.com/r/MachineLearning/comments/o2t66t/d_cpu_amd_and_gpu_nvidia_rtx_for_machine_learning/", "author": "AcanthocephalaIcy675", "subreddit": "r/MachineLearning", "description": "I need to build a PC with the following set up and I like to know if there is any issue regarding the Machine Learning frameworks etc.\n\n* CPU:  AMD Ryzen 9 5950X Processor\n* Motherboard:  Asus ROG X570 Crosshair VIII Hero WI-FI ATX AM4 Motherboard\n* GPUs: (1). NVIDIA RTX 2070 Super Windforce. (2). NVIDIA RTX 3090\n\nUntil now, I am using Intel-based CPU and supported motherboard but as I now want to use AMD-based CPU and supported motherboard, I am a bit confused about if it is ok for machine / deep learning frameworks such as TensorFlow, PyTorch. Any compatibility issue or anything like that?"}, {"id": "o2o3cr", "title": "[D] For combating AI bias for people of different skin colors", "score": 0, "url": "https://www.reddit.com/r/MachineLearning/comments/o2o3cr/d_for_combating_ai_bias_for_people_of_different/", "author": "Competitive-Rub-1958", "subreddit": "r/MachineLearning", "description": "I was thinking about this problem, and I was wondering - what do you guys think is the best approach to solve this? Would we need to tweak network structures, or is correct dataset evaluation more important?\n\nOne of the most obvious solutions is to have a pre-trained GAN as a pre-processing step for facial images that augments the colour space in such a way that all the faces in a big dataset converge on a single randomly chosen colour - thus ensuring no bias for the model to exploit.  \nMy take is that pre-processing ensures the skin colours received by the model is the same, and then reverses it if outputting/storing results would counteract the bias in a naive form. while the diversity of datasets is important, it may not be achieved in every use-case. So I think such a method would be very generalizable to all datasets as well as reduce computation load.\n\nwhat do you all think?"}, {"id": "o26ks9", "title": "[N] IBM Releases UQ360 AI tool, An Open Source Tool To Measure Model Uncertainty", "score": 7, "url": "https://www.reddit.com/r/MachineLearning/comments/o26ks9/n_ibm_releases_uq360_ai_tool_an_open_source_tool/", "author": "ai-lover", "subreddit": "r/MachineLearning", "description": "Deep learning-based Artificial Intelligence (AI) systems have a history of generating overconfident predictions, even when they are inaccurate, which can have significant repercussions. If a self-driving car firmly misidentifies the side of a tractor as a brightly illuminated sky and refuses to brake or alert the human driver, would you prefer to travel in that? I doubt it. Self-driving cars aren\u2019t the only issue. There is a slew of additional applications where AI\u2019s ability to convey doubt is essential. For example, if a chatbot is uncertain when a pharmacy shuts and gives a false answer, a patient may not receive the medicines they require.\n\nHere\u2019s where IBM\u2019s Uncertainty Quantification 360 (UQ360) comes in to rescue the day. UQ360 allows the AI to communicate its uncertainty, making it more intellectually humble and increasing the safety of its deployment. Its goal is to provide data scientists and developers with cutting-edge algorithms for quantifying, analyzing, enhancing, and exposing the uncertainty of machine learning models.\n\nArticle: [https://www.marktechpost.com/2021/06/17/ibm-releases-uq360-ai-tool-an-open-source-tool-to-measure-model-uncertainty/?\\_ga=2.217770833.636390090.1623335762-488125022.1618729090](https://www.marktechpost.com/2021/06/17/ibm-releases-uq360-ai-tool-an-open-source-tool-to-measure-model-uncertainty/?_ga=2.217770833.636390090.1623335762-488125022.1618729090) \n\nIBM Blog: [https://www.research.ibm.com/blog/uncertainty-quantification-360](https://www.research.ibm.com/blog/uncertainty-quantification-360)"}, {"id": "o2626r", "title": "[R] Improving Language Model Behavior by Training on a Small Curated Dataset", "score": 7, "url": "https://www.reddit.com/r/MachineLearning/comments/o2626r/r_improving_language_model_behavior_by_training/", "author": "ClaudeCoulombe", "subreddit": "r/MachineLearning", "description": "Interesting research results by [OpenAI](https://openai.com/blog/improving-language-model-behavior/). It seems possible to improve the behavior of  a  GPT-3 language model  by fine tuning it  on a very small dataset. Of course, we are talking about undesirable biases (hateful, agressive, racist, sexist, etc.). They only used 80 texts. On the other hand, they neglect to say that someone can very well adjust the generated texts to favor biased texts with again a very small corpus. The [scientific paper](https://cdn.openai.com/palms.pdf) (PDF)."}]