{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import praw\n",
    "import os\n",
    "\n",
    "reddit = praw.Reddit(\"Scapper1\")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from datetime import datetime\n",
    "hot_posts = reddit.subreddit('MachineLearning').hot(limit=10)\n",
    "for post in hot_posts:\n",
    "    print(post.name, post.subreddit_name_prefixed)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "t3_odrudt r/MachineLearning\n",
      "t3_oka0v7 r/MachineLearning\n",
      "t3_ok81v4 r/MachineLearning\n",
      "t3_ok6c4k r/MachineLearning\n",
      "t3_okficy r/MachineLearning\n",
      "t3_ojt1jw r/MachineLearning\n",
      "t3_ok5h4r r/MachineLearning\n",
      "t3_ok78o8 r/MachineLearning\n",
      "t3_okamh4 r/MachineLearning\n",
      "t3_ok9wcv r/MachineLearning\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "data = []\n",
    "posts = reddit.subreddit('MachineLearning').hot(limit=20)\n",
    "for post in posts:\n",
    "    data_dict = {\n",
    "        'id': post.id,\n",
    "        'title': post.title,\n",
    "        'score': post.score,\n",
    "        'url': post.url,\n",
    "        'author': post.author, \n",
    "        'subreddit': post.subreddit_name_prefixed,\n",
    "        'description': post.selftext\n",
    "    }\n",
    "    data.append(post)\n",
    "\n",
    "data"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[Submission(id='odrudt'),\n",
       " Submission(id='ojdmza'),\n",
       " Submission(id='ojl611'),\n",
       " Submission(id='oj1w02'),\n",
       " Submission(id='oj75ae'),\n",
       " Submission(id='ojka3z'),\n",
       " Submission(id='ojrndw'),\n",
       " Submission(id='oji7fs'),\n",
       " Submission(id='ojp76r'),\n",
       " Submission(id='ojsouo'),\n",
       " Submission(id='ojhvkl'),\n",
       " Submission(id='ojj7cr'),\n",
       " Submission(id='ojfuqu'),\n",
       " Submission(id='ojqdjl'),\n",
       " Submission(id='oji890'),\n",
       " Submission(id='ojepja'),\n",
       " Submission(id='ojf2hk'),\n",
       " Submission(id='ojlqsz'),\n",
       " Submission(id='ojj1c1'),\n",
       " Submission(id='oj7u47')]"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import os \n",
    "\n",
    "def getParentDir(CurrentPath, levels = 1): \n",
    "    current_new = CurrentPath\n",
    "    for i in range(levels + 1): \n",
    "   \n",
    "        current_new = os.path.dirname(current_new) \n",
    "  \n",
    "   \n",
    "    return os.path.relpath(CurrentPath, current_new)\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "path = os.getcwd()\n",
    "parent_path = getParentDir(path, 1)\n",
    "parent_path"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'workspace/Reddit_Dsys'"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "x = os.path.join(parent_path,\"backend\")\n",
    "y = os.path.join(x, \"app\")\n",
    "\n",
    "x, y"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "('workspace/Reddit_Dsys/backend', 'workspace/Reddit_Dsys/backend/app')"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import os \n",
    "\n",
    "# function to get parent directory\n",
    "def getParentDir(CurrentPath, levels = 1): \n",
    "    current_new = CurrentPath\n",
    "    for i in range(levels + 1): \n",
    "   \n",
    "        current_new = os.path.dirname(current_new) \n",
    "  \n",
    "   \n",
    "    return os.path.relpath(CurrentPath, current_new)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "path = os.getcwd()\n",
    "print(getParentDir(path, 1))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "workspace/Reddit_Dsys\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "# function to get parent directory\n",
    "def getParentDir(CurrentPath, levels = 1): \n",
    "    current_new = CurrentPath\n",
    "    for i in range(levels + 1): \n",
    "   \n",
    "        current_new = os.path.dirname(current_new) \n",
    "  \n",
    "   \n",
    "    return os.path.relpath(CurrentPath, current_new)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "path = os.getcwd()\n",
    "print(getParentDir(path, 1))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "backend/app\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "print(getParentDir(path, -1))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ".\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def getDir_FromParent(CurrentPath, levels = 1, new_path:str=None):\n",
    "    \"\"\"Get parent path of the directory. Can also add path to it.\n",
    "    \"\"\"\n",
    "    current_new = CurrentPath\n",
    "    for i in range(levels + 1): \n",
    "        current_new = os.path.dirname(current_new) \n",
    "    \n",
    "    if new_path is None:\n",
    "        return os.path.relpath(CurrentPath, current_new)\n",
    "    return os.path.join(os.path.relpath(CurrentPath, current_new), new_path)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "save_path = getDir_FromParent(os.getcwd(), 1, 'Data')\n",
    "save_path"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'workspace/Reddit_Dsys/Data'"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "from datetime import datetime\n",
    "\n",
    "site = \"Reddit\"\n",
    "\n",
    "outputfilename=f\"output_{site}_{datetime.now().strftime('%d-%m-%Y-%H-%M')}\"\n",
    "\n",
    "os.path.join('/home/',save_path, outputfilename+\".txt\")"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'/home/workspace/Reddit_Dsys/Data/output_Reddit_14-07-2021-01-38.txt'"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def getDir_FromParent(CurrentPath, levels = 1, new_path:str=None):\n",
    "    \"\"\"Get parent path of the directory. Can also add path to it.\n",
    "    \"\"\"\n",
    "    current_new = CurrentPath\n",
    "    for i in range(levels + 1): \n",
    "        current_new = os.path.dirname(current_new) \n",
    "    \n",
    "    if new_path is None:\n",
    "        return os.path.relpath(CurrentPath, current_new)\n",
    "    return os.path.join(os.path.relpath(CurrentPath, current_new), new_path)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "datetime.utcnow()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "datetime.datetime(2021, 7, 14, 11, 56, 11, 83618)"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "str(datetime.now())"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'2021-07-14 11:59:25.900805'"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "import asyncpraw\n",
    "\n",
    "\n",
    "reddit = asyncpraw.Reddit(\"Scapper1\")\n",
    "\n",
    "data =[]\n",
    "subreddit = await reddit.subreddit('MachineLearning')\n",
    "posts = [post async for post in subreddit.hot(limit=1)]\n",
    "\n",
    "for post in posts:\n",
    "    #print(post.id, post.url)\n",
    "    data_dict = {\n",
    "        'id': post.id,\n",
    "        'title': post.title,\n",
    "        'score': post.score,\n",
    "        'url': post.url,\n",
    "        'author': post.author, \n",
    "        'subreddit': post.subreddit_name_prefixed,\n",
    "        'description': post.selftext\n",
    "    }\n",
    "    data.append(data_dict)\n",
    "\n",
    "data"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'id': 'odrudt',\n",
       "  'title': '[D] Machine Learning - WAYR (What Are You Reading) - Week 116',\n",
       "  'score': 23,\n",
       "  'url': 'https://www.reddit.com/r/MachineLearning/comments/odrudt/d_machine_learning_wayr_what_are_you_reading_week/',\n",
       "  'author': Redditor(name='ML_WAYR_bot'),\n",
       "  'subreddit': 'r/MachineLearning',\n",
       "  'description': \"This is a place to share machine learning research papers, journals, and articles that you're reading this week. If it relates to what you're researching, by all means elaborate and give us your insight, otherwise it could just be an interesting paper you've read.\\n\\nPlease try to provide some insight from your understanding and please don't post things which are present in wiki.\\n\\nPreferably you should link the arxiv page (not the PDF, you can easily access the PDF from the summary page but not the other way around) or any other pertinent links.\\n\\nPrevious weeks :\\n\\n|1-10|11-20|21-30|31-40|41-50|51-60|61-70|71-80|81-90|91-100|101-110|111-120|\\n|----|-----|-----|-----|-----|-----|-----|-----|-----|------|-------|-------|\\n|[Week 1](https://www.reddit.com/4qyjiq)|[Week 11](https://www.reddit.com/57xw56)|[Week 21](https://www.reddit.com/60ildf)|[Week 31](https://www.reddit.com/6s0k1u)|[Week 41](https://www.reddit.com/7tn2ax)|[Week 51](https://reddit.com/9s9el5)|[Week 61](https://reddit.com/bfsx4z)|[Week 71](https://reddit.com/d7vno3)|[Week 81](https://reddit.com/f1f0iq)|[Week 91](https://reddit.com/hlt38o)|[Week 101](https://reddit.com/k81ywb)|[Week 111](https://reddit.com/myg8sm)||||||||||\\n|[Week 2](https://www.reddit.com/4s2xqm)|[Week 12](https://www.reddit.com/5acb1t)|[Week 22](https://www.reddit.com/64jwde)|[Week 32](https://www.reddit.com/72ab5y)|[Week 42](https://www.reddit.com/7wvjfk)|[Week 52](https://reddit.com/a4opot)|[Week 62](https://reddit.com/bl29ov)|[Week 72](https://reddit.com/de8h48)|[Week 82](https://reddit.com/f8fs6z)|[Week 92](https://reddit.com/hu6zq9)|[Week 102](https://reddit.com/kh27nx)|[Week 112](https://reddit.com/n8m6ds)||\\n|[Week 3](https://www.reddit.com/4t7mqm)|[Week 13](https://www.reddit.com/5cwfb6)|[Week 23](https://www.reddit.com/674331)|[Week 33](https://www.reddit.com/75405d)|[Week 43](https://www.reddit.com/807ex4)|[Week 53](https://reddit.com/a8yaro)|[Week 63](https://reddit.com/bqlb3v)|[Week 73](https://reddit.com/dkox1s)|[Week 83](https://reddit.com/ffi41b)|[Week 93](https://reddit.com/iaz892)|[Week 103](https://reddit.com/kpsxtc)|[Week 113](https://reddit.com/njfsc6)||\\n|[Week 4](https://www.reddit.com/4ub2kw)|[Week 14](https://www.reddit.com/5fc5mh)|[Week 24](https://www.reddit.com/68hhhb)|[Week 34](https://www.reddit.com/782js9)|[Week 44](https://reddit.com/8aluhs)|[Week 54](https://reddit.com/ad9ssz)|[Week 64](https://reddit.com/bw1jm7)|[Week 74](https://reddit.com/dr6nca)|[Week 84](https://reddit.com/fn62r1)|[Week 94](https://reddit.com/ijjcep)|[Week 104](https://reddit.com/kzevku)|[Week 114](https://reddit.com/ntu6lq)||\\n|[Week 5](https://www.reddit.com/4xomf7)|[Week 15](https://www.reddit.com/5hy4ur)|[Week 25](https://www.reddit.com/69teiz)|[Week 35](https://www.reddit.com/7b0av0)|[Week 45](https://reddit.com/8tnnez)|[Week 55](https://reddit.com/ai29gi)|[Week 65](https://reddit.com/c7itkk)|[Week 75](https://reddit.com/dxshkg)|[Week 85](https://reddit.com/fvk7j6)|[Week 95](https://reddit.com/is5hj9)|[Week 105](https://reddit.com/l9lvgs)|[Week 115](https://reddit.com/o4dph1)||\\n|[Week 6](https://www.reddit.com/4zcyvk)|[Week 16](https://www.reddit.com/5kd6vd)|[Week 26](https://www.reddit.com/6d7nb1)|[Week 36](https://www.reddit.com/7e3fx6)|[Week 46](https://reddit.com/8x48oj)|[Week 56](https://reddit.com/ap8ctk)|[Week 66](https://reddit.com/cd7gko)|[Week 76](https://reddit.com/e4nmyk)|[Week 86](https://reddit.com/g4eavg)|[Week 96](https://reddit.com/j0xr24)|[Week 106](https://reddit.com/ljx92n)||\\n|[Week 7](https://www.reddit.com/52t6mo)|[Week 17](https://www.reddit.com/5ob7dx)|[Week 27](https://www.reddit.com/6gngwc)|[Week 37](https://www.reddit.com/7hcc2c)|[Week 47](https://reddit.com/910jmh)|[Week 57](https://reddit.com/auci7c)|[Week 67](https://reddit.com/cj0kyc)|[Week 77](https://reddit.com/eb4lxk)|[Week 87](https://reddit.com/gcx3uf)|[Week 97](https://reddit.com/j9cbfs)|[Week 107](https://reddit.com/luqbxl)||\\n|[Week 8](https://www.reddit.com/53heol)|[Week 18](https://www.reddit.com/5r14yd)|[Week 28](https://www.reddit.com/6jgdva)|[Week 38](https://www.reddit.com/7kgcqr)|[Week 48](https://reddit.com/94up0g)|[Week 58](https://reddit.com/azjoht)|[Week 68](https://reddit.com/cp1jex)|[Week 78](https://reddit.com/ehbfst)|[Week 88](https://reddit.com/glm6sv)|[Week 98](https://reddit.com/jhzz9v)|[Week 108](https://reddit.com/m52u5z)||\\n|[Week 9](https://www.reddit.com/54kvsu)|[Week 19](https://www.reddit.com/5tt9cz)|[Week 29](https://www.reddit.com/6m9l1v)|[Week 39](https://www.reddit.com/7nayri)|[Week 49](https://reddit.com/98n2rt)|[Week 59](https://reddit.com/b50r5y)|[Week 69](https://reddit.com/cvde5a)|[Week 79](https://reddit.com/entcxy)|[Week 89](https://reddit.com/gu5t0d)|[Week 99](https://reddit.com/jqjgo2)|[Week 109](https://reddit.com/mf8m6u)||\\n|[Week 10](https://www.reddit.com/56s2oa)|[Week 20](https://www.reddit.com/5wh2wb)|[Week 30](https://www.reddit.com/6p3ha7)|[Week 40](https://www.reddit.com/7qel9p)|[Week 50](https://reddit.com/9cf158)|[Week 60](https://reddit.com/bakew0)|[Week 70](https://reddit.com/d1g1k9)|[Week 80](https://reddit.com/euctyw)|[Week 90](https://reddit.com/hddf7j)|[Week 100](https://reddit.com/jz3evt)|[Week 110](https://reddit.com/moy40m)||\\n\\nMost upvoted papers two weeks ago:\\n\\n/u/NEGU93: [here](https://www.reddit.com/r/MachineLearning/comments/o2q1h8/r_complexvalued_neural_networks/)\\n\\nBesides that, there are no rules, have fun.\"},\n",
       " {'id': 'oka0v7',\n",
       "  'title': '[P] solo-learn: a library of self-supervised methods for visual representation learning',\n",
       "  'score': 102,\n",
       "  'url': 'https://www.reddit.com/r/MachineLearning/comments/oka0v7/p_sololearn_a_library_of_selfsupervised_methods/',\n",
       "  'author': Redditor(name='RobiNoob21'),\n",
       "  'subreddit': 'r/MachineLearning',\n",
       "  'description': 'Following the self-supervised trend, we have been working on a library called **solo-learn** ([https://github.com/vturrisi/solo-learn](https://github.com/vturrisi/solo-learn)) that focuses on ease of use and scalability to any available infrastructure (single-, multi- and distributed GPU/TPU machines). The library is powered by Pytorch and PyTorch Lightning, from which we inherit all the good stuff.\\n\\nWe have implemented most of the SOTA methods, such as:\\n\\n* [Barlow Twins](https://arxiv.org/abs/2103.03230)\\n* [BYOL](https://arxiv.org/abs/2006.07733)\\n* [DINO](https://arxiv.org/abs/2104.14294)\\n* [MoCo V2+](https://arxiv.org/abs/2003.04297)\\n* [NNCLR](https://arxiv.org/abs/2104.14548)\\n* [SimCLR](https://arxiv.org/abs/2002.05709) \\\\+ [Supervised Contrastive Learning](https://arxiv.org/abs/2004.11362)\\n* [SimSiam](https://arxiv.org/abs/2011.10566)\\n* [SwAV](https://arxiv.org/abs/2006.09882)\\n* [VICReg](https://arxiv.org/abs/2105.04906)\\n* [W-MSE](https://arxiv.org/abs/2007.06346)\\n\\nIn addition, apart from the extra stuff offered by PyTorch Lightning, we have implemented data loading pipelines with [Nvidia DALI](https://docs.nvidia.com/deeplearning/dali/user-guide/docs/#:~:text=The%20NVIDIA%20Data%20Loading%20Library,to%20accelerate%20deep%20learning%20applications.&text=These%20data%20processing%20pipelines%2C%20which,scalability%20of%20training%20and%20inference.), which can speed up training by up to 2x.\\n\\nWe have tuned most of the methods on CIFAR-10, CIFAR-100, ImageNet-100 and we are currently working on reproducing results on the full Imagenet. Our implementation of BYOL runs 100 epochs in less than 2 days on 2 Quadro RTX6000 and outperforms the original implementation in JAX by 0.5% on top-1 accuracy. All checkpoints are available for the community to download and use.\\n\\nTutorials and many more features are to come, like automatic TSNE/UMAP visualization, as we are continuously working on improving solo-learn. As soon as new methods will be available, we commit to implement them in the library as fast as possible. For instance, in the upcoming weeks, we will be adding [DeepCluster V2](https://arxiv.org/abs/2006.09882).\\n\\nWe would love to hear feedback and we encourage you to use and contribute if you like our project.\\n\\nVictor and Enrico'},\n",
       " {'id': 'ok81v4',\n",
       "  'title': '[N] TF, Keras and Transformers',\n",
       "  'score': 76,\n",
       "  'url': 'https://www.reddit.com/r/MachineLearning/comments/ok81v4/n_tf_keras_and_transformers/',\n",
       "  'author': Redditor(name='RocketknightHF'),\n",
       "  'subreddit': 'r/MachineLearning',\n",
       "  'description': 'Hi, Tensorflow maintainer at Hugging Face here! We\\'ve been working on a fairly extensive TF revamp for the Transformers repo, and we\\'re trying to get the word out about it.\\n\\n**What\\'s changed?**\\n\\nThe \"old\" style of writing TF with Transformers was extremely PyTorchy - you could either write eager code with your own training loop, or you could use our `TFTrainer` class, which did the boilerplate stuff for you, much like the `Trainer` class does for PyTorch. This is... fine, I guess, but it still feels like PyTorch code that\\'s been translated into TF. In particular, TF already has a trainer class that abstracts away all the boilerplate training loop stuff for you, and it\\'s called Keras. That wheel doesn\\'t need to be reinvented. Also if you make me maintain that wheel I will be sad, whereas if I can make François Chollet maintain that wheel I will be happy. No more `TFTrainer`, in other words - all of our models are Keras models, and the recommended way to deal with them is through the native Keras API. As of today, `TFTrainer` is deprecated on master, and once that makes it to the next release you\\'ll start getting warnings if you\\'re still using it.\\n\\n**What\\'s coming next?**\\n\\nWe\\'re working on other ways to make our integration with TF feel much more idiomatic - in particular, we want a way to get our datasets to automatically convert nicely to `tf.data.Dataset` objects, allowing you get really nice stuff like large dataset streaming without needing to preload the whole thing and variable-length batching to avoid wasted computation during training.\\n\\n**Sounds cool, how do I use it?**\\n\\nEasy, just `pip install transformers`! All our TF models are Keras models now - in fact, they have been for a while! I\\'ve written up [TF examples for a range of NLP tasks in the new style](https://github.com/huggingface/transformers/tree/master/examples/tensorflow) too, so you can run those as-is or adapt them to your particular needs.\\n\\n**Your example code is sloppy garbage and I don\\'t want to read a whole script of it.**\\n\\nYour criticism has been noted. Also that isn\\'t a question.\\n\\n**Okay fine, can I use this without having to read those example scripts?**\\n\\nSure! If you\\'re already a TF/Keras developer, the code below should feel very familiar to you - in fact, the only new bit will be the model loading and the tokenization needed to convert input text into integer IDs:\\n\\n    from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\\n    import tensorflow as tf\\n    \\n    model_name = \\'bert-base-cased\\'\\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\\n    model = TFAutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\\n    \\n    texts = [\"I\\'m a positive example!\", \"I\\'m a negative example!\"]\\n    labels = [1, 0]\\n    \\n    # Pad the tokenizer outputs to the same length for all samples\\n    processed_text = tokenizer(texts, padding=\\'longest\\', return_tensors=\\'tf\\')  \\n    labels = tf.convert_to_tensor(labels)\\n    \\n    opt = tf.keras.optimizers.Adam(5e-5)  # Transformers like lower learning rates\\n    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)  # Model outputs raw logits\\n    model.compile(optimizer=opt, loss=loss)\\n    \\n    model.fit(dict(processed_text), labels, epochs=3)\\n\\nBam. You just fine-tuned a massive pre-trained language model. Swap in your own data for texts and labels, and you can get superb performance on whatever task you want. Call your boss and tell them you\\'re an NLP engineer now and you want a salary review next quarter. We recommend more than two training examples, and maybe a validation set if you\\'re feeling fancy, but we can\\'t force you. Live wild and free. If you want another model, simply replace `bert-base-cased` with your model of choice; there are [plenty to choose from](https://huggingface.co/models).\\n\\n**I ran that code and it worked but there were some scary warnings!**\\n\\nThose are purely cosmetic, but they are *very* annoying, especially because it\\'s not like TF needs **more** console spam. They\\'re on my list of things that need fixing in the near future. If you\\'re reading this in a couple of months and they\\'re still there, [tweet insults at me](https://twitter.com/carrigmat) until they aren\\'t.'},\n",
       " {'id': 'ok6c4k',\n",
       "  'title': '[R] Human Evaluations No Longer the Gold Standard for NLG, Says Washington U & Allen AI Study',\n",
       "  'score': 64,\n",
       "  'url': 'https://www.reddit.com/r/MachineLearning/comments/ok6c4k/r_human_evaluations_no_longer_the_gold_standard/',\n",
       "  'author': Redditor(name='Yuqing7'),\n",
       "  'subreddit': 'r/MachineLearning',\n",
       "  'description': 'University of Washington and the Allen Institute for Artificial Intelligence researchers say human evaluations are no longer the gold standard for evaluating natural language generation models, as evaluators’ focus on surface-level text qualities degrades their ability to accurately assess current NLG models’ overall capabilities. \\n\\nHere is a quick read: [Human Evaluations No Longer the Gold Standard for NLG, Says Washington U & Allen AI Study.](https://syncedreview.com/2021/07/14/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-61/)\\n\\nThe paper *All That’s ‘Human’ Is Not Gold: Evaluating Human Evaluation of Generated Text* is on [arXiv](https://arxiv.org/abs/2107.00061).'},\n",
       " {'id': 'okficy',\n",
       "  'title': '[D] Dominance of the \"Gradient Descent\" over other algorithms',\n",
       "  'score': 7,\n",
       "  'url': 'https://www.reddit.com/r/MachineLearning/comments/okficy/d_dominance_of_the_gradient_descent_over_other/',\n",
       "  'author': Redditor(name='ottawalanguages'),\n",
       "  'subreddit': 'r/MachineLearning',\n",
       "  'description': 'Was there a main reason that led to Gradient Descent being the popular choice of optimization algorithms in the field of Machine Learning?\\n\\nI was reading this question about Gradient Descent vs Newton-Raphson (https://stats.stackexchange.com/questions/253632/why-is-newtons-method-not-widely-used-in-machine-learning). It seems here that the two main advantages that Gradient Descent has are \\n\\n1) Newton-Raphson uses the second derivative whereas Gradient Descent uses the first derivative. This reduces the number of calculations that Gradient Descent has to perform, and makes Gradient Descent faster in the long run.\\n\\n2) An individual example is shown for some function where you can see the Newton-Raphson algorithm getting stuck and identifying a local minimum point.\\n\\nOut of these 2 points, does anyone know what is the main reason that Gradient Descent became more popular than Newton-Raphson? \\n\\nWas for reasons related to speed? Or was it for reasons related to \"mathematical superiority\" (i.e. not getting stuck in local minimums)?\\n\\nThanks'},\n",
       " {'id': 'ojt1jw',\n",
       "  'title': '[R] Stanford’s AI Researchers Introduce QA-GNN Model That Jointly Reasons With Language Models And Knowledge Graphs',\n",
       "  'score': 227,\n",
       "  'url': 'https://www.reddit.com/r/MachineLearning/comments/ojt1jw/r_stanfords_ai_researchers_introduce_qagnn_model/',\n",
       "  'author': Redditor(name='techsucker'),\n",
       "  'subreddit': 'r/MachineLearning',\n",
       "  'description': 'Question-answering systems are the backbone of our digital lives. From search engines to personal assistants, we use them every day and never even realize it! For example, when you ask a question like “Where was Leonardo da Vinci born?” these intelligent computer programs need to gather background knowledge about him (Leonardo’s birthplace is Italy) as well as computational reasoning over that information in order for an answer to be generated – which will often happen automatically without us even realizing what happened behind the scenes.\\n\\nIn recent AI research, background knowledge is usually available in the form of Knowledge Graphs (KGs) and Language Models (LMs) which are pre-trained on a large set of documents. KG’s represent entities as nodes and relations between them as edges, e.g., \\\\[Leonardo da Vinci — born in – Italy\\\\]. Some other examples of KGs include\\xa0[Freebase (general-purpose facts](https://www.semanticscholar.org/paper/Freebase%3A-a-collaboratively-created-graph-database-Bollacker-Evans/1976c9eeccc7115d18a04f1e7fb5145db6b96002)),\\xa0[ConceptNet (commonsense)](https://arxiv.org/abs/1612.03975) and Examples of pre-trained LMs include [BERT (trained on Wikipedia articles and 10,000 books)](https://arxiv.org/abs/1810.04805), [RoBERTa (extending BERT),\\xa0](https://arxiv.org/abs/1907.11692)etc.\\n\\nSummary: [https://www.marktechpost.com/2021/07/13/stanfords-ai-researchers-introduce-qa-gnn-that-jointly-reasons-with-language-models-and-knowledge-graphs/](https://www.marktechpost.com/2021/07/13/stanfords-ai-researchers-introduce-qa-gnn-that-jointly-reasons-with-language-models-and-knowledge-graphs/)\\n\\nGithub: [https://github.com/michiyasunaga/qagnn](https://github.com/michiyasunaga/qagnn)\\n\\nPaper: [https://arxiv.org/pdf/2104.06378.pdf](https://arxiv.org/pdf/2104.06378.pdf)'},\n",
       " {'id': 'ok5h4r',\n",
       "  'title': '[D] Run privacy engineering workloads at the touch of an API call.',\n",
       "  'score': 10,\n",
       "  'url': 'https://www.reddit.com/r/MachineLearning/comments/ok5h4r/d_run_privacy_engineering_workloads_at_the_touch/',\n",
       "  'author': Redditor(name='alig80'),\n",
       "  'subreddit': 'r/MachineLearning',\n",
       "  'description': 'With this release, the Gretel service enables the following workloads without having to write a single line of code: [https://gretel.ai/blog/gretel-releases-beta2](https://gretel.ai/blog/gretel-releases-beta2)  \\n\\n\\n* **Create synthetic data**. Create safe shareable data or mitigate biases by balancing your data, or powering pre-production environments\\n* **Transform sensitive data and PII.** Tokenize, encrypt, or create surrogate values for sensitive information within your data.\\n* **Label and classify.** Identify 40+ entity types, create your own detectors using regular expressions.'},\n",
       " {'id': 'ok78o8',\n",
       "  'title': '[R] Reconnaissance Blind Chess - Join the NeurIPS Competition!',\n",
       "  'score': 8,\n",
       "  'url': 'https://www.reddit.com/r/MachineLearning/comments/ok78o8/r_reconnaissance_blind_chess_join_the_neurips/',\n",
       "  'author': Redditor(name='rwgardner'),\n",
       "  'subreddit': 'r/MachineLearning',\n",
       "  'description': \"Create a bot for the [NeurIPS 2021 competition in Reconnaissance Blind Chess](https://rbc.jhuapl.edu)!\\n\\nReconnaissance Blind Chess is a chess variant designed for new research in artificial intelligence. RBC includes imperfect information, long-term strategy, explicit observations, and almost no common knowledge. These features appear in real-world scenarios, and challenge even state of the art algorithms. Each player of RBC controls traditional chess pieces, but cannot directly see the locations of her opponent's pieces. Rather, she learns partial information each turn by privately sensing a 3x3 area of the board. RBC's foundation in traditional chess makes it familiar and entertaining to human players, too!\\n\\nThere is no cost to enter this tournament. Winners will receive a small monetary prize and authors of the best AIs will be invited talk about their bots at NeurIPS, the world's largest AI conference.\\n\\nReconnaissance Blind Chess is now also a part of the new Hidden Information Games Competition (HIGC - [http://higcompetition.info/](http://higcompetition.info/)) being organized by DeepMind and the Czech Technical University in Prague.\\n\\nLearn more, play a game of RBC yourself, and join our research community at [https://rbc.jhuapl.edu](https://rbc.jhuapl.edu) !\\n\\n&#x200B;\\n\\n&#x200B;\\n\\nhttps://preview.redd.it/dxmdrixx97b71.png?width=150&format=png&auto=webp&s=761dc40195db063fc926539953d6f9edb98ef9c4\\n\\n&#x200B;\\n\\nOrganized by:Johns Hopkins University Applied Physics Laboratory\\n\\nwith\\n\\nAshley J. Llorens (Microsoft Research)\\n\\nTodd W. Neller (Gettysburg College)\\n\\nRaman Arora (Johns Hopkins University)\\n\\nBo Li (University of Illinois)\\n\\nMykel J. Kochenderfer (Stanford University)\"},\n",
       " {'id': 'okamh4',\n",
       "  'title': '[D] Two months in isolation, best overall book I can take to not get rusty?',\n",
       "  'score': 2,\n",
       "  'url': 'https://www.reddit.com/r/MachineLearning/comments/okamh4/d_two_months_in_isolation_best_overall_book_i_can/',\n",
       "  'author': Redditor(name='Woodhouse_20'),\n",
       "  'subreddit': 'r/MachineLearning',\n",
       "  'description': 'I’m leaving to go to a wilderness rehab for health issues. Since Data Science/ML is my field, I don’t want to get back to interviewing having gotten rusty. Is there a book that would cover a bit of everything? I’m particularly interested in image/object recognition as well as GANs. I can probably only take a single book due to weight limits (have to carry everything on my back) and I can’t bring any technology (so no kindle/iPads with a solar charger). Any recommendations?'},\n",
       " {'id': 'ok9wcv',\n",
       "  'title': '[D] Difference between \"heuristic\" and \"metaheuristic\" algorithms',\n",
       "  'score': 3,\n",
       "  'url': 'https://www.reddit.com/r/MachineLearning/comments/ok9wcv/d_difference_between_heuristic_and_metaheuristic/',\n",
       "  'author': Redditor(name='blueest'),\n",
       "  'subreddit': 'r/MachineLearning',\n",
       "  'description': 'https://stackoverflow.com/questions/10445700/what-is-the-difference-between-heuristics-and-metaheuristics\\n\\nI have read this answer a few times, and I am still not sure what exactly is the difference between \"heuristic\" and \"metaheuristic\" algorithms.\\n\\nI always thought of \"heuristic algorithms\" being relatively simple \"man made\" logic rules (e.g. if water level rises over 2 ft, call for help).\\n\\nBut I am not quite sure what are \"metaheuristic\" algorithms. Apparently the word \\'meta\" means \"self-referential\" - how are metaheuristic algorithms \"self-referential\"?\\n\\nIn the context of optimization, is the following comparison correct?\\n\\nAn optimization algorithm like \"newton-raphson\" is \"heuristic\" because it directly evaluates the derivative some loss function, but the \"genetic algorithm\" is \"meta-heuristic\" because it can work when the derivative of the loss function can\\'t be clearly defined (e.g. non-smooth, piecewise)?\\n\\nIs my understanding correct?\\n\\nThanks'}]"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "source": [
    "import requests \n",
    "import pydantic\n",
    "import praw\n",
    "from pydantic import BaseModel\n",
    "from datetime import datetime\n",
    "from pydantic import HttpUrl\n",
    "import json"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "source": [
    "class SubredditBase(BaseModel):\n",
    "    \"\"\"\n",
    "    Subreddit resources.\n",
    "    \"\"\"\n",
    "    extracted_at: datetime\n",
    "    name: str\n",
    "    post_id: str\n",
    "    title: str\n",
    "    score: int\n",
    "    url: str\n",
    "    author: str\n",
    "    subreddit: str\n",
    "    description: str\n",
    "    created_at: datetime\n",
    "    isscore: bool = False\n",
    "\n",
    "class SubredditCreate(SubredditBase):\n",
    "    \"\"\"\n",
    "    Send subreddit post into db.\n",
    "    \"\"\"\n",
    "    extracted_at: float\n",
    "    created_at: float\n",
    "\n",
    "class SubredditIntoDB(SubredditBase):\n",
    "    \"\"\"\n",
    "    extracted_at and converted at in datetime instead of float\n",
    "    \"\"\"\n",
    "    extracted_at: datetime\n",
    "    created_at: datetime\n",
    "\n",
    "\n",
    "class Extracted_Created_Date(BaseModel):\n",
    "    \"\"\"Fixes timezone offset.\"\"\"\n",
    "    extracted_at: datetime\n",
    "    created_at: datetime"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "source": [
    "import aiohttp\n",
    "import logging\n",
    "\n",
    "\n",
    "logging.basicConfig(handlers=[logging.FileHandler(filename=\"notebooklog.log\", \n",
    "                                                 encoding='utf-8', mode='a+')],\n",
    "                    format='%(asctime)s.%(msecs)03d %(levelname)s %(module)s - %(funcName)s: %(message)s', \n",
    "                    datefmt=\"%F %A %T\", \n",
    "                    level=logging.DEBUG)\n",
    "\n",
    "async def do_post(url, data):\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        async with session.post(url, data=data) as response:\n",
    "            #output_data, output_status = await response.text(), await response.status\n",
    "            output_data = await response.text()\n",
    "            logging.INFO(f\"{output_data} with status\")\n",
    "            print(output_data)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "source": [
    "# import asyncio\n",
    "# url = \"http://localhost:8000/api/subreddit/\"\n",
    "\n",
    "\n",
    "# reddit = praw.Reddit(\"Scapper1\")\n",
    "# x = None\n",
    "# posts = reddit.subreddit('MachineLearning').hot(limit=10)\n",
    "# for post in posts:\n",
    "#     #print(post.id, post.url)\n",
    "#     data_dict = {\n",
    "#                 'extracted_at': datetime.utcnow().timestamp(),\n",
    "#                 'name': post.name,\n",
    "#                 'post_id': str(post.id),\n",
    "#                 'title': post.title,\n",
    "#                 'score': post.score,\n",
    "#                 'url': post.url,\n",
    "#                 'author': str(post.author), \n",
    "#                 'subreddit': post.subreddit_name_prefixed,\n",
    "#                 'description': post.selftext,\n",
    "#                 'created_at': post.created_utc,\n",
    "#             }\n",
    "#     data = SubredditCreate(**data_dict)\n",
    "#     data_ = {\"new_subreddit\":data.dict()}\n",
    "#     await do_post(url, data=json.dumps(data_))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "source": [
    "import httpx\n",
    "import asyncio\n",
    "url = \"http://localhost:8000/api/subreddit/\"\n",
    "\n",
    "\n",
    "reddit = praw.Reddit(\"Scapper1\")\n",
    "x = None\n",
    "posts = reddit.subreddit('MachineLearning').hot(limit=13)\n",
    "for post in posts:\n",
    "    #print(post.id, post.url)\n",
    "    data_dict = {\n",
    "                'extracted_at': datetime.utcnow().timestamp(),\n",
    "                'name': post.name,\n",
    "                'post_id': str(post.id),\n",
    "                'title': post.title,\n",
    "                'score': post.score,\n",
    "                'url': post.url,\n",
    "                'author': str(post.author), \n",
    "                'subreddit': post.subreddit_name_prefixed,\n",
    "                'description': post.selftext,\n",
    "                'created_at': post.created_utc,\n",
    "            }\n",
    "data = SubredditCreate(**data_dict)\n",
    "#data_ = json.dumps({\"new_subreddit\":data.dict()})\n",
    "#respose = httpx.post(url, data=json.dumps(data_))\n",
    "#print(respose.status_code, respose.json()['post_id'])\n",
    "    #logging.INFO(f\"{output_data} with status\")\n",
    "json.dumps(data.dict())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def do_post(data: dict):\n",
    "    logging.basicConfig(handlers=[logging.FileHandler(filename=\"postlogs.log\", \n",
    "                                                 encoding='utf-8', mode='a+')],\n",
    "                    format='%(asctime)s.%(msecs)03d %(levelname)s %(module)s - %(funcName)s: %(message)s', \n",
    "                    datefmt=\"%F %A %T\", \n",
    "                    level=logging.DEBUG)\n",
    "    response = httpx.post(url, data=json.dumps(data))\n",
    "    logging.INFO(f\"{response.json()['post_id']} with status{response.status_code}\")\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}