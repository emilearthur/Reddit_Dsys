extracted_at,name,id,title,score,url,author,subreddit,description,created_at
1626706992.532637,t3_odrudt,,[D] Machine Learning - WAYR (What Are You Reading) - Week 116,21,https://www.reddit.com/r/MachineLearning/comments/odrudt/d_machine_learning_wayr_what_are_you_reading_week/,ML_WAYR_bot,r/MachineLearning,"This is a place to share machine learning research papers, journals, and articles that you're reading this week. If it relates to what you're researching, by all means elaborate and give us your insight, otherwise it could just be an interesting paper you've read.

Please try to provide some insight from your understanding and please don't post things which are present in wiki.

Preferably you should link the arxiv page (not the PDF, you can easily access the PDF from the summary page but not the other way around) or any other pertinent links.

Previous weeks :

|1-10|11-20|21-30|31-40|41-50|51-60|61-70|71-80|81-90|91-100|101-110|111-120|
|----|-----|-----|-----|-----|-----|-----|-----|-----|------|-------|-------|
|[Week 1](https://www.reddit.com/4qyjiq)|[Week 11](https://www.reddit.com/57xw56)|[Week 21](https://www.reddit.com/60ildf)|[Week 31](https://www.reddit.com/6s0k1u)|[Week 41](https://www.reddit.com/7tn2ax)|[Week 51](https://reddit.com/9s9el5)|[Week 61](https://reddit.com/bfsx4z)|[Week 71](https://reddit.com/d7vno3)|[Week 81](https://reddit.com/f1f0iq)|[Week 91](https://reddit.com/hlt38o)|[Week 101](https://reddit.com/k81ywb)|[Week 111](https://reddit.com/myg8sm)||||||||||
|[Week 2](https://www.reddit.com/4s2xqm)|[Week 12](https://www.reddit.com/5acb1t)|[Week 22](https://www.reddit.com/64jwde)|[Week 32](https://www.reddit.com/72ab5y)|[Week 42](https://www.reddit.com/7wvjfk)|[Week 52](https://reddit.com/a4opot)|[Week 62](https://reddit.com/bl29ov)|[Week 72](https://reddit.com/de8h48)|[Week 82](https://reddit.com/f8fs6z)|[Week 92](https://reddit.com/hu6zq9)|[Week 102](https://reddit.com/kh27nx)|[Week 112](https://reddit.com/n8m6ds)||
|[Week 3](https://www.reddit.com/4t7mqm)|[Week 13](https://www.reddit.com/5cwfb6)|[Week 23](https://www.reddit.com/674331)|[Week 33](https://www.reddit.com/75405d)|[Week 43](https://www.reddit.com/807ex4)|[Week 53](https://reddit.com/a8yaro)|[Week 63](https://reddit.com/bqlb3v)|[Week 73](https://reddit.com/dkox1s)|[Week 83](https://reddit.com/ffi41b)|[Week 93](https://reddit.com/iaz892)|[Week 103](https://reddit.com/kpsxtc)|[Week 113](https://reddit.com/njfsc6)||
|[Week 4](https://www.reddit.com/4ub2kw)|[Week 14](https://www.reddit.com/5fc5mh)|[Week 24](https://www.reddit.com/68hhhb)|[Week 34](https://www.reddit.com/782js9)|[Week 44](https://reddit.com/8aluhs)|[Week 54](https://reddit.com/ad9ssz)|[Week 64](https://reddit.com/bw1jm7)|[Week 74](https://reddit.com/dr6nca)|[Week 84](https://reddit.com/fn62r1)|[Week 94](https://reddit.com/ijjcep)|[Week 104](https://reddit.com/kzevku)|[Week 114](https://reddit.com/ntu6lq)||
|[Week 5](https://www.reddit.com/4xomf7)|[Week 15](https://www.reddit.com/5hy4ur)|[Week 25](https://www.reddit.com/69teiz)|[Week 35](https://www.reddit.com/7b0av0)|[Week 45](https://reddit.com/8tnnez)|[Week 55](https://reddit.com/ai29gi)|[Week 65](https://reddit.com/c7itkk)|[Week 75](https://reddit.com/dxshkg)|[Week 85](https://reddit.com/fvk7j6)|[Week 95](https://reddit.com/is5hj9)|[Week 105](https://reddit.com/l9lvgs)|[Week 115](https://reddit.com/o4dph1)||
|[Week 6](https://www.reddit.com/4zcyvk)|[Week 16](https://www.reddit.com/5kd6vd)|[Week 26](https://www.reddit.com/6d7nb1)|[Week 36](https://www.reddit.com/7e3fx6)|[Week 46](https://reddit.com/8x48oj)|[Week 56](https://reddit.com/ap8ctk)|[Week 66](https://reddit.com/cd7gko)|[Week 76](https://reddit.com/e4nmyk)|[Week 86](https://reddit.com/g4eavg)|[Week 96](https://reddit.com/j0xr24)|[Week 106](https://reddit.com/ljx92n)||
|[Week 7](https://www.reddit.com/52t6mo)|[Week 17](https://www.reddit.com/5ob7dx)|[Week 27](https://www.reddit.com/6gngwc)|[Week 37](https://www.reddit.com/7hcc2c)|[Week 47](https://reddit.com/910jmh)|[Week 57](https://reddit.com/auci7c)|[Week 67](https://reddit.com/cj0kyc)|[Week 77](https://reddit.com/eb4lxk)|[Week 87](https://reddit.com/gcx3uf)|[Week 97](https://reddit.com/j9cbfs)|[Week 107](https://reddit.com/luqbxl)||
|[Week 8](https://www.reddit.com/53heol)|[Week 18](https://www.reddit.com/5r14yd)|[Week 28](https://www.reddit.com/6jgdva)|[Week 38](https://www.reddit.com/7kgcqr)|[Week 48](https://reddit.com/94up0g)|[Week 58](https://reddit.com/azjoht)|[Week 68](https://reddit.com/cp1jex)|[Week 78](https://reddit.com/ehbfst)|[Week 88](https://reddit.com/glm6sv)|[Week 98](https://reddit.com/jhzz9v)|[Week 108](https://reddit.com/m52u5z)||
|[Week 9](https://www.reddit.com/54kvsu)|[Week 19](https://www.reddit.com/5tt9cz)|[Week 29](https://www.reddit.com/6m9l1v)|[Week 39](https://www.reddit.com/7nayri)|[Week 49](https://reddit.com/98n2rt)|[Week 59](https://reddit.com/b50r5y)|[Week 69](https://reddit.com/cvde5a)|[Week 79](https://reddit.com/entcxy)|[Week 89](https://reddit.com/gu5t0d)|[Week 99](https://reddit.com/jqjgo2)|[Week 109](https://reddit.com/mf8m6u)||
|[Week 10](https://www.reddit.com/56s2oa)|[Week 20](https://www.reddit.com/5wh2wb)|[Week 30](https://www.reddit.com/6p3ha7)|[Week 40](https://www.reddit.com/7qel9p)|[Week 50](https://reddit.com/9cf158)|[Week 60](https://reddit.com/bakew0)|[Week 70](https://reddit.com/d1g1k9)|[Week 80](https://reddit.com/euctyw)|[Week 90](https://reddit.com/hddf7j)|[Week 100](https://reddit.com/jz3evt)|[Week 110](https://reddit.com/moy40m)||

Most upvoted papers two weeks ago:

/u/NEGU93: [here](https://www.reddit.com/r/MachineLearning/comments/o2q1h8/r_complexvalued_neural_networks/)

Besides that, there are no rules, have fun.",1625428805.0
1626706992.561816,t3_omy345,,[D] Machine Learning - WAYR (What Are You Reading) - Week 117,3,https://www.reddit.com/r/MachineLearning/comments/omy345/d_machine_learning_wayr_what_are_you_reading_week/,ML_WAYR_bot,r/MachineLearning,"This is a place to share machine learning research papers, journals, and articles that you're reading this week. If it relates to what you're researching, by all means elaborate and give us your insight, otherwise it could just be an interesting paper you've read.

Please try to provide some insight from your understanding and please don't post things which are present in wiki.

Preferably you should link the arxiv page (not the PDF, you can easily access the PDF from the summary page but not the other way around) or any other pertinent links.

Previous weeks :

|1-10|11-20|21-30|31-40|41-50|51-60|61-70|71-80|81-90|91-100|101-110|111-120|
|----|-----|-----|-----|-----|-----|-----|-----|-----|------|-------|-------|
|[Week 1](https://www.reddit.com/4qyjiq)|[Week 11](https://www.reddit.com/57xw56)|[Week 21](https://www.reddit.com/60ildf)|[Week 31](https://www.reddit.com/6s0k1u)|[Week 41](https://www.reddit.com/7tn2ax)|[Week 51](https://reddit.com/9s9el5)|[Week 61](https://reddit.com/bfsx4z)|[Week 71](https://reddit.com/d7vno3)|[Week 81](https://reddit.com/f1f0iq)|[Week 91](https://reddit.com/hlt38o)|[Week 101](https://reddit.com/k81ywb)|[Week 111](https://reddit.com/myg8sm)||||||||||
|[Week 2](https://www.reddit.com/4s2xqm)|[Week 12](https://www.reddit.com/5acb1t)|[Week 22](https://www.reddit.com/64jwde)|[Week 32](https://www.reddit.com/72ab5y)|[Week 42](https://www.reddit.com/7wvjfk)|[Week 52](https://reddit.com/a4opot)|[Week 62](https://reddit.com/bl29ov)|[Week 72](https://reddit.com/de8h48)|[Week 82](https://reddit.com/f8fs6z)|[Week 92](https://reddit.com/hu6zq9)|[Week 102](https://reddit.com/kh27nx)|[Week 112](https://reddit.com/n8m6ds)||
|[Week 3](https://www.reddit.com/4t7mqm)|[Week 13](https://www.reddit.com/5cwfb6)|[Week 23](https://www.reddit.com/674331)|[Week 33](https://www.reddit.com/75405d)|[Week 43](https://www.reddit.com/807ex4)|[Week 53](https://reddit.com/a8yaro)|[Week 63](https://reddit.com/bqlb3v)|[Week 73](https://reddit.com/dkox1s)|[Week 83](https://reddit.com/ffi41b)|[Week 93](https://reddit.com/iaz892)|[Week 103](https://reddit.com/kpsxtc)|[Week 113](https://reddit.com/njfsc6)||
|[Week 4](https://www.reddit.com/4ub2kw)|[Week 14](https://www.reddit.com/5fc5mh)|[Week 24](https://www.reddit.com/68hhhb)|[Week 34](https://www.reddit.com/782js9)|[Week 44](https://reddit.com/8aluhs)|[Week 54](https://reddit.com/ad9ssz)|[Week 64](https://reddit.com/bw1jm7)|[Week 74](https://reddit.com/dr6nca)|[Week 84](https://reddit.com/fn62r1)|[Week 94](https://reddit.com/ijjcep)|[Week 104](https://reddit.com/kzevku)|[Week 114](https://reddit.com/ntu6lq)||
|[Week 5](https://www.reddit.com/4xomf7)|[Week 15](https://www.reddit.com/5hy4ur)|[Week 25](https://www.reddit.com/69teiz)|[Week 35](https://www.reddit.com/7b0av0)|[Week 45](https://reddit.com/8tnnez)|[Week 55](https://reddit.com/ai29gi)|[Week 65](https://reddit.com/c7itkk)|[Week 75](https://reddit.com/dxshkg)|[Week 85](https://reddit.com/fvk7j6)|[Week 95](https://reddit.com/is5hj9)|[Week 105](https://reddit.com/l9lvgs)|[Week 115](https://reddit.com/o4dph1)||
|[Week 6](https://www.reddit.com/4zcyvk)|[Week 16](https://www.reddit.com/5kd6vd)|[Week 26](https://www.reddit.com/6d7nb1)|[Week 36](https://www.reddit.com/7e3fx6)|[Week 46](https://reddit.com/8x48oj)|[Week 56](https://reddit.com/ap8ctk)|[Week 66](https://reddit.com/cd7gko)|[Week 76](https://reddit.com/e4nmyk)|[Week 86](https://reddit.com/g4eavg)|[Week 96](https://reddit.com/j0xr24)|[Week 106](https://reddit.com/ljx92n)|[Week 116](https://reddit.com/odrudt)||
|[Week 7](https://www.reddit.com/52t6mo)|[Week 17](https://www.reddit.com/5ob7dx)|[Week 27](https://www.reddit.com/6gngwc)|[Week 37](https://www.reddit.com/7hcc2c)|[Week 47](https://reddit.com/910jmh)|[Week 57](https://reddit.com/auci7c)|[Week 67](https://reddit.com/cj0kyc)|[Week 77](https://reddit.com/eb4lxk)|[Week 87](https://reddit.com/gcx3uf)|[Week 97](https://reddit.com/j9cbfs)|[Week 107](https://reddit.com/luqbxl)||
|[Week 8](https://www.reddit.com/53heol)|[Week 18](https://www.reddit.com/5r14yd)|[Week 28](https://www.reddit.com/6jgdva)|[Week 38](https://www.reddit.com/7kgcqr)|[Week 48](https://reddit.com/94up0g)|[Week 58](https://reddit.com/azjoht)|[Week 68](https://reddit.com/cp1jex)|[Week 78](https://reddit.com/ehbfst)|[Week 88](https://reddit.com/glm6sv)|[Week 98](https://reddit.com/jhzz9v)|[Week 108](https://reddit.com/m52u5z)||
|[Week 9](https://www.reddit.com/54kvsu)|[Week 19](https://www.reddit.com/5tt9cz)|[Week 29](https://www.reddit.com/6m9l1v)|[Week 39](https://www.reddit.com/7nayri)|[Week 49](https://reddit.com/98n2rt)|[Week 59](https://reddit.com/b50r5y)|[Week 69](https://reddit.com/cvde5a)|[Week 79](https://reddit.com/entcxy)|[Week 89](https://reddit.com/gu5t0d)|[Week 99](https://reddit.com/jqjgo2)|[Week 109](https://reddit.com/mf8m6u)||
|[Week 10](https://www.reddit.com/56s2oa)|[Week 20](https://www.reddit.com/5wh2wb)|[Week 30](https://www.reddit.com/6p3ha7)|[Week 40](https://www.reddit.com/7qel9p)|[Week 50](https://reddit.com/9cf158)|[Week 60](https://reddit.com/bakew0)|[Week 70](https://reddit.com/d1g1k9)|[Week 80](https://reddit.com/euctyw)|[Week 90](https://reddit.com/hddf7j)|[Week 100](https://reddit.com/jz3evt)|[Week 110](https://reddit.com/moy40m)||

Most upvoted papers two weeks ago:

/u/MrUssek: [Barlow Twins: Self-Supervised Learning via Redundancy Reduction](https://arxiv.org/abs/2103.03230)

/u/TheSunilVarma: https://arxiv.org/abs/2107.00079

Besides that, there are no rules, have fun.",1626638404.0
1626706992.562285,t3_onbunj,,"[D] How did the do hyper-parameter tuning for large models like GPT-3, ERNIE etc, as they cost them millions for just training?",34,https://www.reddit.com/r/MachineLearning/comments/onbunj/d_how_did_the_do_hyperparameter_tuning_for_large/,IndieAIResearcher,r/MachineLearning,"Hi everyone,

I've worked on some deep learning, I've done some custom data training with hyperparameter tuning which taken some significant amount of time an money on cloud. I'm just wondering, how these people do hyperparameter tuning, architecture design etc, as training them costs millions. Or just it comes by experience?",1626692720.0
1626706992.562625,t3_on3lwe,,[P] Bonsai: Bayesian Optimization for Gradient Boosted Trees,115,https://www.reddit.com/r/MachineLearning/comments/on3lwe/p_bonsai_bayesian_optimization_for_gradient/,magi-1,r/MachineLearning,"[Github: Bonsai](https://github.com/magi-1/bonsai)

Here is a project I threw together when trying to streamline my model tuning process for gradient boosted trees. Take a look! This is my first time publishing a package so I am open to critiques as well as contributions, especially for feature importance visualization etc.

Bonsai is a wrapper for the XGBoost and CatBoost model training pipelines that leverages the BayesianOptimization package for computationally efficient hyperparameter tuning.

Despite being a very small package, it has access to nearly all of the configurable parameters in XGBoost and CatBoost as well as the BayesianOptimization package allowing users to specify unique objectives, metrics, parameter search ranges, and search policies. This is made possible thanks to the strong similarities between both libraries.",1626656662.0
1626706992.562962,t3_onax7l,,"Deep Learning should not be blindly applied to EVERY problem out there ""[Discussion]""",14,https://www.reddit.com/r/MachineLearning/comments/onax7l/deep_learning_should_not_be_blindly_applied_to/,erasperiko,r/MachineLearning,"Since the upsurge of Deep Learning (DL) from CNN in the image realm and from RNN for NLP, there is a clear trend towards this type of modeling technique. Mainly in context-specific areas such as finance, traffic, energy, etc, seems like every congress you attend for more than half of the publications are like this: ""So we have these datasets that have been feed to this exotic DL architecture. Our results excel the state of the art in a 0.1%. That is our contribution.""

I understand that it is an easy way to publish a paper as this kind of modeling technique is a hot topic right now. However, in my research context (which can be defined as 'traffic state prediction') can be disadvantageous. In a recent paper ([https://arxiv.org/abs/2012.02260](https://arxiv.org/abs/2012.02260)), we demonstrate that for a time series prediction approach, where data is self-descriptive (meaning that as a human being you can easily interpret the current values of the series) DL architectures do not overperform another less computational demandant algorithms such as the *all mighty* Random Forest.

The core idea behind this investigation is to shed light on an overlooked issue in this field: researchers apply modeling techniques without considering the endemic characteristics of their data collection:

* Is your data self-descriptive? 
* Is your data Euclidean?
* Are you going to benefit from the data fusion capabilities of DL?

I am only in my first research years but after reviewing lots of papers about this topic my concerns can be summarized as the above, and frankly, I do not like where this is going.",1626688474.0
1626706992.563284,t3_onfb69,,[R] Deep Learning over the Internet: Training Language Models Collaboratively,6,https://www.reddit.com/r/MachineLearning/comments/onfb69/r_deep_learning_over_the_internet_training/,justheuristic,r/MachineLearning,"&#x200B;

[\[algorithm explained in a short video\]](https://reddit.com/link/onfb69/video/g9t39hklk6c71/player)

This is a story about 40-ish enthusiasts that trained a large transformer together over the internet. The blog post explains how a careful choice of training algorithm and model architecture can let you train a large BERT-like model on GPUs from across the world, despite the slow network and unreliable hardware.

This experiment is based on a recent paper by Hugging Face & Yandex about training on a swarm of heterogeneous unreliable GPUs. They use the hivemind library for parallelism and HF datasets for streaming the data.

Blog post: [https://huggingface.co/blog/collaborative-training](https://huggingface.co/blog/collaborative-training)

Hivemind: [https://github.com/learning-at-home/hivemind](https://github.com/learning-at-home/hivemind)",1626705442.0
1626706992.563598,t3_on0ayu,,[D] What OOP/design principles/coding best-practices do you utilize on your machine learning/data science projects?,73,https://www.reddit.com/r/MachineLearning/comments/on0ayu/d_what_oopdesign_principlescoding_bestpractices/,jimmykim9001,r/MachineLearning,"I've recently realized that a lot of my ML code is just done on disorganized jupyter notebooks. Every time I run a new experiment I just change a couple of hyperparameters and then run a bunch of cells, which I realize is kind of bad practice.

I've recently found the youtube channel Arjan Codes, which goes over design patterns and I was wondering if you guys do anything similar to modularize and make your code more efficient/easier to maintain. How do you typically organize your github repo?",1626645213.0
1626706992.563926,t3_on3wqk,,"[D] ""A New Publishing Model in Computer Science"" by Yann LeCun",30,https://www.reddit.com/r/MachineLearning/comments/on3wqk/d_a_new_publishing_model_in_computer_science_by/,Flying_Scholars,r/MachineLearning,"Pamphlet (2012): [http://yann.lecun.com/ex/pamphlets/publishing-models.html](http://yann.lecun.com/ex/pamphlets/publishing-models.html)

What is preventing an open market between papers and reviewing entities from happening?",1626657752.0
1626706992.564239,t3_omrhm7,,[D] Alias-Free GAN: The non-sticky & Improved StyleGAN2!,185,https://youtu.be/j1ZY7LInN9g,farfromhome2020,r/MachineLearning,,1626618348.0
1626706992.56453,t3_ona50x,,[D] Using Value Uncertainty/Confidence as Input to ML,5,https://www.reddit.com/r/MachineLearning/comments/ona50x/d_using_value_uncertaintyconfidence_as_input_to_ml/,iamaliver,r/MachineLearning,"I am trying to figure out what has been tried about using uncertainty as a parameter to influence the uncertainty in the output or anyone's thoughts as well!

I am unsure how to properly formulate the idea and appreciate any clarifications or keywords suggestions.

Here are a few examples that hopefully capture the idea:

Example 1:

In number recognition (NMIST) the typical setup would be.  

You provide a 100x100 pixel input.  Each pixel is grayscaled and you feed it to your ML algorithm.  

Implicit in this ML algorithm is that you know for certain your input value (each pixel) with confidence of 1.  

What if you do not know with high confidence the pixel value?  ie: I say this pixel is White, but only have 0.8 confidence?  Shouldn't that influence my final output? 

Say I have an input picture where all my confidence in the pixels is really low, even though the output is 8  the confidence of the 8 should be really low.

Example 2:

I want to figure out if it will rain.  I input a lot of variables, such as ""morning cloud cover"", ""morning temperature"", ""time of year"" etc.  However the variable ""morning cloud cover"" is highly variable.  Saying 50% is not exactly true.  It is more a range of values, \[30%, 60%\].  

Maybe I can input 50% with range of 10% into the ML?

Example 3:

You have collected some data, and implicit to this data is really high noise.  Both in what you are measuring and in the final output.  You know that the uncertainty in the input for any given variable is really high.  Or perhaps the 95% confidence is very wide.  What is the best way to capture this idea?

&#x200B;

My current thoughts:

I am unclear what to search online.  ""uncertainty input parameters machine learning"" is a very crappy set of keywords to search.

I believe that even when you have a lot of samples to train with, this problem of ""uncertain inputs"" persists and should affect your final output. 

Any thoughts appreciated or even how to approach this problem.",1626684567.0
1626706992.564843,t3_onfril,,[D] BYOL explained in 5 minutes: Bootstrap Your Own Latent A New Approach to Self-Supervised Learning by Jean-Bastien Grill et al.,1,https://www.reddit.com/r/MachineLearning/comments/onfril/d_byol_explained_in_5_minutes_bootstrap_your_own/,KirillTheMunchKing,r/MachineLearning,"Is it possible to learn good enough image representations for many downstream tasks at once?

A well known approach is to use self-supervised pretraining such as state-of-the art contrastive methods that are trained to reduce the distance between representation of augmented views of the same image (positive pairs) and increasing the distance between representations of augmented views of different images. These methods need careful treatment of negative pairs, whereas **BYOL achieves higher performance than SOTA contrastive methods without using negative pairs** at all. Instead it uses two networks that learn from each other to iteratively bootstrap the representations by forcing one network to use an augmented view of an image to predict the output of the other network for a different augmented view of the same image. Sounds crazy, I know... but it actually works!

Read the [full paper digest](https://t.me/casual_gan/68) and the [blog post](https://www.casualganpapers.com/self-supervised/representation-learning/online-target-networks/2021/07/13/BYOL.html) (reading time \~5 minutes) to learn about using an online and a target networks to make self-supervised learning work without using any negative pairs during training as well as the general intuition why SSL works in the first place.

Meanwhile, check out the paper digest poster by [Casual GAN Papers](https://www.casualganpapers.com/)!

[BYOL paper poster](https://preview.redd.it/n9nusoxyo6c71.png?width=571&format=png&auto=webp&s=005ec3eb61133c41f909ab19576806530a7a6723)

\[[Full Explanation Post](https://t.me/casual_gan/68) / [Blog Post](https://www.casualganpapers.com/self-supervised/representation-learning/online-target-networks/2021/07/13/BYOL.html)\] \[[Arxiv](https://arxiv.org/pdf/2006.07733.pdf)\] \[[Code](https://github.com/deepmind/deepmind-research/tree/master/byol)\]

More recent popular computer vision paper breakdowns:

>\[[Deferred Neural Rendering](https://t.me/casual_gan/66)\]  
>  
>\[[SimCLR](https://www.casualganpapers.com/self-supervised/representation-learning/online-target-networks/2021/07/13/SimCLR.html)\]  
>  
>\[[GIRAFFE](https://t.me/casual_gan/63)\]",1626706834.0
1626706992.565162,t3_onem8j,,[R] RL for parameter space exploration,0,https://www.reddit.com/r/MachineLearning/comments/onem8j/r_rl_for_parameter_space_exploration/,mehmor,r/MachineLearning," Hi All, If we want to use RL as a search algorithm in parameter space, how should I configure it?  
Is there any best practice on this?  
Thank you in advance.",1626703220.0
1626706992.565467,t3_onekjx,,"[R] Commentary on ""Unsupervised Speech Recognition""",2,https://www.reddit.com/r/MachineLearning/comments/onekjx/r_commentary_on_unsupervised_speech_recognition/,Flying_Scholars,r/MachineLearning,"**Paper:** ""Unsupervised Speech Recognition""  
*Alexei Baevski, Wei-Ning Hsu, Alexis Conneau, Michael Auli*  
[https://arxiv.org/abs/2105.11084](https://arxiv.org/abs/2105.11084)

**Commentary:**  ""I believe that including a term to maximize mutual information between segment representations and the generated sequence of phonemes may result in better control over the textual information in the transcription...""  **Link to full commentary in the comments.**",1626703072.0
1626706992.56578,t3_on3lth,,[D] what tools/practices do you use to document your code/functions?,4,https://www.reddit.com/r/MachineLearning/comments/on3lth/d_what_toolspractices_do_you_use_to_document_your/,carlml,r/MachineLearning,"I noticed that I am writing certain functions over and over again because I didn't document very well, so finding them becomes hard, and I usually just write them again. Is there a tool that allows one to document functions? I like the way numpy or pytorch's functions are documented.

&#x200B;

I know this is not directly to machine learning, and if my post definitely does not belong here, I'll delete it.",1626656654.0
1626706992.566097,t3_om7kq3,,"[N] Stop Calling Everything AI, Machine-Learning Pioneer Says",793,https://spectrum.ieee.org/the-institute/ieee-member-news/stop-calling-everything-ai-machinelearning-pioneer-says,SquirrelOnTheDam,r/MachineLearning,,1626539514.0
1626706992.566409,t3_on56fp,,[D] Need help understanding inverting softmax layer from Michael Nielsen's book,1,https://www.reddit.com/r/MachineLearning/comments/on56fp/d_need_help_understanding_inverting_softmax_layer/,Kay0518,r/MachineLearning,"Hi,

I have a hard time understanding a softmax problem from the book:

>**Inverting the softmax layer** Suppose we have a neural network with a softmax output layer, and the activations a\^L\_j are known. Show that the corresponding weighted inputs have the form z\^L\_j = ln a\^L\_j + C for some constant C that is independent of j.

Here is how I've approached: $a\^L\_j= exp(z\_L\_j) / sum(exp(z\^L\_k)). Take the log of both sides,

ln(a\^L\_j) = z\^L\_j - ln(sum(exp(z\^L\_k))). Then, z\^L\_j = ln(a\^L\_j) + ln(sum(exp(z\^L\_k)))

The problem happens here. Why we're allowed to substitute ln(sum(exp(z\^L\_k))) for C when it includes z\^L\_j? Everyone from my research says C is independent of j so it can be C. But, doesn't that mean we have to extract e\^L\_j out of ln(sum(exp(z\^L\_k))), anda add to z\^L\_j on lhs? Can you please give me a insight into this problem?",1626662497.0
1626706992.566716,t3_on7etm,,[D] How is the accuracy of a computer vision algorithm measured?,0,https://www.reddit.com/r/MachineLearning/comments/on7etm/d_how_is_the_accuracy_of_a_computer_vision/,Master-Cantaloupe750,r/MachineLearning,"I'm reading a lot of CV papers recently and they sometimes will provide a chart where they compared the accuracy and performance of the algorithms, but they never revealed their method of calculating the accuracy. Supposed an algorithm claims it has ""95% accuracy"" with a performance of 50 FPS, how exactly is the 95% calculated, is there a specific standardized method where everyone is using it to calculate accuracy to be fair?",1626671474.0
1626706992.567021,t3_omwt3n,,[R] ECU memory Optimization using machine learning,2,https://www.reddit.com/r/MachineLearning/comments/omwt3n/r_ecu_memory_optimization_using_machine_learning/,axon94,r/MachineLearning,"I want to build a model that optimizes memory usage to improve performance of software on an ECU. Essentially, the model would predict which memory is used for each process software package, when, etc. 

My hypothesis is that even small improvements could help make drastic performance gains or at least reduce the number of crashes, resets, freezes, etc. Improved up-time in certain applications would be a huge boon for customer experience all the way to improved thermal performance(?).

Anyone else working on this type of application? I have started synthesizing some related, but different, research. Was hoping the reddit hive mind would index some more knowledge not collected yet.",1626634483.0
1626706992.567327,t3_omzhes,,[D] surpassing the pareto front in optimization problems,0,https://www.reddit.com/r/MachineLearning/comments/omzhes/d_surpassing_the_pareto_front_in_optimization/,blueest,r/MachineLearning,"In real life multi objective optimization problems, is it ever possible to obtain a solution like the ""black x"" in this picture here? 

https://imgur.com/a/UmXTQ64

I understand that usually in multi objective optimization problems, since there are so many criteria to be optimized - it is very unlikely to have a ""globally best"" solution. Usually, some solutions will be better in some of the criteria - and other solutions will be better in other criteria. For example, if you are trying to find airplane tickets and want to optimize cost/length of flight : it is very likely some tickets will be expensive and short, some tickets will be cheap and long, and some tickets will be in the middle.

But suppose the data is such - sometimes, we can stumble across a ticket that is both cheap and short. Thus, in this case - how does the concept of the Pareto Front apply over here? The Pareto Front would usually refer to a group of airplane tickets that ""cannot be improved in any of the objectives without degrading at least one of the other objectives"" ( source: https://en.wikipedia.org/wiki/Multi-objective_optimization). But suppose there was one airplane ticket that was both cheaper AND shorter than any other ticket - in this example, would the Pareto Front simply be made of this one ""supreme point""?

Also, this must happen sometimes in real life - correct?

Thanks",1626642677.0
1626706992.567631,t3_omorot,,[R] Keeping track of LSTM hidden state in online learning context,6,https://www.reddit.com/r/MachineLearning/comments/omorot/r_keeping_track_of_lstm_hidden_state_in_online/,mrwafflezzz,r/MachineLearning,"Do I need to pass the previous (nonzero) hidden state of an LSTM for the next sample in an online learning (per sample/ no batches) context?

I assume the hidden state is always zero right now for every new sample.

Currently my forward pass consists of this in PyTorch:

    def forward(self, x):
        x = self.src_embedding(x)
        x, _ = self.bigru(x) # Initial hidden state h0 defaults to zeros
        x = x[:, -1] # Forward only output of last hidden state to fc layer
        x = self.fc1(x)
        return x

EDIT:

Big thanks to everyone who commented. I followed their advice.

This is now what my forward pass looks like. Detach prevents gradient descent on the hidden state:

    def forward(self, x, h):
    x = self.src_embedding(x)
    x, h = self.bigru(x, h.detach())
    x = x[:, -1]
    x = self.fc1(x)
        return x, h

The model also has a function for 0 initializing the hidden states before online learning starts. The model overwrites the hidden state during online learning.

    outputs, h = model(input_sequence, h)

The hidden state of the previous sample is now used for the next sample.

Accuracy went up by a tad bit.",1626607573.0
