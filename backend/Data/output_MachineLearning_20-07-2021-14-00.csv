extracted_at,name,id,title,score,url,author,subreddit,description,created_at
1626789602.315272,t3_omy345,,[D] Machine Learning - WAYR (What Are You Reading) - Week 117,5,https://www.reddit.com/r/MachineLearning/comments/omy345/d_machine_learning_wayr_what_are_you_reading_week/,ML_WAYR_bot,r/MachineLearning,"This is a place to share machine learning research papers, journals, and articles that you're reading this week. If it relates to what you're researching, by all means elaborate and give us your insight, otherwise it could just be an interesting paper you've read.

Please try to provide some insight from your understanding and please don't post things which are present in wiki.

Preferably you should link the arxiv page (not the PDF, you can easily access the PDF from the summary page but not the other way around) or any other pertinent links.

Previous weeks :

|1-10|11-20|21-30|31-40|41-50|51-60|61-70|71-80|81-90|91-100|101-110|111-120|
|----|-----|-----|-----|-----|-----|-----|-----|-----|------|-------|-------|
|[Week 1](https://www.reddit.com/4qyjiq)|[Week 11](https://www.reddit.com/57xw56)|[Week 21](https://www.reddit.com/60ildf)|[Week 31](https://www.reddit.com/6s0k1u)|[Week 41](https://www.reddit.com/7tn2ax)|[Week 51](https://reddit.com/9s9el5)|[Week 61](https://reddit.com/bfsx4z)|[Week 71](https://reddit.com/d7vno3)|[Week 81](https://reddit.com/f1f0iq)|[Week 91](https://reddit.com/hlt38o)|[Week 101](https://reddit.com/k81ywb)|[Week 111](https://reddit.com/myg8sm)||||||||||
|[Week 2](https://www.reddit.com/4s2xqm)|[Week 12](https://www.reddit.com/5acb1t)|[Week 22](https://www.reddit.com/64jwde)|[Week 32](https://www.reddit.com/72ab5y)|[Week 42](https://www.reddit.com/7wvjfk)|[Week 52](https://reddit.com/a4opot)|[Week 62](https://reddit.com/bl29ov)|[Week 72](https://reddit.com/de8h48)|[Week 82](https://reddit.com/f8fs6z)|[Week 92](https://reddit.com/hu6zq9)|[Week 102](https://reddit.com/kh27nx)|[Week 112](https://reddit.com/n8m6ds)||
|[Week 3](https://www.reddit.com/4t7mqm)|[Week 13](https://www.reddit.com/5cwfb6)|[Week 23](https://www.reddit.com/674331)|[Week 33](https://www.reddit.com/75405d)|[Week 43](https://www.reddit.com/807ex4)|[Week 53](https://reddit.com/a8yaro)|[Week 63](https://reddit.com/bqlb3v)|[Week 73](https://reddit.com/dkox1s)|[Week 83](https://reddit.com/ffi41b)|[Week 93](https://reddit.com/iaz892)|[Week 103](https://reddit.com/kpsxtc)|[Week 113](https://reddit.com/njfsc6)||
|[Week 4](https://www.reddit.com/4ub2kw)|[Week 14](https://www.reddit.com/5fc5mh)|[Week 24](https://www.reddit.com/68hhhb)|[Week 34](https://www.reddit.com/782js9)|[Week 44](https://reddit.com/8aluhs)|[Week 54](https://reddit.com/ad9ssz)|[Week 64](https://reddit.com/bw1jm7)|[Week 74](https://reddit.com/dr6nca)|[Week 84](https://reddit.com/fn62r1)|[Week 94](https://reddit.com/ijjcep)|[Week 104](https://reddit.com/kzevku)|[Week 114](https://reddit.com/ntu6lq)||
|[Week 5](https://www.reddit.com/4xomf7)|[Week 15](https://www.reddit.com/5hy4ur)|[Week 25](https://www.reddit.com/69teiz)|[Week 35](https://www.reddit.com/7b0av0)|[Week 45](https://reddit.com/8tnnez)|[Week 55](https://reddit.com/ai29gi)|[Week 65](https://reddit.com/c7itkk)|[Week 75](https://reddit.com/dxshkg)|[Week 85](https://reddit.com/fvk7j6)|[Week 95](https://reddit.com/is5hj9)|[Week 105](https://reddit.com/l9lvgs)|[Week 115](https://reddit.com/o4dph1)||
|[Week 6](https://www.reddit.com/4zcyvk)|[Week 16](https://www.reddit.com/5kd6vd)|[Week 26](https://www.reddit.com/6d7nb1)|[Week 36](https://www.reddit.com/7e3fx6)|[Week 46](https://reddit.com/8x48oj)|[Week 56](https://reddit.com/ap8ctk)|[Week 66](https://reddit.com/cd7gko)|[Week 76](https://reddit.com/e4nmyk)|[Week 86](https://reddit.com/g4eavg)|[Week 96](https://reddit.com/j0xr24)|[Week 106](https://reddit.com/ljx92n)|[Week 116](https://reddit.com/odrudt)||
|[Week 7](https://www.reddit.com/52t6mo)|[Week 17](https://www.reddit.com/5ob7dx)|[Week 27](https://www.reddit.com/6gngwc)|[Week 37](https://www.reddit.com/7hcc2c)|[Week 47](https://reddit.com/910jmh)|[Week 57](https://reddit.com/auci7c)|[Week 67](https://reddit.com/cj0kyc)|[Week 77](https://reddit.com/eb4lxk)|[Week 87](https://reddit.com/gcx3uf)|[Week 97](https://reddit.com/j9cbfs)|[Week 107](https://reddit.com/luqbxl)||
|[Week 8](https://www.reddit.com/53heol)|[Week 18](https://www.reddit.com/5r14yd)|[Week 28](https://www.reddit.com/6jgdva)|[Week 38](https://www.reddit.com/7kgcqr)|[Week 48](https://reddit.com/94up0g)|[Week 58](https://reddit.com/azjoht)|[Week 68](https://reddit.com/cp1jex)|[Week 78](https://reddit.com/ehbfst)|[Week 88](https://reddit.com/glm6sv)|[Week 98](https://reddit.com/jhzz9v)|[Week 108](https://reddit.com/m52u5z)||
|[Week 9](https://www.reddit.com/54kvsu)|[Week 19](https://www.reddit.com/5tt9cz)|[Week 29](https://www.reddit.com/6m9l1v)|[Week 39](https://www.reddit.com/7nayri)|[Week 49](https://reddit.com/98n2rt)|[Week 59](https://reddit.com/b50r5y)|[Week 69](https://reddit.com/cvde5a)|[Week 79](https://reddit.com/entcxy)|[Week 89](https://reddit.com/gu5t0d)|[Week 99](https://reddit.com/jqjgo2)|[Week 109](https://reddit.com/mf8m6u)||
|[Week 10](https://www.reddit.com/56s2oa)|[Week 20](https://www.reddit.com/5wh2wb)|[Week 30](https://www.reddit.com/6p3ha7)|[Week 40](https://www.reddit.com/7qel9p)|[Week 50](https://reddit.com/9cf158)|[Week 60](https://reddit.com/bakew0)|[Week 70](https://reddit.com/d1g1k9)|[Week 80](https://reddit.com/euctyw)|[Week 90](https://reddit.com/hddf7j)|[Week 100](https://reddit.com/jz3evt)|[Week 110](https://reddit.com/moy40m)||

Most upvoted papers two weeks ago:

/u/MrUssek: [Barlow Twins: Self-Supervised Learning via Redundancy Reduction](https://arxiv.org/abs/2103.03230)

/u/TheSunilVarma: https://arxiv.org/abs/2107.00079

Besides that, there are no rules, have fun.",1626638404.0
1626789602.326055,t3_onxw90,,"[N] Researchers from IBM, MIT and Harvard Announced The Release Of DARPA ‚ÄúCommon Sense AI‚Äù Dataset Along With Two Machine Learning Models At ICML 2021",72,https://www.reddit.com/r/MachineLearning/comments/onxw90/n_researchers_from_ibm_mit_and_harvard_announced/,techsucker,r/MachineLearning,"Building machines that can make decisions based on common sense is no easy feat. A machine must be able to do more than merely find patterns in data; it also needs a way of interpreting the intentions and beliefs behind people‚Äôs choices.

At the 2021 International Conference on Machine Learning (ICML), Researchers from IBM, MIT, and Harvard University have come together to release a¬†[DARPA ‚ÄúCommon Sense AI‚Äù dataset](https://arxiv.org/pdf/2102.12321.pdf)¬†for benchmarking AI intuition. They are also releasing¬†[two machine learning models](https://arxiv.org/pdf/2102.12321.pdf)¬†that represent different approaches to the problem that relies on testing techniques psychologists use to study infants‚Äô behavior to accelerate the development of AI exhibiting common sense.¬†

Summary: [https://www.marktechpost.com/2021/07/20/researchers-from-ibm-mit-and-harvard-announced-the-release-of-its-darpa-common-sense-ai-dataset-along-with-two-machine-learning-models-at-icml-2021/](https://www.marktechpost.com/2021/07/20/researchers-from-ibm-mit-and-harvard-announced-the-release-of-its-darpa-common-sense-ai-dataset-along-with-two-machine-learning-models-at-icml-2021/) 

Paper: [https://arxiv.org/pdf/2102.12321.pdf](https://arxiv.org/pdf/2102.12321.pdf)

IBM Blog: https://research.ibm.com/blog/icml-darpa-agent",1626769244.0
1626789602.33337,t3_ontqo0,,"[D] Best methods for imbalanced multi-class classification with high dimensional, sparse predictors",75,https://www.reddit.com/r/MachineLearning/comments/ontqo0/d_best_methods_for_imbalanced_multiclass/,ChazzFingers,r/MachineLearning,"Hi everyone,

Im currently working with a very high dimensional data set of dimensions \~ 84,000 \* 190,000. The response variable is categorical with 9 levels and is quite imbalanced. Further, the predictors are very sparse, i.e. the vast majority of predictors are 0 for any given observation. I'm looking to develop a strong predictive model with this data but am at a bit of a loss as to what models /techniques to use.

Currently I'm using SVD (non-centered to preserve sparse matrix format of predictors) to reduce dimensions of predictors and have been experimenting with OVR logistic regression with/without penalties and random forests on the SVD components. I've heard SVCs can be good for these kinds of problems but my results using OVR SVCs have been disappointing (below zero R accuracy).

Would love to hear any/all suggestions on what models might provide accurate classification on this data.

Thanks very much",1626750871.0
1626789602.340532,t3_onyofp,,[D] What is the method to deal with sparse high dimensional feature columns,4,https://www.reddit.com/r/MachineLearning/comments/onyofp/d_what_is_the_method_to_deal_with_sparse_high/,aviisu,r/MachineLearning,"For more context: I am working on medicine application that take an input as the symptom of patience (headache, nausea, rash on the skin, coughing, etc.) encoded in HPO code form (https://hpo.jax.org/app/) and make a prediction on what disease should the patient be (fever, flu, diabetes, etc.)

My currently design choice is encode the HPO code (symptoms) into one hot vector and use a machine learning algorithm (random forest, for example). The training set could look something like this



Text| HPO1(headache)| HPO2(nausea)| HPO3(coughing)| ...| ...| HPO30000(rash on skin)| labels
---|---|----|----|----|----|----|----
headache, nausea, coughing | 1| 1| 1| ...| ...| 0| flu
headache, coughing | 1 | 0| 1| ...| ...| 0| fever
...| ...| ...| ...| ...| ...| ...| ...
...| ...| ...| ...| ...| ...| ...| ...
100000+ rows| ...| ...| ...| ...| ...| ...| ...

The problem is that with so much features columns (30000+ coumns) and rows(100000+ rows), I'm not sure if using machine learning algorithm like random forest will be good for this option,** especially that most of the columns are 0s and only fews are 1s (for example, the first row is 3 'one' out of 30000 'zero')** I feel like there is a better way to tackle/look at the problem. I feel like using dimensionality reduction (like PCA) will not be good because all columns are important.

I would love to hear what you guys think about this, or suggest the resources to read further. Thank you very much.",1626773321.0
1626789602.348265,t3_ontz4j,,[R] Autonomy 2.0: Why is self-driving always 5 years away?,13,https://arxiv.org/abs/2107.08142,hardmaru,r/MachineLearning,,1626751775.0
1626789602.356212,t3_onmjv3,,[P] Reverse-mode automatic differentiation in Haskell using delimited continuations,37,https://www.reddit.com/r/MachineLearning/comments/onmjv3/p_reversemode_automatic_differentiation_in/,spindly-torque,r/MachineLearning,"A Haskell implementation of the reverse AD framework presented in [1], available as a lightweight and easily extensible library:

[https://hackage.haskell.org/package/ad-delcont](https://hackage.haskell.org/package/ad-delcont) 

Blog post introducing the delimited continuation operators `shift`/`reset` and explaining why this technique is an alternative approach to AD by source transformation, or non-standard evaluation based on ""tape"" or ""graph"" reified programs:

[http://ocramz.github.io/haskell/automatic-differentiation/2021/07/19/ad-delcont.html](http://ocramz.github.io/haskell/automatic-differentiation/2021/07/19/ad-delcont.html) 



References

\[1\] Wang et al, Backpropagation with callbacks: Foundations for Efficient and Expressive Differentiable Programming, [https://proceedings.neurips.cc/paper/2018/hash/34e157766f31db3d2099831d348a7933-Abstract.html](https://proceedings.neurips.cc/paper/2018/hash/34e157766f31db3d2099831d348a7933-Abstract.html)",1626726679.0
1626789602.364538,t3_onpgii,,[D] How does the mind work (as an algorithm) -- join us this Wednesday.,27,https://www.reddit.com/r/MachineLearning/comments/onpgii/d_how_does_the_mind_work_as_an_algorithm_join_us/,Rina-Panigrahy,r/MachineLearning,"What is the right logical architecture that captures the essential capabilities of the mind? Is it logically equivalent to some deep learned system perhaps augmented with some kind of memory or knowledge graph and consists of several deep learned modules that interact with each other?

It would be nice to have a discussion about these questions -- please join the following google-meet link for an informal discussion (there are no planned participants yet and anyone can chime in). We could touch upon basic questions around an architecture for the mind (such as [https://docs.google.com/document/d/1tS-B2BTXixqucUPzLm5o6VnbvTY5EMbxzr\_ox5RF3zQ/edit](https://docs.google.com/document/d/1tS-B2BTXixqucUPzLm5o6VnbvTY5EMbxzr_ox5RF3zQ/edit) discussed in this panel discussion: [https://www.youtube.com/watch?v=g5DGBWjiULQ&t=6496s](https://www.youtube.com/watch?v=g5DGBWjiULQ&t=6496s)) and also use inspiration from psychology, neuroscience and may touch upon philosophical issues such as ""what is the mind?""

If you are interested in joining please email me at [rinapy@gmail.com](mailto:rinapy@gmail.com) with subject ""How does the mind work"" so I have a sense of how many people will join.

Discussion: How does the mind work as an algorithm?

Wednesday, July 21 ¬∑ 5:00 ‚Äì 6:00pm (PST) ([Google-Calendar-link](https://calendar.google.com/event?action=TEMPLATE&tmeid=NzRxbWVhcjhpbnJwZnM0cWRmcm1wN290YmwgcmluYXB5QG0&tmsrc=rinapy%40gmail.com))

Google Meet joining info

Video call link: [https://meet.google.com/fdw-qmsa-ndx](https://meet.google.com/fdw-qmsa-ndx)

Thanks, Best,

Rina Panigrahy ([http://theory.stanford.edu/\~rinap/](http://theory.stanford.edu/~rinap/))",1626735611.0
1626789602.373521,t3_onbunj,,"[D] How did the do hyper-parameter tuning for large models like GPT-3, ERNIE etc, as they cost them millions for just training?",185,https://www.reddit.com/r/MachineLearning/comments/onbunj/d_how_did_the_do_hyperparameter_tuning_for_large/,IndieAIResearcher,r/MachineLearning,"Hi everyone,

I've worked on some deep learning, I've done some custom data training with hyperparameter tuning which taken some significant amount of time an money on cloud. I'm just wondering, how these people do hyperparameter tuning, architecture design etc, as training them costs millions. Or just it comes by experience?",1626692720.0
1626789602.381351,t3_onfb69,,[R] Deep Learning over the Internet: Training Language Models Collaboratively,84,https://www.reddit.com/r/MachineLearning/comments/onfb69/r_deep_learning_over_the_internet_training/,justheuristic,r/MachineLearning,"&#x200B;

[\[algorithm explained in a short video\]](https://reddit.com/link/onfb69/video/g9t39hklk6c71/player)

This is a story about 40-ish enthusiasts that trained a large transformer together over the internet. The blog post explains how a careful choice of training algorithm and model architecture can let you train a large BERT-like model on GPUs from across the world, despite the slow network and unreliable hardware.

This experiment is based on a recent paper by Hugging Face & Yandex about training on a swarm of heterogeneous unreliable GPUs. They use the hivemind library for parallelism and HF datasets for streaming the data.

Blog post: [https://huggingface.co/blog/collaborative-training](https://huggingface.co/blog/collaborative-training)

Hivemind: [https://github.com/learning-at-home/hivemind](https://github.com/learning-at-home/hivemind)",1626705442.0
1626789602.390821,t3_oo1on1,,[D] What resolution images do you usually use while training neural nets in general?,1,https://www.reddit.com/r/MachineLearning/comments/oo1on1/d_what_resolution_images_do_you_usually_use_while/,Alex55936,r/MachineLearning,"Does it matter much if use 64x64 images or 128x128 or even 256x256 images to train the neural network. How much the training accuracy is affected generally with lower v/s higher resolution images ?  

Also if anyone can tell what is the industry standard ?  And what is the standard for research applications if there is even one ? 

One obvious disadvantage of using very high res images is that the training takes more time and resources, are there are any other disadvantages ? And what about advantages ?


Edit: By industry standards I mean like the pretrained  models that are a available and widely used like the YOLO or many of the face detection models that have very high accuracy , etc .",1626786425.0
1626789602.400456,t3_oo0osb,,[D] Generalization through Memorization: Nearest Neighbor Language Models (Research Paper Walkthrough),1,https://www.reddit.com/r/MachineLearning/comments/oo0osb/d_generalization_through_memorization_nearest/,prakhar21,r/MachineLearning,"Bigger Models, Better Results ?? Always ?

This paper from Stanford University  and Facebook AI  proposes this very interesting idea of combining k-nearest neighbours with decently sized Language Model for handling factual inconsistency, generalisation and other existing issues with current text generation systems and surpassing large language models üî• üî• 

https://youtu.be/nJaekQb6DwU",1626782544.0
1626789602.408244,t3_onta3u,,"[D] Scalarization for Optimizing Multi-Objective ""Blackbox"" Functions (i.e. Gradient Free)",3,https://www.reddit.com/r/MachineLearning/comments/onta3u/d_scalarization_for_optimizing_multiobjective/,jj4646,r/MachineLearning,"Has anyone ever worked on problems in which you had to optimize multi-objective ""blackbox"" functions (i.e. functions where you can not take the derivatives, algorithms like gradient descent do not apply), e.g. using the genetic algorithm?

In the context of multi-objective optimization of non-blackbox functions, I read about some methods called ""scalarization"" which effectively transform multi-objective optimization problems into single-objective optimization problems.

For example: If you are trying to optimize three cost functions F1, F2, F3 ... you could combine these into a single problem using weighted coefficients, e.g. T = A * F1 + B* F2 + C *F3

A popular way to solve the above equation is to use methods like ""epsilon-constraint"": This is where you apply the desired constraints to F1, F2, F3 ... and then instruct the computer to loop through different values of A, B, C. Then, you see which combination of parameters (used in F1, F2, F3) result in the minimization of ""T"" - this is much easier to compare, since you can just rank all the candidate solutions. (source: https://www.youtube.com/watch?v=yc9NwvlpEpI)

This leads me to my question:

1) Do methods like ""epsilon constraint"" apply to ""Blackbox"" Functions? I.e. Can you use the ""epsilon constraint"" method along with the genetic algorithm?

2) Intuitively, when dealing with a multi-objective optimization problem: is there any way to deal with all the solutions along the ""Pareto Front""? Using the concept of the ""Pareto Front"" - suppose the optimization algorithm identifies a set of solutions that ""can not be made better in some criteria without worsening some other criteria"" ... how exactly can you rank and compare all the solutions along the Pareto Front? The concept of scalarization seemed useful, seeing how it converts a multi-objective optimization problem into a single-objective optimization problem, and therefore you can rank all the candidate solutions according to the ones that result in the minimum cost of the single objective .... but otherwise, how are you supposed to pick a solution among the set of solutions along the Pareto Front?

Thanks",1626749157.0
1626789602.416444,t3_onvrgb,,[D] Importance of Convergence Proof for Gradient Descent,2,https://www.reddit.com/r/MachineLearning/comments/onvrgb/d_importance_of_convergence_proof_for_gradient/,jj4646,r/MachineLearning,"https://imgur.com/a/r7jRcPJ

We all  probably know the importance of the gradient descent algorithm within the domain of machine learning (e.g. calculating the weights for neural networks) - but not all of have studied the inner details behind the gradient descent algorithm.

For instance, I was looking at this ""proof of convergence"" for the gradient descent algorithm over here: https://imgur.com/a/r7jRcPJ 

1) Just to clarify - in this case, does ""convergence"" mean it will necessarily reach some ""minimum"" (whether ""local"" or ""global"") point after k iterations? 

Or does it mean it mean that ""differences in the k-1 and k iteration will be negligible""? (I am not sure if this statement is characteristic of a ""minimum"" point? i.e the difference between the k-1 and k iteration is ONLY negligible when you reach some sort of minimum point?)

2) for a real-world dataset and choice of algorithm that uses gradient descent, is it ever possible to ""approximate"" the value of k in advance? could you ever (hypothetically) say that ""for this problem, I will approximately need to run gradient descent N number of times before I can reach an error of 0.2?""? (is this strong convergence or convergence in probability?)

3) are the performance results from an algorithm that uses gradient descent ever ""guaranteed within a certain error bound outside of the data it was exposed to""? or is this just wishful thinking?

4) https://imgur.com/a/r7jRcPJ in this picture, in the numerator on the term on the right hand side, do the ""two 2's stacked on each other"" refer to ""squared"" and ""l2 norm""?

if anyone has any other insights they would like to share on the ""importance of convergence for gradient descent"", please feel free to do so!

thanks",1626759124.0
1626789602.42539,t3_onax7l,,"Deep Learning should not be blindly applied to EVERY problem out there ""[Discussion]""",63,https://www.reddit.com/r/MachineLearning/comments/onax7l/deep_learning_should_not_be_blindly_applied_to/,erasperiko,r/MachineLearning,"Since the upsurge of Deep Learning (DL) from CNN in the image realm and from RNN for NLP, there is a clear trend towards this type of modeling technique. Mainly in context-specific areas such as finance, traffic, energy, etc, seems like every congress you attend for more than half of the publications are like this: ""So we have these datasets that have been feed to this exotic DL architecture. Our results excel the state of the art in a 0.1%. That is our contribution.""

I understand that it is an easy way to publish a paper as this kind of modeling technique is a hot topic right now. However, in my research context (which can be defined as 'traffic state prediction') can be disadvantageous. In a recent paper ([https://arxiv.org/abs/2012.02260](https://arxiv.org/abs/2012.02260)), we demonstrate that for a time series prediction approach, where data is self-descriptive (meaning that as a human being you can easily interpret the current values of the series) DL architectures do not overperform another less computational demandant algorithms such as the *all mighty* Random Forest.

The core idea behind this investigation is to shed light on an overlooked issue in this field: researchers apply modeling techniques without considering the endemic characteristics of their data collection:

* Is your data self-descriptive? 
* Is your data Euclidean?
* Are you going to benefit from the data fusion capabilities of DL?

I am only in my first research years but after reviewing lots of papers about this topic my concerns can be summarized as the above, and frankly, I do not like where this is going.",1626688474.0
1626789602.434447,t3_onz0k9,,A new-generation deep learning framework is launched[Project],0,https://www.reddit.com/r/MachineLearning/comments/onz0k9/a_newgeneration_deep_learning_framework_is/,Just0by,r/MachineLearning,"Hi, everyone.

This is the first time to introduce our project  [OneFlow](https://github.com/Oneflow-Inc/oneflow)  on Reddit, which is a performance-centered and open-source deep learning framework.

**OneFlow on GitHub**Ôºö[https://github.com/Oneflow-Inc/oneflow](https://github.com/Oneflow-Inc/oneflow)

OneFlow BenchmarkÔºö[https://github.com/Oneflow-Inc/OneFlow-Benchmark](https://github.com/Oneflow-Inc/OneFlow-Benchmark)

What is the big difference between OneFlow and TensorFlow or PyTorch? What challenges of DL framework OneFlow has solved for AI developers? To make you know well about OneFlow, Here are [some related articles](https://oneflow2020.medium.com) to explain its core techniques, including techniques of distributed training and the correct level of abstraction for distributed DL frameworks.

**Welcome to visit our project,** we would love to hear feedback and we wish you to use and contribute if you like our project.

Thank you!",1626775037.0
1626789602.446957,t3_onpn8k,,[R] Social Analysis Network analysis for Twitter in real time,3,https://www.reddit.com/r/MachineLearning/comments/onpn8k/r_social_analysis_network_analysis_for_twitter_in/,srpantano,r/MachineLearning,"I'm researching for my graduation work to analyze, in real time, how news affects the popularity of some on social media. For that, I would like to know if using Social Analysis Network is ideal?",1626736223.0
1626789602.455376,t3_onirgu,,[R] HTLM: Hyper-Text Pre-Training and Prompting of Language Models,9,https://www.reddit.com/r/MachineLearning/comments/onirgu/r_htlm_hypertext_pretraining_and_prompting_of/,ArmenAg,r/MachineLearning,"Arxiv: [https://arxiv.org/abs/2107.06955](https://arxiv.org/abs/2107.06955)

Twitter: [https://twitter.com/ArmenAgha/status/1415928833488785409](https://twitter.com/ArmenAgha/status/1415928833488785409)",1626715566.0
1626789602.462773,t3_onteod,,[D] Why does the genetic algorithm tend to NOT produce garbage results?,0,https://www.reddit.com/r/MachineLearning/comments/onteod/d_why_does_the_genetic_algorithm_tend_to_not/,jj4646,r/MachineLearning,"I spent a lot of time reading different articles online and watching lectures on youtube - and I could not seem to find any major theoretical results that ""guarantee"" some sort of performance for the genetic algorithm. The genetic algorithm seems to be a clever attempt to mimic the process of evolution and natural selection within the optimization framework - but it seems to work very well.

Does anyone know why this is? Has anyone ever attempted to understand from a mathematical perspective, why the genetic algorithm has been known to successfully explore complex spaces in real-world problems, and then return respectable results?",1626749644.0
1626789602.473886,t3_onk79q,,"[P] Pre-trained neural network for checking weapons (guns, knives) on image",3,https://www.reddit.com/r/MachineLearning/comments/onk79q/p_pretrained_neural_network_for_checking_weapons/,Affectionate_Teach23,r/MachineLearning,"Need a neural network for project to check if the weapon is on image or not. Nothing appropriate was found on Kaggle, GitHub, and Google. Do you know a pre-trained neural network to apply or where else can I look for it? Or it is better to train it by myself?",1626719733.0
1626789602.486581,t3_onlvhb,,[D] Black-box adversarial attacks and stochastic defensive methods,2,https://www.reddit.com/r/MachineLearning/comments/onlvhb/d_blackbox_adversarial_attacks_and_stochastic/,NongHyupJoy,r/MachineLearning,"I know that the paper 'On Evaluating Adversarial Robustness' recommends to do sanity check with black-box attacks to see if robustness comes from the gradient obfuscation. So I see many AA papers do additional experiment with black-box attacks.

However, I also am aware that the black-box attacks are not effective on the defensive methods using stochastic factors (injecting random noise to input image, weights or intermediate states ...), but I don't remember the source of it.

I see some papers using random factors as defense do not perform black-box attacks. For example, those two recent papers, [Adv-BNN: Improved Adversarial Defense through Robust Bayesian Neural Network](https://arxiv.org/abs/1810.01279) (Bayesian model with stochastic sampling) and [Simulating a Primary Visual Cortex at the Front of CNNs Improves Robustness to Image Perturbations](https://www.biorxiv.org/content/10.1101/2020.06.16.154542v1.full.pdf) (injecting noise to input image), they don't do any black-box attack. I am not fully confident of the reason why they didn't do black-box attack, but I guess the reason is that their model is utilizing randomness inside.

&#x200B;

I think doing black-box attack is necessary to make sure if it is not from gradient obfuscation even for defenses with randomness. Otherwise, how do one knows if the robustness is due to obfuscation or not?

I just want to know your thought on performing black-box attacks on defensive models with randomness. As those papers I posted did not do black-box attack, do you think it is not necessary to do black-box attack for stochastic defensive methods? (Maybe the reason for those works do not perform black-box attack is not what I expect.)

If you know any paper perform black-box attacks on stochastic defensive method, could you let me know? I want to know the proper way of doing black-box attack on stochastic defensive methods. ",1626724665.0
1626789602.494701,t3_onpdlq,,[D] Voice Style Transfer: State of the Art,1,https://www.reddit.com/r/MachineLearning/comments/onpdlq/d_voice_style_transfer_state_of_the_art/,monfre_97,r/MachineLearning,"I am looking around for the state of the art on voice style transfer models. This is widely recognized as a domain with many industrial application, but (of course) I haven't found anything fully satisfactory around, meaning a good quality tool ready for use. Too much optimistic and lazy, but this is what ideally I was shooting for. 

So, plan B is to dig deeper for myself, looking for cool models to implement or similar. Personally I am curious to experiment style transfer from a source to a target speaker in music (singing). Do you have any model, repository, paper to advice to frame what's the state of the art as of today? Any help is really appreciated",1626735338.0
1626789602.511377,t3_ono5xl,,"[D] Splitting large dataset into two sub-sets, training on each separately, then fusing together.",1,https://www.reddit.com/r/MachineLearning/comments/ono5xl/d_splitting_large_dataset_into_two_subsets/,smokesick,r/MachineLearning,"I have a situation which I have been trying to find an answer for but still need more info on. Roughly speaking, I have a large dataset in which \~50% of the data is uniformly distributed, and the other 50% has a *very* complex distribution. In general, my target is the ""complex distribution"", but I would like the network to also perform well on the ""uniform distribution"". I am trying to figure out the pros and cons of splitting the dataset into two subsets, pre-training separate networks on each, and then training them in tandem on a subset of the full dataset.

I am thinking the following:

* The network trained on the uniformly distributed data can gain high accuracy while working with a randomly sampled subset of the corresponding sub-dataset. This could decrease the overall training time since I will feed less data into that specific network.
* Fusing and re-training the networks on a *subset* of the full dataset should resemble a network training on the full dataset while retaining its overall distribution. We can assume the networks are performing well on the sub-datasets, so fusing them should retain the correct overall distribution while reducing training time.
* My network will use L1 or L2 loss. If I train on a lot on uniformly-distributed samples, the network will tend to converge towards this value. Not sure I can do anything about this though unless I alter the overall data distribution.

I would like to hear your input on this. Cheers!",1626731544.0
1626789602.523293,t3_on0ayu,,[D] What OOP/design principles/coding best-practices do you utilize on your machine learning/data science projects?,76,https://www.reddit.com/r/MachineLearning/comments/on0ayu/d_what_oopdesign_principlescoding_bestpractices/,jimmykim9001,r/MachineLearning,"I've recently realized that a lot of my ML code is just done on disorganized jupyter notebooks. Every time I run a new experiment I just change a couple of hyperparameters and then run a bunch of cells, which I realize is kind of bad practice.

I've recently found the youtube channel Arjan Codes, which goes over design patterns and I was wondering if you guys do anything similar to modularize and make your code more efficient/easier to maintain. How do you typically organize your github repo?",1626645213.0
1626789602.533349,t3_ong331,,[R] ‚ÄòQuanTaichi‚Äô Quantized Simulation: High Visual Quality With Reduced Memory Cost,3,https://www.reddit.com/r/MachineLearning/comments/ong331/r_quantaichi_quantized_simulation_high_visual/,Yuqing7,r/MachineLearning,"A research team from Taichi Graphics, MIT CSAIL, Zhejiang University, Tsinghua University and Kuaishou Technology introduces a programming language and compiler for quantized simulation that achieves both high performance and significantly reduced memory costs by enabling flexible and aggressive quantization. 

Here is a quick read: [‚ÄòQuanTaichi‚Äô Quantized Simulation: High Visual Quality With Reduced Memory Cost.](https://syncedreview.com/2021/07/19/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-64/)

The code is available on the project [GitHub](https://github.com/taichi-dev/quantaichi). The paper *QuanTaichi: A Compiler for Quantized Simulations* is a [SIGGRAPH 2021](https://s2021.siggraph.org/) conference paper and is available on [yuanming.taichi.graphics](https://yuanming.taichi.graphics/publication/2021-quantaichi/).",1626707787.0
1626789602.543928,t3_on3wqk,,"[D] ""A New Publishing Model in Computer Science"" by Yann LeCun",33,https://www.reddit.com/r/MachineLearning/comments/on3wqk/d_a_new_publishing_model_in_computer_science_by/,Flying_Scholars,r/MachineLearning,"Pamphlet (2012): [http://yann.lecun.com/ex/pamphlets/publishing-models.html](http://yann.lecun.com/ex/pamphlets/publishing-models.html)

What is preventing an open market between papers and reviewing entities from happening?",1626657752.0
1626789602.553935,t3_onm2j8,,"""[D]"" Interference Management Techniques in Small Cells Overlaid Heterogeneous Cellular Networks",1,https://www.reddit.com/r/MachineLearning/comments/onm2j8/d_interference_management_techniques_in_small/,GTKdope,r/MachineLearning,"Trying to find how to implement the below paper. Any help would be greatly appreciated ,

 [Interference Management Techniques in Small Cells Overlaid Heterogeneous Cellular Networks (riverpublishers.com)](https://www.riverpublishers.com/journal_read_html_article.php?j=JMM/14/3/2) 

Thank you",1626725223.0
1626789602.562873,t3_ona50x,,[D] Using Value Uncertainty/Confidence as Input to ML,7,https://www.reddit.com/r/MachineLearning/comments/ona50x/d_using_value_uncertaintyconfidence_as_input_to_ml/,iamaliver,r/MachineLearning,"I am trying to figure out what has been tried about using uncertainty as a parameter to influence the uncertainty in the output or anyone's thoughts as well!

I am unsure how to properly formulate the idea and appreciate any clarifications or keywords suggestions.

Here are a few examples that hopefully capture the idea:

Example 1:

In number recognition (NMIST) the typical setup would be.  

You provide a 100x100 pixel input.  Each pixel is grayscaled and you feed it to your ML algorithm.  

Implicit in this ML algorithm is that you know for certain your input value (each pixel) with confidence of 1.  

What if you do not know with high confidence the pixel value?  ie: I say this pixel is White, but only have 0.8 confidence?  Shouldn't that influence my final output? 

Say I have an input picture where all my confidence in the pixels is really low, even though the output is 8  the confidence of the 8 should be really low.

Example 2:

I want to figure out if it will rain.  I input a lot of variables, such as ""morning cloud cover"", ""morning temperature"", ""time of year"" etc.  However the variable ""morning cloud cover"" is highly variable.  Saying 50% is not exactly true.  It is more a range of values, \[30%, 60%\].  

Maybe I can input 50% with range of 10% into the ML?

Example 3:

You have collected some data, and implicit to this data is really high noise.  Both in what you are measuring and in the final output.  You know that the uncertainty in the input for any given variable is really high.  Or perhaps the 95% confidence is very wide.  What is the best way to capture this idea?

&#x200B;

My current thoughts:

I am unclear what to search online.  ""uncertainty input parameters machine learning"" is a very crappy set of keywords to search.

I believe that even when you have a lot of samples to train with, this problem of ""uncertain inputs"" persists and should affect your final output. 

Any thoughts appreciated or even how to approach this problem.",1626684567.0
1626789602.57211,t3_omrhm7,,[D] Alias-Free GAN: The non-sticky & Improved StyleGAN2!,191,https://youtu.be/j1ZY7LInN9g,farfromhome2020,r/MachineLearning,,1626618348.0
1626789602.581043,t3_onjunq,,[D] To generate images/sentences of certain properties,1,https://www.reddit.com/r/MachineLearning/comments/onjunq/d_to_generate_imagessentences_of_certain/,thanrl,r/MachineLearning,"Say I have a differentiable function f(x) that computes some scaler property (e.g. saturation, negative sentiment) of x, where x is an image or a sentence. Given a dataset X of images/sentences, how can I modify existing generative models (e.g. autoencoders, sequence-to-sequence models) to generate samples that, instead of replicating X, replicates f(X)?",1626718721.0
1626789602.593698,t3_onekjx,,"[R] Commentary on ""Unsupervised Speech Recognition""",1,https://www.reddit.com/r/MachineLearning/comments/onekjx/r_commentary_on_unsupervised_speech_recognition/,Flying_Scholars,r/MachineLearning,"**Paper:** ""Unsupervised Speech Recognition""  
*Alexei Baevski, Wei-Ning Hsu, Alexis Conneau, Michael Auli*  
[https://arxiv.org/abs/2105.11084](https://arxiv.org/abs/2105.11084)

**Commentary:**  ""I believe that including a term to maximize mutual information between segment representations and the generated sequence of phonemes may result in better control over the textual information in the transcription...""  **Link to full commentary in the comments.**",1626703072.0
1626789602.606095,t3_onip9t,,[D] What is the specific name currently used for this ML model?,0,https://www.reddit.com/r/MachineLearning/comments/onip9t/d_what_is_the_specific_name_currently_used_for/,dingsda2,r/MachineLearning,"1. It takes results data...
2. Feeds it into the core systems...
3. Modifies the core systems...
4. Results become more optimized...
5. Repeats from step 1...

What would you call this ML model? If even ML at all.",1626715390.0
1626789602.619745,t3_onfril,,[D] BYOL explained in 5 minutes: Bootstrap Your Own Latent A New Approach to Self-Supervised Learning by Jean-Bastien Grill et al.,0,https://www.reddit.com/r/MachineLearning/comments/onfril/d_byol_explained_in_5_minutes_bootstrap_your_own/,KirillTheMunchKing,r/MachineLearning,"Is it possible to learn good enough image representations for many downstream tasks at once?

A well known approach is to use self-supervised pretraining such as state-of-the art contrastive methods that are trained to reduce the distance between representation of augmented views of the same image (positive pairs) and increasing the distance between representations of augmented views of different images. These methods need careful treatment of negative pairs, whereas **BYOL achieves higher performance than SOTA contrastive methods without using negative pairs** at all. Instead it uses two networks that learn from each other to iteratively bootstrap the representations by forcing one network to use an augmented view of an image to predict the output of the other network for a different augmented view of the same image. Sounds crazy, I know... but it actually works!

Read the [full paper digest](https://t.me/casual_gan/68) and the [blog post](https://www.casualganpapers.com/self-supervised/representation-learning/online-target-networks/2021/07/13/BYOL.html) (reading time \~5 minutes) to learn about using an online and a target networks to make self-supervised learning work without using any negative pairs during training as well as the general intuition why SSL works in the first place.

Meanwhile, check out the paper digest poster by [Casual GAN Papers](https://www.casualganpapers.com/)!

[BYOL paper poster](https://preview.redd.it/n9nusoxyo6c71.png?width=571&format=png&auto=webp&s=005ec3eb61133c41f909ab19576806530a7a6723)

\[[Full Explanation Post](https://t.me/casual_gan/68) / [Blog Post](https://www.casualganpapers.com/self-supervised/representation-learning/online-target-networks/2021/07/13/BYOL.html)\] \[[Arxiv](https://arxiv.org/pdf/2006.07733.pdf)\] \[[Code](https://github.com/deepmind/deepmind-research/tree/master/byol)\]

More recent popular computer vision paper breakdowns:

>\[[Deferred Neural Rendering](https://t.me/casual_gan/66)\]  
>  
>\[[SimCLR](https://www.casualganpapers.com/self-supervised/representation-learning/online-target-networks/2021/07/13/SimCLR.html)\]  
>  
>\[[GIRAFFE](https://t.me/casual_gan/63)\]",1626706834.0
1626789602.636441,t3_onem8j,,[R] RL for parameter space exploration,0,https://www.reddit.com/r/MachineLearning/comments/onem8j/r_rl_for_parameter_space_exploration/,mehmor,r/MachineLearning," Hi All, If we want to use RL as a search algorithm in parameter space, how should I configure it?  
Is there any best practice on this?  
Thank you in advance.",1626703220.0
1626789602.652184,t3_on3lth,,[D] what tools/practices do you use to document your code/functions?,4,https://www.reddit.com/r/MachineLearning/comments/on3lth/d_what_toolspractices_do_you_use_to_document_your/,carlml,r/MachineLearning,"I noticed that I am writing certain functions over and over again because I didn't document very well, so finding them becomes hard, and I usually just write them again. Is there a tool that allows one to document functions? I like the way numpy or pytorch's functions are documented.

&#x200B;

I know this is not directly to machine learning, and if my post definitely does not belong here, I'll delete it.",1626656654.0
1626789602.667142,t3_om7kq3,,"[N] Stop Calling Everything AI, Machine-Learning Pioneer Says",796,https://spectrum.ieee.org/the-institute/ieee-member-news/stop-calling-everything-ai-machinelearning-pioneer-says,SquirrelOnTheDam,r/MachineLearning,,1626539514.0
1626789602.677318,t3_on56fp,,[D] Need help understanding inverting softmax layer from Michael Nielsen's book,0,https://www.reddit.com/r/MachineLearning/comments/on56fp/d_need_help_understanding_inverting_softmax_layer/,Kay0518,r/MachineLearning,"Hi,

I have a hard time understanding a softmax problem from the book:

>**Inverting the softmax layer** Suppose we have a neural network with a softmax output layer, and the activations a\^L\_j are known. Show that the corresponding weighted inputs have the form z\^L\_j = ln a\^L\_j + C for some constant C that is independent of j.

Here is how I've approached: $a\^L\_j= exp(z\_L\_j) / sum(exp(z\^L\_k)). Take the log of both sides,

ln(a\^L\_j) = z\^L\_j - ln(sum(exp(z\^L\_k))). Then, z\^L\_j = ln(a\^L\_j) + ln(sum(exp(z\^L\_k)))

The problem happens here. Why we're allowed to substitute ln(sum(exp(z\^L\_k))) for C when it includes z\^L\_j? Everyone from my research says C is independent of j so it can be C. But, doesn't that mean we have to extract e\^L\_j out of ln(sum(exp(z\^L\_k))), anda add to z\^L\_j on lhs? Can you please give me a insight into this problem?",1626662497.0
1626789602.690779,t3_omwt3n,,[R] ECU memory Optimization using machine learning,2,https://www.reddit.com/r/MachineLearning/comments/omwt3n/r_ecu_memory_optimization_using_machine_learning/,axon94,r/MachineLearning,"I want to build a model that optimizes memory usage to improve performance of software on an ECU. Essentially, the model would predict which memory is used for each process software package, when, etc. 

My hypothesis is that even small improvements could help make drastic performance gains or at least reduce the number of crashes, resets, freezes, etc. Improved up-time in certain applications would be a huge boon for customer experience all the way to improved thermal performance(?).

Anyone else working on this type of application? I have started synthesizing some related, but different, research. Was hoping the reddit hive mind would index some more knowledge not collected yet.",1626634483.0
1626789602.704373,t3_on7etm,,[D] How is the accuracy of a computer vision algorithm measured?,0,https://www.reddit.com/r/MachineLearning/comments/on7etm/d_how_is_the_accuracy_of_a_computer_vision/,Master-Cantaloupe750,r/MachineLearning,"I'm reading a lot of CV papers recently and they sometimes will provide a chart where they compared the accuracy and performance of the algorithms, but they never revealed their method of calculating the accuracy. Supposed an algorithm claims it has ""95% accuracy"" with a performance of 50 FPS, how exactly is the 95% calculated, is there a specific standardized method where everyone is using it to calculate accuracy to be fair?",1626671474.0
1626789602.71794,t3_omzhes,,[D] surpassing the pareto front in optimization problems,0,https://www.reddit.com/r/MachineLearning/comments/omzhes/d_surpassing_the_pareto_front_in_optimization/,blueest,r/MachineLearning,"In real life multi objective optimization problems, is it ever possible to obtain a solution like the ""black x"" in this picture here? 

https://imgur.com/a/UmXTQ64

I understand that usually in multi objective optimization problems, since there are so many criteria to be optimized - it is very unlikely to have a ""globally best"" solution. Usually, some solutions will be better in some of the criteria - and other solutions will be better in other criteria. For example, if you are trying to find airplane tickets and want to optimize cost/length of flight : it is very likely some tickets will be expensive and short, some tickets will be cheap and long, and some tickets will be in the middle.

But suppose the data is such - sometimes, we can stumble across a ticket that is both cheap and short. Thus, in this case - how does the concept of the Pareto Front apply over here? The Pareto Front would usually refer to a group of airplane tickets that ""cannot be improved in any of the objectives without degrading at least one of the other objectives"" ( source: https://en.wikipedia.org/wiki/Multi-objective_optimization). But suppose there was one airplane ticket that was both cheaper AND shorter than any other ticket - in this example, would the Pareto Front simply be made of this one ""supreme point""?

Also, this must happen sometimes in real life - correct?

Thanks",1626642677.0
1626789602.731648,t3_omawwl,,[N] OpenAI disbands its robotics research team,96,https://www.reddit.com/r/MachineLearning/comments/omawwl/n_openai_disbands_its_robotics_research_team/,regalalgorithm,r/MachineLearning,"Or at least, [this has now been low-key confirmed](https://venturebeat.com/2021/07/16/openai-disbands-its-robotics-research-team/) , presumably it happened a while ago.

>OpenAI has disbanded its robotics team after years of research into machines that can learn to perform tasks like [solving a Rubik‚Äôs Cube](https://venturebeat.com/2019/10/15/openai-teaches-a-robotic-hand-to-solve-a-rubiks-cube/). Company cofounder Wojciech Zaremba quietly revealed on a [podcast](https://www.youtube.com/watch?v=429QC4Yl-mA&t=1153s)  hosted by startup Weights & Biases that OpenAI has shifted its  focus to other domains, where data is more readily available.  
>  
>‚ÄúSo it turns out that we can make a gigantic progress whenever we  have access to data. And I kept all of our machinery unsupervised,  \[using\] reinforcement learning ‚Äî \[it\] work\[s\] extremely well. There  \[are\] actually plenty of domains that are very, very rich with data. And  ultimately that was holding us back in terms of robotics,‚Äù Zaremba  said. ‚ÄúThe decision \[to disband the robotics team\] was quite hard for  me. But I got the realization some time ago that actually, that‚Äôs for  the best from the perspective of the company.‚Äù

Not really surprising. I wonder if it's just too early to make much headway in general-purpose robotics? Kind of like neural nets did not really take off until we had GPUs+data, some key ingredients necessary for success are still just not there...",1626550029.0
1626789602.739,t3_omorot,,[R] Keeping track of LSTM hidden state in online learning context,5,https://www.reddit.com/r/MachineLearning/comments/omorot/r_keeping_track_of_lstm_hidden_state_in_online/,mrwafflezzz,r/MachineLearning,"Do I need to pass the previous (nonzero) hidden state of an LSTM for the next sample in an online learning (per sample/ no batches) context?

I assume the hidden state is always zero right now for every new sample.

Currently my forward pass consists of this in PyTorch:

    def forward(self, x):
        x = self.src_embedding(x)
        x, _ = self.bigru(x) # Initial hidden state h0 defaults to zeros
        x = x[:, -1] # Forward only output of last hidden state to fc layer
        x = self.fc1(x)
        return x

EDIT:

Big thanks to everyone who commented. I followed their advice.

This is now what my forward pass looks like. Detach prevents gradient descent on the hidden state:

    def forward(self, x, h):
    x = self.src_embedding(x)
    x, h = self.bigru(x, h.detach())
    x = x[:, -1]
    x = self.fc1(x)
        return x, h

The model also has a function for 0 initializing the hidden states before online learning starts. The model overwrites the hidden state during online learning.

    outputs, h = model(input_sequence, h)

The hidden state of the previous sample is now used for the next sample.

Accuracy went up by a tad bit.",1626607573.0
1626789602.755362,t3_omsqwb,,[D] Why aren't Normalizing Flows suitable for Discrete Distributions?,1,https://www.reddit.com/r/MachineLearning/comments/omsqwb/d_why_arent_normalizing_flows_suitable_for/,arcxtriy,r/MachineLearning,"I am currently trying to understand why normalizing flows are not  applicable to discrete distributions (a quick primer on NF can be found [here](http://akosiorek.github.io/ml/2018/04/03/norm_flows.html)). The assumptions on the transformation f  between the probability distributions are:

&#x200B;

1. f must be an invertible function
2. f must be a smooth function

Assume I want to learn a normalizing flow between a Poisson and a Normal distribution.  
 If I discretize my Normal distribution, both have an infinite support,  i.e., the same number of support elements and hence, I can find an  invertible mapping between them (another option would be to consider  only the first n natural numbers for the Poisson distribution and also  select n elements from the Normal distribution).

Moreover, when I am training a Normalizing Flow on my computer, I never have *continuous*  data - instead, I have discrete samples from my distribution. So, where  is the issue with taking these samples from a discrete distribution  now? Unfortunately, I cannot see the difference to the case where I am  trying to map between two Normal distributions: if I sample from a Normal distribution I have discrete samples as well.

Thanks in advance!",1626622353.0
1626789602.768812,t3_omk943,,[R] Feature Engineering based on Memes for Trading Strategies,12,https://www.reddit.com/r/MachineLearning/comments/omk943/r_feature_engineering_based_on_memes_for_trading/,Beginning_Income6840,r/MachineLearning,"
Hello toghter, we are a group of researches studying the impact of ‚ÄûMemes‚Äú on low market cap equities. We are about to publish a paper that found a strong predictive correlation between memes and deltas on equities ( GME, AMC, Crypto etc). As we want to analyze this on a larger scale for a longer time Periode, we are putting the code into production letting it run for a long time to further study the impact. As I am the only engineer currently active involved in the Project, we are looking for someone with experience in ML Engineering and cloud services. We are a team of a php student in Statistics and the professor of the department of statistics at the university of Augsburg. We are planning to keep on publishing papers about that and have multiple Deep-Learning projects at hand. I am putting around 10 hours a week into the project, so manageable time. We are also studying image classification models on chart movements for some of the predictions. 
So what‚Äôs in if you wanna join. 

- become Co Author in our papers to come 
- studying a very new phenomenon 
- getting hands on experience with DL/ML, Stats, MLops and Cloud solutions. 
- work with highly motivated people 
- and who knows, maybe we are becoming the next Jim Simons ( this was sarcasm) 

Just drop me a PM or comment and I set up a meeting. I would be very happy to find some motivate humans from anywhere !",1626585382.0
1626789602.781322,t3_omj0sa,,[P] Parallelformers: An Efficient Model Parallelization Toolkit for Deployment,13,https://www.reddit.com/r/MachineLearning/comments/omj0sa/p_parallelformers_an_efficient_model/,hyunwoongko,r/MachineLearning,"&#x200B;

[A logo that can be a bit scary... \(We used the emoji in Unicode, not the Hugging face CI.\)](https://preview.redd.it/kt404oyd7wb71.png?width=552&format=png&auto=webp&s=7e5beb81330fc7b65a65e4060892d527b2fbacbe)

&#x200B;

Hello, I am writing to inform you about the release of Parallelformers ([https://github.com/tunib-ai/parallelformers](https://github.com/tunib-ai/parallelformers)), a model parallelization library at TUNiB. Parallelformers is a toolkit that supports inference parallelism for 68 of 70 models in Huggingface Transformers with 1 line of code.

Previously, DeepSpeed-Inference was used as a parallelization toolkit for model inference.

(1) It was impossible to deploy to the web server due to the process flow,

(2) Lack of integration with Huggingface Transformers, which has now become the de facto standard for natural language processing tools. (DeepSpeed-Inference only supports 3 models)

(3) Also, since parallelization starts in the GPU state, there was a problem that all parameters of the model had to be put on the GPU before parallelization.

Parallelformers solved a number of problems in DeepSpeed-Inference. Using this toolkit internally, we were able to easily deploy a large model to our web server, reducing the cost of deployment by up to 3-5x. More detailed information and source code can be found on GitHub. Thanks !",1626580031.0
1626789602.795001,t3_omnow9,,[Project] Newsemble: An API to fetch current news data,4,https://www.reddit.com/r/MachineLearning/comments/omnow9/project_newsemble_an_api_to_fetch_current_news/,rg089,r/MachineLearning,"&#x200B;

https://preview.redd.it/kzsxxkld2yb71.png?width=480&format=png&auto=webp&s=015490a1ebd09d11f0a22c979f592375e7242436

Note: *This is not exactly a Machine Learning project, but a tool that can be useful in many Machine Learning (NLP)  projects.*

Hey everyone,

I (along with 2 other people) made a project called **Newsemble**.      It is an API that allows for fast retrieval of current news (at the     moment, only Indian websites are supported, but we can add others if     anyone wants that). It's a REST API built using *Flask, MongoDB and BeautifulSoup*.      Due to some of the drawbacks of current news APIs (full content not     available, character limit, limited requests), we wanted to build  our    own as were looking to do news analysis.

We have made all the code open source. Please refer to the [medium blog](https://medium.com/@rg089/newsemble-3311d2dc9817) for further details and implementation of this API.

*This will be useful for news analysis, trend detection, keyword detection amongst other NLP tasks.*

*We are planning to release some NLP projects using this API very soon!*

***Most  importantly, if there are any additional features or extra news  sites    you want, or any other improvements in general, please do let  us  know. Thanks!ü§ùüèª***

If you found the project useful, please üëè the article or üåü the repo. It really motivates us going forward!

Blog link :

[https://medium.com/@rg089/newsemble-3311d2dc9817](https://medium.com/@rg089/newsemble-3311d2dc9817)

Source code :

[https://github.com/rg089/newsemble](https://github.com/rg089/newsemble)

API link:

[http://www.newsemble.ml/news/](http://www.newsemble.ml/news/)",1626602365.0
1626789602.808193,t3_om3j56,,"[N] Not exactly ML, but I know a lot of folks here use Python. I wonder when popular packages will start upgrading to incorporate newer versions of Python.",74,https://twitter.com/pyblogsal/status/1416034899639750659,Seankala,r/MachineLearning,,1626525799.0
1626789602.816802,t3_omlv1d,,"[D] machine learning models for ""prioritization""",1,https://www.reddit.com/r/MachineLearning/comments/omlv1d/d_machine_learning_models_for_prioritization/,blueest,r/MachineLearning,"Are there any common machine learning algorithms that are currently being used for ""prioritization"" tasks? 

I have read that in the field of operations research, there are methods from linear programming that can be used for scheduling tasks (e.g. which worker to assign to which machine), but these often require a predetermined cost matrix that details the relative advantage of having a particular worker assigned to a particular machine.

But is there anything in machine learning that tries to solve the ""prioritization task""? If 5 new jobs arrive, which job should be started first? 

Has anyone ever worked on something like that?

Thanks",1626593078.0
1626789602.830864,t3_omlr81,,[D] Random Search in Machine Learning,1,https://www.reddit.com/r/MachineLearning/comments/omlr81/d_random_search_in_machine_learning/,ottawalanguages,r/MachineLearning,"Is random search still used in Machine Learning beyond a teaching example? With all the advanced search algorithms that now exist, are there still any instances where it might be advantageous to perform a random search?

Thanks",1626592549.0
1626789602.843,t3_ome7r8,,[D] RL in industry,6,https://www.reddit.com/r/MachineLearning/comments/ome7r8/d_rl_in_industry/,cozyoverlazy,r/MachineLearning,"Just wanted to ask for those who have experience with RL, how is it being applied besides in closed game environments? I took an RL class for my masters and loved it but I could wrap my mind around the complexity in environment setup and reward systems. I‚Äôm interested in exploring it more but I‚Äôd like to use it outside of its usual game environments",1626561034.0
1626789602.857018,t3_omaqmx,,[D] Cheapest Cloud GPU's providing the latest NVIDIA GPU's in 2021?,8,https://www.reddit.com/r/MachineLearning/comments/omaqmx/d_cheapest_cloud_gpus_providing_the_latest_nvidia/,Rohit901,r/MachineLearning,"Hey all,

I wanted to access the latest and the greatest Nvidia GPU's and was wondering which cloud provider would be the cheapest?

In my search so far, I was able to find that Genesis Cloud provides you with Nvidia 3080 and Nvidia 3090, are there any other good cloud providers who provide the same GPU? which one would be the cheapest?

TIA",1626549467.0
